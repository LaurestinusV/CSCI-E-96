{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 01 - Pitfalls in Data Mining     \n",
    "## CSCI E-96\n",
    "\n",
    "The goal of data mining is to find important relationships in large complex datasets. These dataset typically contain a large number of variables. The **high-dimensional** nature of the data leads to some commonly encountered pitfalls which lead to incorrect inferences.   \n",
    "\n",
    "In this assignment you will gain a bit of experience with three important concepts in data mining:  \n",
    "\n",
    "1. **False Discovery Rate Control:** The goal of data mining is to find important relationships in large complex datasets. These dataset typically contain a large number of variables. The **high-dimensional** nature of the data leads to some commonly encountered pitfalls which lead to incorrect inferences. A related problem is cutting off a large-scale analysis when a desired relationship is 'found'. This practice of **p-value mining** often leads to unwarranted inferences. You will apply false discovery rate (FDR) control methods to address this problem.   \n",
    "2. **Key-Value Pairs:** Large scale data is typically managed using key-value (KV) pairs. The exercises in this assignment give you some experience working with KV pair data management.  \n",
    "3. **Map and Reduce Processes:** Much of large scale data mining requires use of a split-apply-combine approach. The data is split into manageable chunks, analytic transformations are applied, and the result combined or aggregated. A commonly used class of a split-apply-combine algorithm is MapReduce. \n",
    "\n",
    "In order to keep the scope of this assignment manageable, you will use limited versions of KV pair management and MapReduce. Specifically, you will use common Python tools to implement these concepts rather than dedicated large scale analytic platforms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Hypothesis Tesing\n",
    "\n",
    "Testing multiple hypothesis in high-dimensional data can be problematic. Exhaustively testing all pairwise relationships between variables in a data set is a commonly used, but generally misleading from of **multiple comparisons**. The chance of finding false significance, using such a **data dredging** approach, can be surprisingly high. \n",
    "\n",
    "In this exercise you will perform multiple comparisons on only 20 **identically distributed independent (iid)** variables. Ideally, such tests should not find significant relationships, but the actual result is quite different. \n",
    "\n",
    "To get started, execute the code in the cell below to load the required packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "from scipy.stats import ttest_ind, f_oneway\n",
    "from itertools import product, combinations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.stats.multitest as smt\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you will apply a t-test to all pairwise combinations of identical Normally distributed variables. In this case, we will create a data set with 20 iid Normal distributions of 1000 samples each. Execute the code in the cell below to find this data and display the mean and variance of each variable.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NORMAL VARS\n",
      "[[ 0.81879162 -1.04355064  0.3509007  ... -0.17542066 -0.08270438\n",
      "  -0.88845473]\n",
      " [-0.30076649  0.90837517 -0.64559131 ... -0.46415036 -0.79785911\n",
      "   1.31076281]\n",
      " [ 1.17479389 -0.05046953  0.71895176 ...  0.71999964 -0.86680023\n",
      "  -0.34668516]\n",
      " ...\n",
      " [ 0.0943446  -0.88593117  2.21144516 ... -0.48951209 -0.14650596\n",
      "   1.468948  ]\n",
      " [-0.99396682 -0.60068401 -1.06598225 ...  0.80637671 -0.38532756\n",
      "  -0.06067443]\n",
      " [-0.7191216   0.0248463   0.2728283  ...  0.3302862   0.2384886\n",
      "   0.98507076]]\n",
      "Len of normal vars\n",
      "1000\n",
      "The means of the columns are\n",
      " [-1.16191649e-01  2.80829317e-02 -1.78516419e-02 -1.44691489e-02\n",
      "  3.03718152e-02  1.20007442e-02 -9.58845606e-05  1.98662580e-03\n",
      "  4.94154934e-02 -4.11640866e-02 -6.32977862e-03 -5.93868192e-02\n",
      " -2.56373595e-02  1.43568791e-02 -1.44725765e-02 -1.37023955e-02\n",
      "  1.80622439e-02  5.87029691e-02 -2.02650514e-02 -1.56346106e-02]\n",
      "\n",
      "The variances of the columns are\n",
      " [0.94834508 1.04744241 1.0258018  0.96977571 1.0089001  1.04113864\n",
      " 1.00657222 0.99192594 1.04713487 1.04329434 1.04023108 0.96791346\n",
      " 1.03706907 1.07179865 1.01431404 1.05060289 1.02054329 0.9686211\n",
      " 1.02810287 0.99521555]\n",
      "[-0.11619164926199306, 0.028082931743419342, -0.017851641926649935, -0.014469148926407141, 0.030371815211215986, 0.01200074415037437, -9.588456058957173e-05, 0.0019866258031239945, 0.04941549338764833, -0.04116408661462267, -0.0063297786154073365, -0.0593868191915101, -0.025637359468878882, 0.014356879111661789, -0.014472576450111173, -0.013702395461434471, 0.01806224389911345, 0.05870296913581026, -0.020265051397245912, -0.015634610559834186]\n",
      "[0.9483450841278454, 1.0474424115924386, 1.025801797556006, 0.9697757102682736, 1.0089000973212925, 1.0411386386684267, 1.0065722193307869, 0.9919259445834522, 1.0471348686585749, 1.0432943362335887, 1.040231080627933, 0.9679134633786484, 1.0370690685544615, 1.0717986540361213, 1.0143140442818062, 1.050602885420908, 1.0205432948150148, 0.9686211004006502, 1.0281028694152574, 0.9952155477459146]\n"
     ]
    }
   ],
   "source": [
    "ncolumns = 20\n",
    "nr.seed(234)\n",
    "mean = []\n",
    "var = []\n",
    "ttest = []\n",
    "normal_vars = nr.normal(size=(1000,ncolumns))\n",
    "print('NORMAL VARS')\n",
    "print(normal_vars)\n",
    "print('Len of normal vars')\n",
    "print(len(normal_vars))\n",
    "print('The means of the columns are\\n', np.mean(normal_vars, axis = 0))\n",
    "print('\\nThe variances of the columns are\\n', np.var(normal_vars, axis = 0))\n",
    "##create arrays for mean and var\n",
    "for mn in np.mean(normal_vars, axis =0 ):\n",
    "    mean.append(mn)\n",
    "print(mean)\n",
    "    \n",
    "for vr in np.var(normal_vars, axis = 0):\n",
    "    var.append(vr)\n",
    "print(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that means and variances are close to 0.0 and 1.0 respectively. As expected, there is not much difference between these variables.\n",
    "\n",
    "How many of these t-tests will show **significance** at the 0.05 cut-off level? There are 380 pairwise combinations, so we expect to find a number of falsely significant test results at this level. To find out, complete and execute the code in the cell below to filter the test results and print those that show significance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a hash \n",
    "\n",
    "The goal of this exercise is to compute pairwise hypothesis tests of the differences in means for each of the iid Normal vectors. As an intermediate step you will create a Pandas data frame using a hash of the keys of the vectors. The data frame will contain the **key-value**, $(K,V)$, pairs. Each key must represent an index for the two vectors used to compute the test statistic. The keys will then be used to index the results of these hypothesis tests. \n",
    "\n",
    "The question is, how can we create a hash from the keys for the pair of vectors? In this case to we will use a simple, but far from optimal hash. For the two vector indicies $i, j$, for some key and modulo, $m$, we will compute the hash as:  \n",
    "\n",
    "$$h(i,j) = (i + key*j) mod m$$\n",
    "\n",
    "> **Computational Note:** The Pandas data frame is an efficient and reasonably scalable **hash table**. The hash function used depends on the type of the key; integer, string, etc. The resulting dictionary of key-value pairs, $(K,V)$, can therefore be access in far less than linear time, often about $O(log(N))$.  \n",
    "\n",
    "If you are not familiar with Python dictionaries you can find a short tutorial [here](https://www.tutorialspoint.com/python_data_structure/python_hash_table.htm), as well as many other places on the web."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 1-1:** Given that our space of vectors is actually quite small, just 20, we do not need a sophisticated and scalable hash function. This hashed key will then be used to store and retrieve the values using a Python dictionary, in about $O(log(N))$ time.     \n",
    "\n",
    "> In this exercise you will test a simple hash function and its inverse. Examine the code below and notice that the hash function encodes the two indexes into a single integer by simple additional and multiplication. The modulo operation limits the size of the hash table. However, to keep things simple you will not need to implement any hash collision resolution mechanism. As a result, the size of the table is set much larger than required.  \n",
    "\n",
    "> To test this hash, do the following:    \n",
    "> 1. Create a function called hash function to compute the hash. The arguments to the function are $i$ and $j$, the `hash\\_key` and the `modulo\\_multiplier`. The defaults of the arguments are $hash\\_key=1024$ and $modulo\\_multiplier=32$. The modulo number is $hash\\_key * modulo\\_multiplier$, e.g. $modulo = 32,768$. The multiplier is the ratio of expected values stored, $n$, to the number of unique hash keys, $m$, e.g. the ratio $m/n$.\n",
    "> 2. Using the Python [ittertools.combinations](https://docs.python.org/3/library/itertools.html#itertools.combinations) function create all unique pairwise combinations of indexes i and j. The arguments to this function are the indexes to the iid Normal vectors. The iterator is `range(ncolumns)` choose 2, since these comparisons are pairwise.    \n",
    "> 3. Within this loop call the hash with the values of $i$ and $j$ as arguments.   \n",
    "> 3. On a single line print the following; the values of i and j, the hash key value, but only if $i \\le 6$. The restriction is to keep the printed output shorter.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Count = 0\n",
      " Count = 1\n",
      " Count = 2\n",
      " Count = 3\n",
      " Count = 4\n",
      " Count = 5\n",
      " Count = 6\n",
      " Count = 7\n",
      " Count = 8\n",
      " Count = 9\n",
      " Count = 10\n",
      " Count = 11\n",
      " Count = 12\n",
      " Count = 13\n",
      " Count = 14\n",
      " Count = 15\n",
      " Count = 16\n",
      " Count = 17\n",
      " Count = 18\n",
      " Count = 19\n",
      " Count = 20\n",
      " Count = 21\n",
      " Count = 22\n",
      " Count = 23\n",
      " Count = 24\n",
      " Count = 25\n",
      " Count = 26\n",
      " Count = 27\n",
      " Count = 28\n",
      " Count = 29\n",
      " Count = 30\n",
      " Count = 31\n",
      " Count = 32\n",
      " Count = 33\n",
      " Count = 34\n",
      " Count = 35\n",
      " Count = 36\n",
      " Count = 37\n",
      " Count = 38\n",
      " Count = 39\n",
      " Count = 40\n",
      " Count = 41\n",
      " Count = 42\n",
      " Count = 43\n",
      " Count = 44\n",
      " Count = 45\n",
      " Count = 46\n",
      " Count = 47\n",
      " Count = 48\n",
      " Count = 49\n",
      " Count = 50\n",
      " Count = 51\n",
      " Count = 52\n",
      " Count = 53\n",
      " Count = 54\n",
      " Count = 55\n",
      " Count = 56\n",
      " Count = 57\n",
      " Count = 58\n",
      " Count = 59\n",
      " Count = 60\n",
      " Count = 61\n",
      " Count = 62\n",
      " Count = 63\n",
      " Count = 64\n",
      " Count = 65\n",
      " Count = 66\n",
      " Count = 67\n",
      " Count = 68\n",
      " Count = 69\n",
      " Count = 70\n",
      " Count = 71\n",
      " Count = 72\n",
      " Count = 73\n",
      " Count = 74\n",
      " Count = 75\n",
      " Count = 76\n",
      " Count = 77\n",
      " Count = 78\n",
      " Count = 79\n",
      " Count = 80\n",
      " Count = 81\n",
      " Count = 82\n",
      " Count = 83\n",
      " Count = 84\n",
      " Count = 85\n",
      " Count = 86\n",
      " Count = 87\n",
      " Count = 88\n",
      " Count = 89\n",
      " Count = 90\n",
      " Count = 91\n",
      " Count = 92\n",
      " Count = 93\n",
      " Count = 94\n",
      " Count = 95\n",
      " Count = 96\n",
      " Count = 97\n",
      " Count = 98\n",
      " Count = 99\n",
      " Count = 100\n",
      " Count = 101\n",
      " Count = 102\n",
      " Count = 103\n",
      " Count = 104\n",
      " Count = 105\n",
      " Count = 106\n",
      " Count = 107\n",
      " Count = 108\n",
      " Count = 109\n",
      " Count = 110\n",
      " Count = 111\n",
      "COUNT: 1:  hash[(0 , 1)] : 1024\n",
      "COUNT: 2:  hash[(0 , 2)] : 2048\n",
      "COUNT: 3:  hash[(0 , 3)] : 3072\n",
      "COUNT: 4:  hash[(0 , 4)] : 4096\n",
      "COUNT: 5:  hash[(0 , 5)] : 5120\n",
      "COUNT: 6:  hash[(0 , 6)] : 6144\n",
      "COUNT: 7:  hash[(0 , 7)] : 7168\n",
      "COUNT: 8:  hash[(0 , 8)] : 8192\n",
      "COUNT: 9:  hash[(0 , 9)] : 9216\n",
      "COUNT: 10:  hash[(0 , 10)] : 10240\n",
      "COUNT: 11:  hash[(0 , 11)] : 11264\n",
      "COUNT: 12:  hash[(0 , 12)] : 12288\n",
      "COUNT: 13:  hash[(0 , 13)] : 13312\n",
      "COUNT: 14:  hash[(0 , 14)] : 14336\n",
      "COUNT: 15:  hash[(0 , 15)] : 15360\n",
      "COUNT: 16:  hash[(0 , 16)] : 16384\n",
      "COUNT: 17:  hash[(0 , 17)] : 17408\n",
      "COUNT: 18:  hash[(0 , 18)] : 18432\n",
      "COUNT: 19:  hash[(0 , 19)] : 19456\n",
      "COUNT: 20:  hash[(1 , 2)] : 2050\n",
      "COUNT: 21:  hash[(1 , 3)] : 3075\n",
      "COUNT: 22:  hash[(1 , 4)] : 4100\n",
      "COUNT: 23:  hash[(1 , 5)] : 5125\n",
      "COUNT: 24:  hash[(1 , 6)] : 6150\n",
      "COUNT: 25:  hash[(1 , 7)] : 7175\n",
      "COUNT: 26:  hash[(1 , 8)] : 8200\n",
      "COUNT: 27:  hash[(1 , 9)] : 9225\n",
      "COUNT: 28:  hash[(1 , 10)] : 10250\n",
      "COUNT: 29:  hash[(1 , 11)] : 11275\n",
      "COUNT: 30:  hash[(1 , 12)] : 12300\n",
      "COUNT: 31:  hash[(1 , 13)] : 13325\n",
      "COUNT: 32:  hash[(1 , 14)] : 14350\n",
      "COUNT: 33:  hash[(1 , 15)] : 15375\n",
      "COUNT: 34:  hash[(1 , 16)] : 16400\n",
      "COUNT: 35:  hash[(1 , 17)] : 17425\n",
      "COUNT: 36:  hash[(1 , 18)] : 18450\n",
      "COUNT: 37:  hash[(1 , 19)] : 19475\n",
      "COUNT: 38:  hash[(2 , 3)] : 3078\n",
      "COUNT: 39:  hash[(2 , 4)] : 4104\n",
      "COUNT: 40:  hash[(2 , 5)] : 5130\n",
      "COUNT: 41:  hash[(2 , 6)] : 6156\n",
      "COUNT: 42:  hash[(2 , 7)] : 7182\n",
      "COUNT: 43:  hash[(2 , 8)] : 8208\n",
      "COUNT: 44:  hash[(2 , 9)] : 9234\n",
      "COUNT: 45:  hash[(2 , 10)] : 10260\n",
      "COUNT: 46:  hash[(2 , 11)] : 11286\n",
      "COUNT: 47:  hash[(2 , 12)] : 12312\n",
      "COUNT: 48:  hash[(2 , 13)] : 13338\n",
      "COUNT: 49:  hash[(2 , 14)] : 14364\n",
      "COUNT: 50:  hash[(2 , 15)] : 15390\n",
      "COUNT: 51:  hash[(2 , 16)] : 16416\n",
      "COUNT: 52:  hash[(2 , 17)] : 17442\n",
      "COUNT: 53:  hash[(2 , 18)] : 18468\n",
      "COUNT: 54:  hash[(2 , 19)] : 19494\n",
      "COUNT: 55:  hash[(3 , 4)] : 4108\n",
      "COUNT: 56:  hash[(3 , 5)] : 5135\n",
      "COUNT: 57:  hash[(3 , 6)] : 6162\n",
      "COUNT: 58:  hash[(3 , 7)] : 7189\n",
      "COUNT: 59:  hash[(3 , 8)] : 8216\n",
      "COUNT: 60:  hash[(3 , 9)] : 9243\n",
      "COUNT: 61:  hash[(3 , 10)] : 10270\n",
      "COUNT: 62:  hash[(3 , 11)] : 11297\n",
      "COUNT: 63:  hash[(3 , 12)] : 12324\n",
      "COUNT: 64:  hash[(3 , 13)] : 13351\n",
      "COUNT: 65:  hash[(3 , 14)] : 14378\n",
      "COUNT: 66:  hash[(3 , 15)] : 15405\n",
      "COUNT: 67:  hash[(3 , 16)] : 16432\n",
      "COUNT: 68:  hash[(3 , 17)] : 17459\n",
      "COUNT: 69:  hash[(3 , 18)] : 18486\n",
      "COUNT: 70:  hash[(3 , 19)] : 19513\n",
      "COUNT: 71:  hash[(4 , 5)] : 5140\n",
      "COUNT: 72:  hash[(4 , 6)] : 6168\n",
      "COUNT: 73:  hash[(4 , 7)] : 7196\n",
      "COUNT: 74:  hash[(4 , 8)] : 8224\n",
      "COUNT: 75:  hash[(4 , 9)] : 9252\n",
      "COUNT: 76:  hash[(4 , 10)] : 10280\n",
      "COUNT: 77:  hash[(4 , 11)] : 11308\n",
      "COUNT: 78:  hash[(4 , 12)] : 12336\n",
      "COUNT: 79:  hash[(4 , 13)] : 13364\n",
      "COUNT: 80:  hash[(4 , 14)] : 14392\n",
      "COUNT: 81:  hash[(4 , 15)] : 15420\n",
      "COUNT: 82:  hash[(4 , 16)] : 16448\n",
      "COUNT: 83:  hash[(4 , 17)] : 17476\n",
      "COUNT: 84:  hash[(4 , 18)] : 18504\n",
      "COUNT: 85:  hash[(4 , 19)] : 19532\n",
      "COUNT: 86:  hash[(5 , 6)] : 6174\n",
      "COUNT: 87:  hash[(5 , 7)] : 7203\n",
      "COUNT: 88:  hash[(5 , 8)] : 8232\n",
      "COUNT: 89:  hash[(5 , 9)] : 9261\n",
      "COUNT: 90:  hash[(5 , 10)] : 10290\n",
      "COUNT: 91:  hash[(5 , 11)] : 11319\n",
      "COUNT: 92:  hash[(5 , 12)] : 12348\n",
      "COUNT: 93:  hash[(5 , 13)] : 13377\n",
      "COUNT: 94:  hash[(5 , 14)] : 14406\n",
      "COUNT: 95:  hash[(5 , 15)] : 15435\n",
      "COUNT: 96:  hash[(5 , 16)] : 16464\n",
      "COUNT: 97:  hash[(5 , 17)] : 17493\n",
      "COUNT: 98:  hash[(5 , 18)] : 18522\n",
      "COUNT: 99:  hash[(5 , 19)] : 19551\n",
      "COUNT: 100:  hash[(6 , 7)] : 7210\n",
      "COUNT: 101:  hash[(6 , 8)] : 8240\n",
      "COUNT: 102:  hash[(6 , 9)] : 9270\n",
      "COUNT: 103:  hash[(6 , 10)] : 10300\n",
      "COUNT: 104:  hash[(6 , 11)] : 11330\n",
      "COUNT: 105:  hash[(6 , 12)] : 12360\n",
      "COUNT: 106:  hash[(6 , 13)] : 13390\n",
      "COUNT: 107:  hash[(6 , 14)] : 14420\n",
      "COUNT: 108:  hash[(6 , 15)] : 15450\n",
      "COUNT: 109:  hash[(6 , 16)] : 16480\n",
      "COUNT: 110:  hash[(6 , 17)] : 17510\n",
      "COUNT: 111:  hash[(6 , 18)] : 18540\n",
      "COUNT: 112:  hash[(6 , 19)] : 19570\n",
      " The Total Number of Combinations is: 190\n",
      "harr_unique(0): 1024 COUNT: 1\n",
      "harr_unique(1): 2048 COUNT: 2\n",
      "harr_unique(2): 2050 COUNT: 3\n",
      "harr_unique(3): 3072 COUNT: 4\n",
      "harr_unique(4): 3075 COUNT: 5\n",
      "harr_unique(5): 3078 COUNT: 6\n",
      "harr_unique(6): 4096 COUNT: 7\n",
      "harr_unique(7): 4100 COUNT: 8\n",
      "harr_unique(8): 4104 COUNT: 9\n",
      "harr_unique(9): 4108 COUNT: 10\n",
      "harr_unique(10): 5120 COUNT: 11\n",
      "harr_unique(11): 5125 COUNT: 12\n",
      "harr_unique(12): 5130 COUNT: 13\n",
      "harr_unique(13): 5135 COUNT: 14\n",
      "harr_unique(14): 5140 COUNT: 15\n",
      "harr_unique(15): 6144 COUNT: 16\n",
      "harr_unique(16): 6150 COUNT: 17\n",
      "harr_unique(17): 6156 COUNT: 18\n",
      "harr_unique(18): 6162 COUNT: 19\n",
      "harr_unique(19): 6168 COUNT: 20\n",
      "harr_unique(20): 6174 COUNT: 21\n",
      "harr_unique(21): 7168 COUNT: 22\n",
      "harr_unique(22): 7175 COUNT: 23\n",
      "harr_unique(23): 7182 COUNT: 24\n",
      "harr_unique(24): 7189 COUNT: 25\n",
      "harr_unique(25): 7196 COUNT: 26\n",
      "harr_unique(26): 7203 COUNT: 27\n",
      "harr_unique(27): 7210 COUNT: 28\n",
      "harr_unique(28): 8192 COUNT: 29\n",
      "harr_unique(29): 8200 COUNT: 30\n",
      "harr_unique(30): 8208 COUNT: 31\n",
      "harr_unique(31): 8216 COUNT: 32\n",
      "harr_unique(32): 8224 COUNT: 33\n",
      "harr_unique(33): 8232 COUNT: 34\n",
      "harr_unique(34): 8240 COUNT: 35\n",
      "harr_unique(35): 8248 COUNT: 36\n",
      "harr_unique(36): 9216 COUNT: 37\n",
      "harr_unique(37): 9225 COUNT: 38\n",
      "harr_unique(38): 9234 COUNT: 39\n",
      "harr_unique(39): 9243 COUNT: 40\n",
      "harr_unique(40): 9252 COUNT: 41\n",
      "harr_unique(41): 9261 COUNT: 42\n",
      "harr_unique(42): 9270 COUNT: 43\n",
      "harr_unique(43): 9279 COUNT: 44\n",
      "harr_unique(44): 9288 COUNT: 45\n",
      "harr_unique(45): 10240 COUNT: 46\n",
      "harr_unique(46): 10250 COUNT: 47\n",
      "harr_unique(47): 10260 COUNT: 48\n",
      "harr_unique(48): 10270 COUNT: 49\n",
      "harr_unique(49): 10280 COUNT: 50\n",
      "harr_unique(50): 10290 COUNT: 51\n",
      "harr_unique(51): 10300 COUNT: 52\n",
      "harr_unique(52): 10310 COUNT: 53\n",
      "harr_unique(53): 10320 COUNT: 54\n",
      "harr_unique(54): 10330 COUNT: 55\n",
      "harr_unique(55): 11264 COUNT: 56\n",
      "harr_unique(56): 11275 COUNT: 57\n",
      "harr_unique(57): 11286 COUNT: 58\n",
      "harr_unique(58): 11297 COUNT: 59\n",
      "harr_unique(59): 11308 COUNT: 60\n",
      "harr_unique(60): 11319 COUNT: 61\n",
      "harr_unique(61): 11330 COUNT: 62\n",
      "harr_unique(62): 11341 COUNT: 63\n",
      "harr_unique(63): 11352 COUNT: 64\n",
      "harr_unique(64): 11363 COUNT: 65\n",
      "harr_unique(65): 11374 COUNT: 66\n",
      "harr_unique(66): 12288 COUNT: 67\n",
      "harr_unique(67): 12300 COUNT: 68\n",
      "harr_unique(68): 12312 COUNT: 69\n",
      "harr_unique(69): 12324 COUNT: 70\n",
      "harr_unique(70): 12336 COUNT: 71\n",
      "harr_unique(71): 12348 COUNT: 72\n",
      "harr_unique(72): 12360 COUNT: 73\n",
      "harr_unique(73): 12372 COUNT: 74\n",
      "harr_unique(74): 12384 COUNT: 75\n",
      "harr_unique(75): 12396 COUNT: 76\n",
      "harr_unique(76): 12408 COUNT: 77\n",
      "harr_unique(77): 12420 COUNT: 78\n",
      "harr_unique(78): 13312 COUNT: 79\n",
      "harr_unique(79): 13325 COUNT: 80\n",
      "harr_unique(80): 13338 COUNT: 81\n",
      "harr_unique(81): 13351 COUNT: 82\n",
      "harr_unique(82): 13364 COUNT: 83\n",
      "harr_unique(83): 13377 COUNT: 84\n",
      "harr_unique(84): 13390 COUNT: 85\n",
      "harr_unique(85): 13403 COUNT: 86\n",
      "harr_unique(86): 13416 COUNT: 87\n",
      "harr_unique(87): 13429 COUNT: 88\n",
      "harr_unique(88): 13442 COUNT: 89\n",
      "harr_unique(89): 13455 COUNT: 90\n",
      "harr_unique(90): 13468 COUNT: 91\n",
      "harr_unique(91): 14336 COUNT: 92\n",
      "harr_unique(92): 14350 COUNT: 93\n",
      "harr_unique(93): 14364 COUNT: 94\n",
      "harr_unique(94): 14378 COUNT: 95\n",
      "harr_unique(95): 14392 COUNT: 96\n",
      "harr_unique(96): 14406 COUNT: 97\n",
      "harr_unique(97): 14420 COUNT: 98\n",
      "harr_unique(98): 14434 COUNT: 99\n",
      "harr_unique(99): 14448 COUNT: 100\n",
      "harr_unique(100): 14462 COUNT: 101\n",
      "harr_unique(101): 14476 COUNT: 102\n",
      "harr_unique(102): 14490 COUNT: 103\n",
      "harr_unique(103): 14504 COUNT: 104\n",
      "harr_unique(104): 14518 COUNT: 105\n",
      "harr_unique(105): 15360 COUNT: 106\n",
      "harr_unique(106): 15375 COUNT: 107\n",
      "harr_unique(107): 15390 COUNT: 108\n",
      "harr_unique(108): 15405 COUNT: 109\n",
      "harr_unique(109): 15420 COUNT: 110\n",
      "harr_unique(110): 15435 COUNT: 111\n",
      "harr_unique(111): 15450 COUNT: 112\n",
      "harr_unique(112): 15465 COUNT: 113\n",
      "harr_unique(113): 15480 COUNT: 114\n",
      "harr_unique(114): 15495 COUNT: 115\n",
      "harr_unique(115): 15510 COUNT: 116\n",
      "harr_unique(116): 15525 COUNT: 117\n",
      "harr_unique(117): 15540 COUNT: 118\n",
      "harr_unique(118): 15555 COUNT: 119\n",
      "harr_unique(119): 15570 COUNT: 120\n",
      "harr_unique(120): 16384 COUNT: 121\n",
      "harr_unique(121): 16400 COUNT: 122\n",
      "harr_unique(122): 16416 COUNT: 123\n",
      "harr_unique(123): 16432 COUNT: 124\n",
      "harr_unique(124): 16448 COUNT: 125\n",
      "harr_unique(125): 16464 COUNT: 126\n",
      "harr_unique(126): 16480 COUNT: 127\n",
      "harr_unique(127): 16496 COUNT: 128\n",
      "harr_unique(128): 16512 COUNT: 129\n",
      "harr_unique(129): 16528 COUNT: 130\n",
      "harr_unique(130): 16544 COUNT: 131\n",
      "harr_unique(131): 16560 COUNT: 132\n",
      "harr_unique(132): 16576 COUNT: 133\n",
      "harr_unique(133): 16592 COUNT: 134\n",
      "harr_unique(134): 16608 COUNT: 135\n",
      "harr_unique(135): 16624 COUNT: 136\n",
      "harr_unique(136): 17408 COUNT: 137\n",
      "harr_unique(137): 17425 COUNT: 138\n",
      "harr_unique(138): 17442 COUNT: 139\n",
      "harr_unique(139): 17459 COUNT: 140\n",
      "harr_unique(140): 17476 COUNT: 141\n",
      "harr_unique(141): 17493 COUNT: 142\n",
      "harr_unique(142): 17510 COUNT: 143\n",
      "harr_unique(143): 17527 COUNT: 144\n",
      "harr_unique(144): 17544 COUNT: 145\n",
      "harr_unique(145): 17561 COUNT: 146\n",
      "harr_unique(146): 17578 COUNT: 147\n",
      "harr_unique(147): 17595 COUNT: 148\n",
      "harr_unique(148): 17612 COUNT: 149\n",
      "harr_unique(149): 17629 COUNT: 150\n",
      "harr_unique(150): 17646 COUNT: 151\n",
      "harr_unique(151): 17663 COUNT: 152\n",
      "harr_unique(152): 17680 COUNT: 153\n",
      "harr_unique(153): 18432 COUNT: 154\n",
      "harr_unique(154): 18450 COUNT: 155\n",
      "harr_unique(155): 18468 COUNT: 156\n",
      "harr_unique(156): 18486 COUNT: 157\n",
      "harr_unique(157): 18504 COUNT: 158\n",
      "harr_unique(158): 18522 COUNT: 159\n",
      "harr_unique(159): 18540 COUNT: 160\n",
      "harr_unique(160): 18558 COUNT: 161\n",
      "harr_unique(161): 18576 COUNT: 162\n",
      "harr_unique(162): 18594 COUNT: 163\n",
      "harr_unique(163): 18612 COUNT: 164\n",
      "harr_unique(164): 18630 COUNT: 165\n",
      "harr_unique(165): 18648 COUNT: 166\n",
      "harr_unique(166): 18666 COUNT: 167\n",
      "harr_unique(167): 18684 COUNT: 168\n",
      "harr_unique(168): 18702 COUNT: 169\n",
      "harr_unique(169): 18720 COUNT: 170\n",
      "harr_unique(170): 18738 COUNT: 171\n",
      "harr_unique(171): 19456 COUNT: 172\n",
      "harr_unique(172): 19475 COUNT: 173\n",
      "harr_unique(173): 19494 COUNT: 174\n",
      "harr_unique(174): 19513 COUNT: 175\n",
      "harr_unique(175): 19532 COUNT: 176\n",
      "harr_unique(176): 19551 COUNT: 177\n",
      "harr_unique(177): 19570 COUNT: 178\n",
      "harr_unique(178): 19589 COUNT: 179\n",
      "harr_unique(179): 19608 COUNT: 180\n",
      "harr_unique(180): 19627 COUNT: 181\n",
      "harr_unique(181): 19646 COUNT: 182\n",
      "harr_unique(182): 19665 COUNT: 183\n",
      "harr_unique(183): 19684 COUNT: 184\n",
      "harr_unique(184): 19703 COUNT: 185\n",
      "harr_unique(185): 19722 COUNT: 186\n",
      "harr_unique(186): 19741 COUNT: 187\n",
      "harr_unique(187): 19760 COUNT: 188\n",
      "harr_unique(188): 19779 COUNT: 189\n",
      "harr_unique(189): 19798 COUNT: 190\n",
      " The Total Number of unique instances is: 190\n"
     ]
    }
   ],
   "source": [
    "def hash_function(i, j, hash_key=1024, modulo_multiplier=32):\n",
    "    ## Put your code below. \n",
    "    ##return ((i + hash_key) * j) % modulo_multiplier\n",
    "    return  ((i + hash_key) * j) % (hash_key*modulo_multiplier)\n",
    "\n",
    "count =0\n",
    "hash = {}\n",
    "harr =  []\n",
    "    ##h[(i,j)] = ((i + hash_key) * j ) % modulo_multiplier\n",
    "\n",
    "for i,j in combinations(range(ncolumns), 2):\n",
    "    if i <= 6: \n",
    "        \n",
    "        print( ' Count = ' + str(count))\n",
    "        ##hash[(i,j)] = hash_function(i,j)\n",
    "    hash[(i,j)] =     hash_function(i,j)\n",
    "    harr.append( hash_function(i,j))\n",
    "\n",
    "        ##print('i = ' + str(i) + '  j = ' + str(j) + '   hash = ' + str(hash) + '     Count hash = ' + str(count+1))\n",
    "    count += 1\n",
    "    combCount = 1\n",
    "        \n",
    "for i,j in combinations(range(ncolumns),2):     \n",
    "    if i<= 6:\n",
    "        print('COUNT: '+ str(combCount) + ':  hash[(' + str(i)+ ' , '+ str( j) + ')] : ' + str(hash[i,j]))\n",
    "    combCount += 1\n",
    "        \n",
    "print(' The Total Number of Combinations is: ' + str(count))\n",
    "    \n",
    "Count1 = 0  \n",
    "harr_u = np.unique(harr)\n",
    "for u in harr_u:\n",
    "    \n",
    "    print('harr_unique(' + str(Count1) + '): ' + str(u) + ' COUNT: '+ str(Count1+1))\n",
    "    Count1 += 1\n",
    "## print (harr_u)\n",
    "## print('harr_unique(' + str(Count1-1) + '): ' + str(u) + ' COUNT: '+ str(Count1))\n",
    "print(' The Total Number of unique instances is: '+ str(Count1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Examine the key pairs and the hash values. The question is, are there any hash collisions? This can be done as follows:   \n",
    "> 5. Compute a list of the hash values for all combinations of $i$ and $j$.   \n",
    "> 6. Print the length of the list.  \n",
    "> 7. Print the length of the unique values of the hash. You can find the unique values in a list with the [numpy.unique](https://numpy.org/doc/stable/reference/generated/numpy.unique.html) function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "harr_unique(0): 1024 Reoccurrances: 1\n",
      "harr_unique(1): 2048 Reoccurrances: 1\n",
      "harr_unique(2): 2050 Reoccurrances: 1\n",
      "harr_unique(3): 3072 Reoccurrances: 1\n",
      "harr_unique(4): 3075 Reoccurrances: 1\n",
      "harr_unique(5): 3078 Reoccurrances: 1\n",
      "harr_unique(6): 4096 Reoccurrances: 1\n",
      "harr_unique(7): 4100 Reoccurrances: 1\n",
      "harr_unique(8): 4104 Reoccurrances: 1\n",
      "harr_unique(9): 4108 Reoccurrances: 1\n",
      "harr_unique(10): 5120 Reoccurrances: 1\n",
      "harr_unique(11): 5125 Reoccurrances: 1\n",
      "harr_unique(12): 5130 Reoccurrances: 1\n",
      "harr_unique(13): 5135 Reoccurrances: 1\n",
      "harr_unique(14): 5140 Reoccurrances: 1\n",
      "harr_unique(15): 6144 Reoccurrances: 1\n",
      "harr_unique(16): 6150 Reoccurrances: 1\n",
      "harr_unique(17): 6156 Reoccurrances: 1\n",
      "harr_unique(18): 6162 Reoccurrances: 1\n",
      "harr_unique(19): 6168 Reoccurrances: 1\n",
      "harr_unique(20): 6174 Reoccurrances: 1\n",
      "harr_unique(21): 7168 Reoccurrances: 1\n",
      "harr_unique(22): 7175 Reoccurrances: 1\n",
      "harr_unique(23): 7182 Reoccurrances: 1\n",
      "harr_unique(24): 7189 Reoccurrances: 1\n",
      "harr_unique(25): 7196 Reoccurrances: 1\n",
      "harr_unique(26): 7203 Reoccurrances: 1\n",
      "harr_unique(27): 7210 Reoccurrances: 1\n",
      "harr_unique(28): 8192 Reoccurrances: 1\n",
      "harr_unique(29): 8200 Reoccurrances: 1\n",
      "harr_unique(30): 8208 Reoccurrances: 1\n",
      "harr_unique(31): 8216 Reoccurrances: 1\n",
      "harr_unique(32): 8224 Reoccurrances: 1\n",
      "harr_unique(33): 8232 Reoccurrances: 1\n",
      "harr_unique(34): 8240 Reoccurrances: 1\n",
      "harr_unique(35): 8248 Reoccurrances: 1\n",
      "harr_unique(36): 9216 Reoccurrances: 1\n",
      "harr_unique(37): 9225 Reoccurrances: 1\n",
      "harr_unique(38): 9234 Reoccurrances: 1\n",
      "harr_unique(39): 9243 Reoccurrances: 1\n",
      "harr_unique(40): 9252 Reoccurrances: 1\n",
      "harr_unique(41): 9261 Reoccurrances: 1\n",
      "harr_unique(42): 9270 Reoccurrances: 1\n",
      "harr_unique(43): 9279 Reoccurrances: 1\n",
      "harr_unique(44): 9288 Reoccurrances: 1\n",
      "harr_unique(45): 10240 Reoccurrances: 1\n",
      "harr_unique(46): 10250 Reoccurrances: 1\n",
      "harr_unique(47): 10260 Reoccurrances: 1\n",
      "harr_unique(48): 10270 Reoccurrances: 1\n",
      "harr_unique(49): 10280 Reoccurrances: 1\n",
      "harr_unique(50): 10290 Reoccurrances: 1\n",
      "harr_unique(51): 10300 Reoccurrances: 1\n",
      "harr_unique(52): 10310 Reoccurrances: 1\n",
      "harr_unique(53): 10320 Reoccurrances: 1\n",
      "harr_unique(54): 10330 Reoccurrances: 1\n",
      "harr_unique(55): 11264 Reoccurrances: 1\n",
      "harr_unique(56): 11275 Reoccurrances: 1\n",
      "harr_unique(57): 11286 Reoccurrances: 1\n",
      "harr_unique(58): 11297 Reoccurrances: 1\n",
      "harr_unique(59): 11308 Reoccurrances: 1\n",
      "harr_unique(60): 11319 Reoccurrances: 1\n",
      "harr_unique(61): 11330 Reoccurrances: 1\n",
      "harr_unique(62): 11341 Reoccurrances: 1\n",
      "harr_unique(63): 11352 Reoccurrances: 1\n",
      "harr_unique(64): 11363 Reoccurrances: 1\n",
      "harr_unique(65): 11374 Reoccurrances: 1\n",
      "harr_unique(66): 12288 Reoccurrances: 1\n",
      "harr_unique(67): 12300 Reoccurrances: 1\n",
      "harr_unique(68): 12312 Reoccurrances: 1\n",
      "harr_unique(69): 12324 Reoccurrances: 1\n",
      "harr_unique(70): 12336 Reoccurrances: 1\n",
      "harr_unique(71): 12348 Reoccurrances: 1\n",
      "harr_unique(72): 12360 Reoccurrances: 1\n",
      "harr_unique(73): 12372 Reoccurrances: 1\n",
      "harr_unique(74): 12384 Reoccurrances: 1\n",
      "harr_unique(75): 12396 Reoccurrances: 1\n",
      "harr_unique(76): 12408 Reoccurrances: 1\n",
      "harr_unique(77): 12420 Reoccurrances: 1\n",
      "harr_unique(78): 13312 Reoccurrances: 1\n",
      "harr_unique(79): 13325 Reoccurrances: 1\n",
      "harr_unique(80): 13338 Reoccurrances: 1\n",
      "harr_unique(81): 13351 Reoccurrances: 1\n",
      "harr_unique(82): 13364 Reoccurrances: 1\n",
      "harr_unique(83): 13377 Reoccurrances: 1\n",
      "harr_unique(84): 13390 Reoccurrances: 1\n",
      "harr_unique(85): 13403 Reoccurrances: 1\n",
      "harr_unique(86): 13416 Reoccurrances: 1\n",
      "harr_unique(87): 13429 Reoccurrances: 1\n",
      "harr_unique(88): 13442 Reoccurrances: 1\n",
      "harr_unique(89): 13455 Reoccurrances: 1\n",
      "harr_unique(90): 13468 Reoccurrances: 1\n",
      "harr_unique(91): 14336 Reoccurrances: 1\n",
      "harr_unique(92): 14350 Reoccurrances: 1\n",
      "harr_unique(93): 14364 Reoccurrances: 1\n",
      "harr_unique(94): 14378 Reoccurrances: 1\n",
      "harr_unique(95): 14392 Reoccurrances: 1\n",
      "harr_unique(96): 14406 Reoccurrances: 1\n",
      "harr_unique(97): 14420 Reoccurrances: 1\n",
      "harr_unique(98): 14434 Reoccurrances: 1\n",
      "harr_unique(99): 14448 Reoccurrances: 1\n",
      "harr_unique(100): 14462 Reoccurrances: 1\n",
      "harr_unique(101): 14476 Reoccurrances: 1\n",
      "harr_unique(102): 14490 Reoccurrances: 1\n",
      "harr_unique(103): 14504 Reoccurrances: 1\n",
      "harr_unique(104): 14518 Reoccurrances: 1\n",
      "harr_unique(105): 15360 Reoccurrances: 1\n",
      "harr_unique(106): 15375 Reoccurrances: 1\n",
      "harr_unique(107): 15390 Reoccurrances: 1\n",
      "harr_unique(108): 15405 Reoccurrances: 1\n",
      "harr_unique(109): 15420 Reoccurrances: 1\n",
      "harr_unique(110): 15435 Reoccurrances: 1\n",
      "harr_unique(111): 15450 Reoccurrances: 1\n",
      "harr_unique(112): 15465 Reoccurrances: 1\n",
      "harr_unique(113): 15480 Reoccurrances: 1\n",
      "harr_unique(114): 15495 Reoccurrances: 1\n",
      "harr_unique(115): 15510 Reoccurrances: 1\n",
      "harr_unique(116): 15525 Reoccurrances: 1\n",
      "harr_unique(117): 15540 Reoccurrances: 1\n",
      "harr_unique(118): 15555 Reoccurrances: 1\n",
      "harr_unique(119): 15570 Reoccurrances: 1\n",
      "harr_unique(120): 16384 Reoccurrances: 1\n",
      "harr_unique(121): 16400 Reoccurrances: 1\n",
      "harr_unique(122): 16416 Reoccurrances: 1\n",
      "harr_unique(123): 16432 Reoccurrances: 1\n",
      "harr_unique(124): 16448 Reoccurrances: 1\n",
      "harr_unique(125): 16464 Reoccurrances: 1\n",
      "harr_unique(126): 16480 Reoccurrances: 1\n",
      "harr_unique(127): 16496 Reoccurrances: 1\n",
      "harr_unique(128): 16512 Reoccurrances: 1\n",
      "harr_unique(129): 16528 Reoccurrances: 1\n",
      "harr_unique(130): 16544 Reoccurrances: 1\n",
      "harr_unique(131): 16560 Reoccurrances: 1\n",
      "harr_unique(132): 16576 Reoccurrances: 1\n",
      "harr_unique(133): 16592 Reoccurrances: 1\n",
      "harr_unique(134): 16608 Reoccurrances: 1\n",
      "harr_unique(135): 16624 Reoccurrances: 1\n",
      "harr_unique(136): 17408 Reoccurrances: 1\n",
      "harr_unique(137): 17425 Reoccurrances: 1\n",
      "harr_unique(138): 17442 Reoccurrances: 1\n",
      "harr_unique(139): 17459 Reoccurrances: 1\n",
      "harr_unique(140): 17476 Reoccurrances: 1\n",
      "harr_unique(141): 17493 Reoccurrances: 1\n",
      "harr_unique(142): 17510 Reoccurrances: 1\n",
      "harr_unique(143): 17527 Reoccurrances: 1\n",
      "harr_unique(144): 17544 Reoccurrances: 1\n",
      "harr_unique(145): 17561 Reoccurrances: 1\n",
      "harr_unique(146): 17578 Reoccurrances: 1\n",
      "harr_unique(147): 17595 Reoccurrances: 1\n",
      "harr_unique(148): 17612 Reoccurrances: 1\n",
      "harr_unique(149): 17629 Reoccurrances: 1\n",
      "harr_unique(150): 17646 Reoccurrances: 1\n",
      "harr_unique(151): 17663 Reoccurrances: 1\n",
      "harr_unique(152): 17680 Reoccurrances: 1\n",
      "harr_unique(153): 18432 Reoccurrances: 1\n",
      "harr_unique(154): 18450 Reoccurrances: 1\n",
      "harr_unique(155): 18468 Reoccurrances: 1\n",
      "harr_unique(156): 18486 Reoccurrances: 1\n",
      "harr_unique(157): 18504 Reoccurrances: 1\n",
      "harr_unique(158): 18522 Reoccurrances: 1\n",
      "harr_unique(159): 18540 Reoccurrances: 1\n",
      "harr_unique(160): 18558 Reoccurrances: 1\n",
      "harr_unique(161): 18576 Reoccurrances: 1\n",
      "harr_unique(162): 18594 Reoccurrances: 1\n",
      "harr_unique(163): 18612 Reoccurrances: 1\n",
      "harr_unique(164): 18630 Reoccurrances: 1\n",
      "harr_unique(165): 18648 Reoccurrances: 1\n",
      "harr_unique(166): 18666 Reoccurrances: 1\n",
      "harr_unique(167): 18684 Reoccurrances: 1\n",
      "harr_unique(168): 18702 Reoccurrances: 1\n",
      "harr_unique(169): 18720 Reoccurrances: 1\n",
      "harr_unique(170): 18738 Reoccurrances: 1\n",
      "harr_unique(171): 19456 Reoccurrances: 1\n",
      "harr_unique(172): 19475 Reoccurrances: 1\n",
      "harr_unique(173): 19494 Reoccurrances: 1\n",
      "harr_unique(174): 19513 Reoccurrances: 1\n",
      "harr_unique(175): 19532 Reoccurrances: 1\n",
      "harr_unique(176): 19551 Reoccurrances: 1\n",
      "harr_unique(177): 19570 Reoccurrances: 1\n",
      "harr_unique(178): 19589 Reoccurrances: 1\n",
      "harr_unique(179): 19608 Reoccurrances: 1\n",
      "harr_unique(180): 19627 Reoccurrances: 1\n",
      "harr_unique(181): 19646 Reoccurrances: 1\n",
      "harr_unique(182): 19665 Reoccurrances: 1\n",
      "harr_unique(183): 19684 Reoccurrances: 1\n",
      "harr_unique(184): 19703 Reoccurrances: 1\n",
      "harr_unique(185): 19722 Reoccurrances: 1\n",
      "harr_unique(186): 19741 Reoccurrances: 1\n",
      "harr_unique(187): 19760 Reoccurrances: 1\n",
      "harr_unique(188): 19779 Reoccurrances: 1\n",
      "harr_unique(189): 19798 Reoccurrances: 1\n",
      " The Total Number of unique instances is: 190\n"
     ]
    }
   ],
   "source": [
    "## Put your code below. \n",
    "\n",
    "Count1 = 0\n",
    "ui, harr_u = np.unique(harr,return_counts = True)\n",
    "\n",
    "for uindex in range(len(harr_u)):\n",
    "    Count1 += 1\n",
    "    print('harr_unique(' + str(uindex) + '): ' + str(ui[uindex]) + ' Reoccurrances: '+ str(harr_u[uindex]))\n",
    "\n",
    "print(' The Total Number of unique instances is: '+ str(Count1))\n",
    "\n",
    "##print (ui)\n",
    "##print (harr_u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Examine the results you have printed. Is there any evidence of hash key collisions?     \n",
    "> The ratio of $m/n$ is deliberately kept high since the simple hash function has no collision resolution mechanism. Optionally, you can try reducing this ration (the multiplier) to 16 and 8, noting the increase in hash collisions.  \n",
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No: There is no evidence of hash key collisions. There are 190 Combinations with 190 unique values. Each reoccurance has been determined for the unique hash values indicating the number of collisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The map process\n",
    "\n",
    "We are constructing this example a map and a reduce process. The processes are intended to compute the hypothesis test for differences of means between all the pairs of vectors. The first step is the map process, which creates the keys, or values of $i$ and $j$ for these pairs.   \n",
    "\n",
    "> **Exercise 1-2:** You will now create the code for the map task which build a data frame with $i, j$ key pairs indexed by the hash. By the following steps you will create code that represents a map task.  \n",
    "> 1. Create a data frame with two columns $i$ and $j$ with rows $= hash_key * modulo_multiplier $ and set all values to $= numpy.nan$.\n",
    "> 2. Create a loop over all combinations of the pairs of i and j.   \n",
    "> 2. Compute the hash key value for the indexes, i and j.  \n",
    "> 4. Add the $i$ and $j$ values to the row indexed by the hash key.  \n",
    "> 5. Return the hash table. \n",
    "> 6. Execute the function to create the hash table.  \n",
    "> 7. Compute and print the length of the hash table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal_vars\n",
      "[[ 0.81879162 -1.04355064  0.3509007  ... -0.17542066 -0.08270438\n",
      "  -0.88845473]\n",
      " [-0.30076649  0.90837517 -0.64559131 ... -0.46415036 -0.79785911\n",
      "   1.31076281]\n",
      " [ 1.17479389 -0.05046953  0.71895176 ...  0.71999964 -0.86680023\n",
      "  -0.34668516]\n",
      " ...\n",
      " [ 0.0943446  -0.88593117  2.21144516 ... -0.48951209 -0.14650596\n",
      "   1.468948  ]\n",
      " [-0.99396682 -0.60068401 -1.06598225 ...  0.80637671 -0.38532756\n",
      "  -0.06067443]\n",
      " [-0.7191216   0.0248463   0.2728283  ...  0.3302862   0.2384886\n",
      "   0.98507076]]\n",
      "VARS ARE HERE: 1000 rows with 20 columns\n",
      "[[ 0.81879162 -1.04355064  0.3509007  ... -0.17542066 -0.08270438\n",
      "  -0.88845473]\n",
      " [-0.30076649  0.90837517 -0.64559131 ... -0.46415036 -0.79785911\n",
      "   1.31076281]\n",
      " [ 1.17479389 -0.05046953  0.71895176 ...  0.71999964 -0.86680023\n",
      "  -0.34668516]\n",
      " ...\n",
      " [ 0.0943446  -0.88593117  2.21144516 ... -0.48951209 -0.14650596\n",
      "   1.468948  ]\n",
      " [-0.99396682 -0.60068401 -1.06598225 ...  0.80637671 -0.38532756\n",
      "  -0.06067443]\n",
      " [-0.7191216   0.0248463   0.2728283  ...  0.3302862   0.2384886\n",
      "   0.98507076]]\n",
      "VARS Rows\n",
      "1000\n",
      "SHAPE Rows\n",
      "(1000, 20)\n",
      "   i  j\n",
      "0  0  0\n",
      "1  0  1\n",
      "2  1  0\n",
      "3  1  1\n",
      "4  1  2\n",
      "ncols-shape :20 ncols1-len(vars[0]) :20\n",
      "190\n",
      "DF_ARR\n",
      "        i   j    hash\n",
      "1024    0   1   32768\n",
      "2048    0   2   65536\n",
      "3072    0   3   98304\n",
      "4096    0   4  131072\n",
      "5120    0   5  163840\n",
      "6144    0   6  196608\n",
      "7168    0   7  229376\n",
      "8192    0   8  262144\n",
      "9216    0   9  294912\n",
      "10240   0  10  327680\n",
      "11264   0  11  360448\n",
      "12288   0  12  393216\n",
      "13312   0  13  425984\n",
      "14336   0  14  458752\n",
      "15360   0  15  491520\n",
      "16384   0  16  524288\n",
      "17408   0  17  557056\n",
      "18432   0  18  589824\n",
      "19456   0  19  622592\n",
      "2050    1   2   65600\n",
      "3075    1   3   98400\n",
      "4100    1   4  131200\n",
      "5125    1   5  164000\n",
      "6150    1   6  196800\n",
      "7175    1   7  229600\n",
      "8200    1   8  262400\n",
      "9225    1   9  295200\n",
      "10250   1  10  328000\n",
      "11275   1  11  360800\n",
      "12300   1  12  393600\n",
      "13325   1  13  426400\n",
      "14350   1  14  459200\n",
      "15375   1  15  492000\n",
      "16400   1  16  524800\n",
      "17425   1  17  557600\n",
      "18450   1  18  590400\n",
      "19475   1  19  623200\n",
      "3078    2   3   98496\n",
      "4104    2   4  131328\n",
      "5130    2   5  164160\n",
      "6156    2   6  196992\n",
      "7182    2   7  229824\n",
      "8208    2   8  262656\n",
      "9234    2   9  295488\n",
      "10260   2  10  328320\n",
      "11286   2  11  361152\n",
      "12312   2  12  393984\n",
      "13338   2  13  426816\n",
      "14364   2  14  459648\n",
      "15390   2  15  492480\n",
      "16416   2  16  525312\n",
      "17442   2  17  558144\n",
      "18468   2  18  590976\n",
      "19494   2  19  623808\n",
      "4108    3   4  131456\n",
      "5135    3   5  164320\n",
      "6162    3   6  197184\n",
      "7189    3   7  230048\n",
      "8216    3   8  262912\n",
      "9243    3   9  295776\n",
      "10270   3  10  328640\n",
      "11297   3  11  361504\n",
      "12324   3  12  394368\n",
      "13351   3  13  427232\n",
      "14378   3  14  460096\n",
      "15405   3  15  492960\n",
      "16432   3  16  525824\n",
      "17459   3  17  558688\n",
      "18486   3  18  591552\n",
      "19513   3  19  624416\n",
      "5140    4   5  164480\n",
      "6168    4   6  197376\n",
      "7196    4   7  230272\n",
      "8224    4   8  263168\n",
      "9252    4   9  296064\n",
      "10280   4  10  328960\n",
      "11308   4  11  361856\n",
      "12336   4  12  394752\n",
      "13364   4  13  427648\n",
      "14392   4  14  460544\n",
      "15420   4  15  493440\n",
      "16448   4  16  526336\n",
      "17476   4  17  559232\n",
      "18504   4  18  592128\n",
      "19532   4  19  625024\n",
      "6174    5   6  197568\n",
      "7203    5   7  230496\n",
      "8232    5   8  263424\n",
      "9261    5   9  296352\n",
      "10290   5  10  329280\n",
      "11319   5  11  362208\n",
      "12348   5  12  395136\n",
      "13377   5  13  428064\n",
      "14406   5  14  460992\n",
      "15435   5  15  493920\n",
      "16464   5  16  526848\n",
      "17493   5  17  559776\n",
      "18522   5  18  592704\n",
      "19551   5  19  625632\n",
      "7210    6   7  230720\n",
      "8240    6   8  263680\n",
      "9270    6   9  296640\n",
      "10300   6  10  329600\n",
      "11330   6  11  362560\n",
      "12360   6  12  395520\n",
      "13390   6  13  428480\n",
      "14420   6  14  461440\n",
      "15450   6  15  494400\n",
      "16480   6  16  527360\n",
      "17510   6  17  560320\n",
      "18540   6  18  593280\n",
      "19570   6  19  626240\n",
      "8248    7   8  263936\n",
      "9279    7   9  296928\n",
      "10310   7  10  329920\n",
      "11341   7  11  362912\n",
      "12372   7  12  395904\n",
      "13403   7  13  428896\n",
      "14434   7  14  461888\n",
      "15465   7  15  494880\n",
      "16496   7  16  527872\n",
      "17527   7  17  560864\n",
      "18558   7  18  593856\n",
      "19589   7  19  626848\n",
      "9288    8   9  297216\n",
      "10320   8  10  330240\n",
      "11352   8  11  363264\n",
      "12384   8  12  396288\n",
      "13416   8  13  429312\n",
      "14448   8  14  462336\n",
      "15480   8  15  495360\n",
      "16512   8  16  528384\n",
      "17544   8  17  561408\n",
      "18576   8  18  594432\n",
      "19608   8  19  627456\n",
      "10330   9  10  330560\n",
      "11363   9  11  363616\n",
      "12396   9  12  396672\n",
      "13429   9  13  429728\n",
      "14462   9  14  462784\n",
      "15495   9  15  495840\n",
      "16528   9  16  528896\n",
      "17561   9  17  561952\n",
      "18594   9  18  595008\n",
      "19627   9  19  628064\n",
      "11374  10  11  363968\n",
      "12408  10  12  397056\n",
      "13442  10  13  430144\n",
      "14476  10  14  463232\n",
      "15510  10  15  496320\n",
      "16544  10  16  529408\n",
      "17578  10  17  562496\n",
      "18612  10  18  595584\n",
      "19646  10  19  628672\n",
      "12420  11  12  397440\n",
      "13455  11  13  430560\n",
      "14490  11  14  463680\n",
      "15525  11  15  496800\n",
      "16560  11  16  529920\n",
      "17595  11  17  563040\n",
      "18630  11  18  596160\n",
      "19665  11  19  629280\n",
      "13468  12  13  430976\n",
      "14504  12  14  464128\n",
      "15540  12  15  497280\n",
      "16576  12  16  530432\n",
      "17612  12  17  563584\n",
      "18648  12  18  596736\n",
      "19684  12  19  629888\n",
      "14518  13  14  464576\n",
      "15555  13  15  497760\n",
      "16592  13  16  530944\n",
      "17629  13  17  564128\n",
      "18666  13  18  597312\n",
      "19703  13  19  630496\n",
      "15570  14  15  498240\n",
      "16608  14  16  531456\n",
      "17646  14  17  564672\n",
      "18684  14  18  597888\n",
      "19722  14  19  631104\n",
      "16624  15  16  531968\n",
      "17663  15  17  565216\n",
      "18702  15  18  598464\n",
      "19741  15  19  631712\n",
      "17680  16  17  565760\n",
      "18720  16  18  599040\n",
      "19760  16  19  632320\n",
      "18738  17  18  599616\n",
      "19779  17  19  632928\n",
      "19798  18  19  633536\n",
      "Length of Hash Table is: 190\n"
     ]
    }
   ],
   "source": [
    "def map_hypothesis(vars, hash_key=1024, modulo_multiplier=32):\n",
    "\n",
    "\n",
    "    ## Put your code below. \n",
    "    print('VARS ARE HERE: 1000 rows with 20 columns')\n",
    "    print(vars)\n",
    "    print('VARS Rows')\n",
    "    print(len(vars))\n",
    "    print('SHAPE Rows')\n",
    "    print(vars.shape)\n",
    "    arr = [[], []]\n",
    "    arr_vals = []\n",
    "    hash_arr = []\n",
    "    arr = [[0,0],\n",
    "           [0,1],\n",
    "           [1,0],\n",
    "           [1,1],\n",
    "           [1,2]]\n",
    "    lenrow = len(arr)\n",
    "    lencol = len(arr[0])\n",
    "    ind = 0\n",
    "    ##for x in range(lenrow):\n",
    "    ##    for y in range(lencol):\n",
    "    ##        arr_vals[ind]= [(i),(j),np.NAN]\n",
    "    ##        ind += 1\n",
    "    \n",
    "    ##ncolumns = 20\n",
    "    ##nr.seed(234)\n",
    "    ##normal_vars = nr.normal(size=(1000,ncolumns))\n",
    "    \n",
    "    \n",
    "    \n",
    "    ## make the index or rows\n",
    "    df = pd.DataFrame(arr)\n",
    "    \n",
    "    ##df.index = hash_key * modulo_multiplier\n",
    "    df.columns = ['i','j']\n",
    "    print(df)\n",
    "    ##df.loc = pd.columns\n",
    "    ##print(df)\n",
    "    ncols = vars.shape[1]\n",
    "    ncols1 = len(vars[0])\n",
    "    ##ij_val = [[],[]]\n",
    "    j_val = []\n",
    "    hash_val = []\n",
    "    ncolumns = ncols\n",
    "    print('ncols-shape :' + str(ncols) + ' ncols1-len(vars[0]) :' + str(ncols1))\n",
    "    ##ncls = 20\n",
    "    ## J goes from 0 to 19\n",
    "    ind = 0\n",
    "    for i,j in combinations(range(ncols), 2):\n",
    "        \n",
    "        \n",
    "        ##ij_val[i,j] = vars[i,j]\n",
    "        \n",
    "        \n",
    "        ind += 1\n",
    "        \n",
    "        ##hash_function(i, j, hash_key=1024, modulo_multiplier=32):\n",
    "        ## The hash is actually the values of the columns\n",
    "        hash[(i,j)] = hash_function(i,j)\n",
    "        hash_arr.append(hash_function(i,j))\n",
    "        hash_val.append( hash[(i,j)]*modulo_multiplier)\n",
    "        newRow = (i,j,hash[i,j]*modulo_multiplier)\n",
    "        arr_vals.append(newRow)\n",
    "        ##arr[i,j] = np.nan\n",
    "        ##df.append(new_row, ignore_index = True)\n",
    "        ##arr[int(str(i)),int(str(j))] = np.nan\n",
    "        \n",
    "        ## Put your code below. \n",
    "        ## Compute the hash key and added the values to the \n",
    "        ## row of the data frame. \n",
    "    ##print(df.loc[:,'i','j'])\n",
    "    print (ind)\n",
    "    df_arr = pd.DataFrame(arr_vals)\n",
    "   \n",
    "    df_arr.columns = ['i','j', 'hash']\n",
    "    df_arr.loc[:,'hash'] = pd.DataFrame(hash_val)\n",
    "    combCount = 1\n",
    "    ##for x,y in combinations(range(ncols), 2):\n",
    "    ##    hash_arr.append(hash_function(x,y))\n",
    "    ##    print('COUNT: '+ str(combCount) + ':  hash[(' + str(x)+ ' , '+ str( y) + ')] : ' + str(hash_function(x,y)))\n",
    "    ##df_arr.loc[:,'hash'] = hash_arr\n",
    "    df_arr.index = hash_arr\n",
    "    print('DF_ARR')\n",
    "    print(df_arr)\n",
    "    return df_arr \n",
    "    ##return hash_table\n",
    "    ##return print(normal_vars)\n",
    "\n",
    "##normal_vars = nr.normal(size=(1000,ncolumns))\n",
    "print('normal_vars')\n",
    "print(normal_vars)\n",
    "hash_table = map_hypothesis(normal_vars)\n",
    "print('Length of Hash Table is: '+ str(len(hash_table)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The shuffle and reduce task\n",
    "\n",
    "Now that you have the keys for the pairwise combinations of the vectors it is time to perform the reduce process. The reduce process computes the pair-wise t-statistics and p-values. These statistical values are indexed by the keys of the pair of vectors. This process reduces the full vectors of values down to just two numbers for each pair of vectors. \n",
    "\n",
    "> **Exercise 1-3:** You will now create and apply the following code for the reduce process:   \n",
    "> 1. Create an empty data frame with columns, `i`, `j`, `t_statistic`, and `p_value`.    \n",
    "> 2. Using a for loop iterate over all possible (hashed) keys of the data frame. An if statement is used to test if these are valid values of the key, i. Use the [numpy.isnan](https://numpy.org/doc/stable/reference/generated/numpy.isnan.html) function for this test.  \n",
    "> 3. Extract the values of i and j from the input data frame. \n",
    "> 4. Using keys, compute the t-statistic and p-value using [scipy.stats import ttest_ind](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind.html).\n",
    "> 5. Append a row to the output data frame.\n",
    "> 6. Return the data frame, sorted in ascending order, using the [Pandas.DataFrame.sort_values](https://turned.pydata.org/docs/reference/api/pandas.DataFrame.sort_values.html) method and re-indexed using the [Pandas.DataFrame.reset_index](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.reset_index.html) method.    \n",
    "> 7. Execute your function and save the returned data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HASH TABLE\n",
      "Ttest_indResult(statistic=array([-2.25686203]), pvalue=array([0.02412432]))\n",
      "THIS IS THE data\n",
      "[(0, 1, 1024), (0, 2, 2048), (0, 3, 3072), (0, 4, 4096), (0, 5, 5120), (0, 6, 6144), (0, 7, 7168), (0, 8, 8192), (0, 9, 9216), (0, 10, 10240), (0, 11, 11264), (0, 12, 12288), (0, 13, 13312), (0, 14, 14336), (0, 15, 15360), (0, 16, 16384), (0, 17, 17408), (0, 18, 18432), (0, 19, 19456), (1, 2, 2050), (1, 3, 3075), (1, 4, 4100), (1, 5, 5125), (1, 6, 6150), (1, 7, 7175), (1, 8, 8200), (1, 9, 9225), (1, 10, 10250), (1, 11, 11275), (1, 12, 12300), (1, 13, 13325), (1, 14, 14350), (1, 15, 15375), (1, 16, 16400), (1, 17, 17425), (1, 18, 18450), (1, 19, 19475), (2, 3, 3078), (2, 4, 4104), (2, 5, 5130), (2, 6, 6156), (2, 7, 7182), (2, 8, 8208), (2, 9, 9234), (2, 10, 10260), (2, 11, 11286), (2, 12, 12312), (2, 13, 13338), (2, 14, 14364), (2, 15, 15390), (2, 16, 16416), (2, 17, 17442), (2, 18, 18468), (2, 19, 19494), (3, 4, 4108), (3, 5, 5135), (3, 6, 6162), (3, 7, 7189), (3, 8, 8216), (3, 9, 9243), (3, 10, 10270), (3, 11, 11297), (3, 12, 12324), (3, 13, 13351), (3, 14, 14378), (3, 15, 15405), (3, 16, 16432), (3, 17, 17459), (3, 18, 18486), (3, 19, 19513), (4, 5, 5140), (4, 6, 6168), (4, 7, 7196), (4, 8, 8224), (4, 9, 9252), (4, 10, 10280), (4, 11, 11308), (4, 12, 12336), (4, 13, 13364), (4, 14, 14392), (4, 15, 15420), (4, 16, 16448), (4, 17, 17476), (4, 18, 18504), (4, 19, 19532), (5, 6, 6174), (5, 7, 7203), (5, 8, 8232), (5, 9, 9261), (5, 10, 10290), (5, 11, 11319), (5, 12, 12348), (5, 13, 13377), (5, 14, 14406), (5, 15, 15435), (5, 16, 16464), (5, 17, 17493), (5, 18, 18522), (5, 19, 19551), (6, 7, 7210), (6, 8, 8240), (6, 9, 9270), (6, 10, 10300), (6, 11, 11330), (6, 12, 12360), (6, 13, 13390), (6, 14, 14420), (6, 15, 15450), (6, 16, 16480), (6, 17, 17510), (6, 18, 18540), (6, 19, 19570), (7, 8, 8248), (7, 9, 9279), (7, 10, 10310), (7, 11, 11341), (7, 12, 12372), (7, 13, 13403), (7, 14, 14434), (7, 15, 15465), (7, 16, 16496), (7, 17, 17527), (7, 18, 18558), (7, 19, 19589), (8, 9, 9288), (8, 10, 10320), (8, 11, 11352), (8, 12, 12384), (8, 13, 13416), (8, 14, 14448), (8, 15, 15480), (8, 16, 16512), (8, 17, 17544), (8, 18, 18576), (8, 19, 19608), (9, 10, 10330), (9, 11, 11363), (9, 12, 12396), (9, 13, 13429), (9, 14, 14462), (9, 15, 15495), (9, 16, 16528), (9, 17, 17561), (9, 18, 18594), (9, 19, 19627), (10, 11, 11374), (10, 12, 12408), (10, 13, 13442), (10, 14, 14476), (10, 15, 15510), (10, 16, 16544), (10, 17, 17578), (10, 18, 18612), (10, 19, 19646), (11, 12, 12420), (11, 13, 13455), (11, 14, 14490), (11, 15, 15525), (11, 16, 16560), (11, 17, 17595), (11, 18, 18630), (11, 19, 19665), (12, 13, 13468), (12, 14, 14504), (12, 15, 15540), (12, 16, 16576), (12, 17, 17612), (12, 18, 18648), (12, 19, 19684), (13, 14, 14518), (13, 15, 15555), (13, 16, 16592), (13, 17, 17629), (13, 18, 18666), (13, 19, 19703), (14, 15, 15570), (14, 16, 16608), (14, 17, 17646), (14, 18, 18684), (14, 19, 19722), (15, 16, 16624), (15, 17, 17663), (15, 18, 18702), (15, 19, 19741), (16, 17, 17680), (16, 18, 18720), (16, 19, 19760), (17, 18, 18738), (17, 19, 19779), (18, 19, 19798)]\n",
      "len(hash_table): 190\n",
      "len(test_results): 0\n",
      "len(tt_arr): 190\n",
      "len(pv_arr): 190\n",
      "len(sig_arr): 190\n",
      "Sig-Count: 22\n",
      "PRINT TEST_RESULTS\n",
      "      i   j   hash  t_statistic   p_value      sig\n",
      "0     0   1   1024    -3.227865  0.001267      SIG\n",
      "1     0   2   2048    -2.212194  0.027066      SIG\n",
      "2     0   3   3072    -2.321463  0.020362      SIG\n",
      "3     0   4   4096    -3.311203  0.000946      SIG\n",
      "4     0   5   5120    -2.872598  0.004114      SIG\n",
      "5     0   6   6144    -2.624430  0.008746      SIG\n",
      "6     0   7   7168    -2.681570  0.007388      SIG\n",
      "7     0   8   8192    -3.705426  0.000217      SIG\n",
      "8     0   9   9216    -1.680344  0.093047  Not-Sig\n",
      "9     0  10  10240    -2.462400  0.013885      SIG\n",
      "10    0  11  11264    -1.297003  0.194780  Not-Sig\n",
      "11    0  12  12288    -2.031263  0.042360      SIG\n",
      "12    0  13  13312    -2.903111  0.003735      SIG\n",
      "13    0  14  14336    -2.294894  0.021842      SIG\n",
      "14    0  15  15360    -2.291186  0.022056      SIG\n",
      "15    0  16  16384    -3.024120  0.002525      SIG\n",
      "16    0  17  17408    -3.992565  0.000068      SIG\n",
      "17    0  18  18432    -2.156647  0.031152      SIG\n",
      "18    0  19  19456    -2.279797  0.022725      SIG\n",
      "19    1   2   2050     1.008317  0.313424  Not-Sig\n",
      "20    1   3   3075     0.946950  0.343779  Not-Sig\n",
      "21    1   4   4100    -0.050450  0.959769  Not-Sig\n",
      "22    1   5   5125     0.351724  0.725082  Not-Sig\n",
      "23    1   6   6150     0.621446  0.534377  Not-Sig\n",
      "24    1   7   7175     0.577582  0.563611  Not-Sig\n",
      "25    1   8   8200    -0.465884  0.641349  Not-Sig\n",
      "26    1   9   9225     1.513680  0.130265  Not-Sig\n",
      "27    1  10  10250     0.752784  0.451668  Not-Sig\n",
      "28    1  11  11275     1.947443  0.051622  Not-Sig\n",
      "29    1  12  12300     1.176032  0.239722  Not-Sig\n",
      "30    1  13  13325     0.298015  0.765723  Not-Sig\n",
      "31    1  14  14350     0.936742  0.349005  Not-Sig\n",
      "32    1  15  15375     0.911799  0.361985  Not-Sig\n",
      "33    1  16  16400     0.220245  0.825703  Not-Sig\n",
      "34    1  17  17425    -0.681611  0.495564  Not-Sig\n",
      "35    1  18  18450     1.060706  0.288952  Not-Sig\n",
      "36    1  19  19475     0.966809  0.333757  Not-Sig\n",
      "37    2   3   3078    -0.075681  0.939681  Not-Sig\n",
      "38    2   4   4104    -1.068540  0.285406  Not-Sig\n",
      "39    2   5   5130    -0.656293  0.511711  Not-Sig\n",
      "40    2   6   6156    -0.393659  0.693875  Not-Sig\n",
      "41    2   7   7182    -0.441423  0.658954  Not-Sig\n",
      "42    2   8   8208    -1.476701  0.139913  Not-Sig\n",
      "43    2   9   9234     0.512248  0.608534  Not-Sig\n",
      "44    2  10  10260    -0.253359  0.800017  Not-Sig\n",
      "45    2  11  11286     0.929752  0.352612  Not-Sig\n",
      "46    2  12  12312     0.171335  0.863978  Not-Sig\n",
      "47    2  13  13338    -0.702898  0.482201  Not-Sig\n",
      "48    2  14  14364    -0.074774  0.940402  Not-Sig\n",
      "49    2  15  15390    -0.091011  0.927493  Not-Sig\n",
      "50    2  16  16416    -0.793516  0.427571  Not-Sig\n",
      "51    2  17  17442    -1.713348  0.086804  Not-Sig\n",
      "52    2  18  18468     0.053226  0.957557  Not-Sig\n",
      "53    2  19  19494    -0.049291  0.960692  Not-Sig\n",
      "54    3   4   4108    -1.007559  0.313788  Not-Sig\n",
      "55    3   5   5135    -0.589981  0.555270  Not-Sig\n",
      "56    3   6   6162    -0.323152  0.746614  Not-Sig\n",
      "57    3   7   7189    -0.371351  0.710415  Not-Sig\n",
      "58    3   8   8216    -1.421791  0.155243  Not-Sig\n",
      "59    3   9   9243     0.594678  0.552126  Not-Sig\n",
      "60    3  10  10270    -0.181457  0.856027  Not-Sig\n",
      "61    3  11  11297     1.019901  0.307899  Not-Sig\n",
      "62    3  12  12324     0.249178  0.803249  Not-Sig\n",
      "63    3  13  13351    -0.637654  0.523772  Not-Sig\n",
      "64    3  14  14378     0.000077  0.999939  Not-Sig\n",
      "65    3  15  15405    -0.017050  0.986399  Not-Sig\n",
      "66    3  16  16432    -0.728826  0.466193  Not-Sig\n",
      "67    3  17  17459    -1.661143  0.096842  Not-Sig\n",
      "68    3  18  18486     0.129604  0.896893  Not-Sig\n",
      "69    3  19  19513     0.026278  0.979038  Not-Sig\n",
      "70    4   5   5140     0.405542  0.685122  Not-Sig\n",
      "71    4   6   6168     0.678319  0.497648  Not-Sig\n",
      "72    4   7   7196     0.634264  0.525981  Not-Sig\n",
      "73    4   8   8224    -0.419777  0.674694  Not-Sig\n",
      "74    4   9   9252     1.578329  0.114648  Not-Sig\n",
      "75    4  10  10280     0.810369  0.417825  Not-Sig\n",
      "76    4  11  11308     2.017791  0.043747      SIG\n",
      "77    4  12  12336     1.237634  0.215997  Not-Sig\n",
      "78    4  13  13364     0.350916  0.725688  Not-Sig\n",
      "79    4  14  14392     0.996483  0.319136  Not-Sig\n",
      "80    4  15  15420     0.970702  0.331814  Not-Sig\n",
      "81    4  16  16448     0.273110  0.784797  Not-Sig\n",
      "82    4  17  17476    -0.636776  0.524344  Not-Sig\n",
      "83    4  18  18504     1.121382  0.262260  Not-Sig\n",
      "84    4  19  19532     1.027164  0.304468  Not-Sig\n",
      "85    5   6   6174     0.267185  0.789354  Not-Sig\n",
      "86    5   7   7203     0.221983  0.824350  Not-Sig\n",
      "87    5   8   8232    -0.818336  0.413263  Not-Sig\n",
      "88    5   9   9261     1.163893  0.244606  Not-Sig\n",
      "89    5  10  10290     0.401590  0.688029  Not-Sig\n",
      "90    5  11  11319     1.591878  0.111570  Not-Sig\n",
      "91    5  12  12348     0.825213  0.409349  Not-Sig\n",
      "92    5  13  13377    -0.051232  0.959146  Not-Sig\n",
      "93    5  14  14406     0.583630  0.559535  Not-Sig\n",
      "94    5  15  15435     0.561714  0.574374  Not-Sig\n",
      "95    5  16  16464    -0.133430  0.893867  Not-Sig\n",
      "96    5  17  17493    -1.041234  0.297893  Not-Sig\n",
      "97    5  18  18522     0.708956  0.478434  Not-Sig\n",
      "98    5  19  19551     0.612098  0.540542  Not-Sig\n",
      "99    6   7   7210    -0.046561  0.962868  Not-Sig\n",
      "100   6   8   8240    -1.091990  0.274969  Not-Sig\n",
      "101   6   9   9270     0.906621  0.364717  Not-Sig\n",
      "102   6  10  10300     0.137722  0.890474  Not-Sig\n",
      "103   6  11  11330     1.333657  0.182468  Not-Sig\n",
      "104   6  12  12360     0.564711  0.572334  Not-Sig\n",
      "105   6  13  13390    -0.316863  0.751380  Not-Sig\n",
      "106   6  14  14420     0.319647  0.749269  Not-Sig\n",
      "107   6  15  15450     0.299843  0.764328  Not-Sig\n",
      "108   6  16  16480    -0.403102  0.686917  Not-Sig\n",
      "109   6  17  17510    -1.322351  0.186203  Not-Sig\n",
      "110   6  18  18540     0.446913  0.654986  Not-Sig\n",
      "111   6  19  19570     0.347128  0.728532  Not-Sig\n",
      "112   7   8   8248    -1.049809  0.293933  Not-Sig\n",
      "113   7   9   9279     0.956016  0.339180  Not-Sig\n",
      "114   7  10  10310     0.184391  0.853725  Not-Sig\n",
      "115   7  11  11341     1.385648  0.166009  Not-Sig\n",
      "116   7  12  12372     0.612955  0.539976  Not-Sig\n",
      "117   7  13  13403    -0.272167  0.785522  Not-Sig\n",
      "118   7  14  14434     0.367282  0.713447  Not-Sig\n",
      "119   7  15  15465     0.346972  0.728649  Not-Sig\n",
      "120   7  16  16496    -0.358167  0.720256  Not-Sig\n",
      "121   7  17  17527    -1.280272  0.200598  Not-Sig\n",
      "122   7  18  18558     0.494842  0.620766  Not-Sig\n",
      "123   7  19  19589     0.395098  0.692813  Not-Sig\n",
      "124   8   9   9288     1.980137  0.047825      SIG\n",
      "125   8  10  10320     1.219527  0.222788  Not-Sig\n",
      "126   8  11  11352     2.422580  0.015499      SIG\n",
      "127   8  12  12384     1.643160  0.100507  Not-Sig\n",
      "128   8  13  13416     0.761235  0.446606  Not-Sig\n",
      "129   8  14  14448     1.406424  0.159754  Not-Sig\n",
      "130   8  15  15480     1.377399  0.168543  Not-Sig\n",
      "131   8  16  16512     0.689166  0.490799  Not-Sig\n",
      "132   8  17  17544    -0.206758  0.836220  Not-Sig\n",
      "133   8  18  18576     1.528834  0.126464  Not-Sig\n",
      "134   8  19  19608     1.438685  0.150397  Not-Sig\n",
      "135   9  10  10330    -0.762765  0.445694  Not-Sig\n",
      "136   9  11  11363     0.406133  0.684689  Not-Sig\n",
      "137   9  12  12396    -0.340246  0.733707  Not-Sig\n",
      "138   9  13  13429    -1.206633  0.227717  Not-Sig\n",
      "139   9  14  14462    -0.588132  0.556510  Not-Sig\n",
      "140   9  15  15495    -0.599836  0.548684  Not-Sig\n",
      "141   9  16  16528    -1.303046  0.192709  Not-Sig\n",
      "142   9  17  17561    -2.225359  0.026168      SIG\n",
      "143   9  18  18594    -0.458963  0.646311  Not-Sig\n",
      "144   9  19  19627    -0.565156  0.572031  Not-Sig\n",
      "145  10  11  11374     1.183391  0.236795  Not-Sig\n",
      "146  10  12  12408     0.423410  0.672042  Not-Sig\n",
      "147  10  13  13442    -0.449907  0.652826  Not-Sig\n",
      "148  10  14  14476     0.179555  0.857520  Not-Sig\n",
      "149  10  15  15510     0.161155  0.871987  Not-Sig\n",
      "150  10  16  16544    -0.537051  0.591292  Not-Sig\n",
      "151  10  17  17578    -1.450243  0.147148  Not-Sig\n",
      "152  10  18  18612     0.306258  0.759440  Not-Sig\n",
      "153  10  19  19646     0.206140  0.836703  Not-Sig\n",
      "154  11  12  12420    -0.753346  0.451331  Not-Sig\n",
      "155  11  13  13455    -1.632012  0.102835  Not-Sig\n",
      "156  11  14  14490    -1.008301  0.313432  Not-Sig\n",
      "157  11  15  15525    -1.016330  0.309595  Not-Sig\n",
      "158  11  16  16560    -1.735964  0.082724  Not-Sig\n",
      "159  11  17  17595    -2.682146  0.007375      SIG\n",
      "160  11  18  18630    -0.875224  0.381557  Not-Sig\n",
      "161  11  19  19665    -0.986980  0.323772  Not-Sig\n",
      "162  12  13  13468    -0.870473  0.384146  Not-Sig\n",
      "163  12  14  14504    -0.246382  0.805412  Not-Sig\n",
      "164  12  15  15540    -0.261079  0.794058  Not-Sig\n",
      "165  12  16  16576    -0.962894  0.335717  Not-Sig\n",
      "166  12  17  17612    -1.882288  0.059942  Not-Sig\n",
      "167  12  18  18648    -0.118159  0.905954  Not-Sig\n",
      "168  12  19  19684    -0.221774  0.824513  Not-Sig\n",
      "169  13  14  14518     0.630885  0.528188  Not-Sig\n",
      "170  13  15  15555     0.608759  0.542753  Not-Sig\n",
      "171  13  16  16592    -0.080965  0.935478  Not-Sig\n",
      "172  13  17  17629    -0.981247  0.326590  Not-Sig\n",
      "173  13  18  18666     0.755152  0.450247  Not-Sig\n",
      "174  13  19  19703     0.659339  0.509754  Not-Sig\n",
      "175  14  15  15570    -0.016940  0.986486  Not-Sig\n",
      "176  14  16  16608    -0.720882  0.471066  Not-Sig\n",
      "177  14  17  17646    -1.642459  0.100652  Not-Sig\n",
      "178  14  18  18684     0.128108  0.898077  Not-Sig\n",
      "179  14  19  19722     0.025909  0.979332  Not-Sig\n",
      "180  15  16  16624    -0.697624  0.485494  Not-Sig\n",
      "181  15  17  17663    -1.610502  0.107446  Not-Sig\n",
      "182  15  18  18702     0.143869  0.885619  Not-Sig\n",
      "183  15  19  19741     0.042698  0.965947  Not-Sig\n",
      "184  16  17  17680    -0.910770  0.362526  Not-Sig\n",
      "185  16  18  18720     0.846365  0.397451  Not-Sig\n",
      "186  16  19  19760     0.750158  0.453248  Not-Sig\n",
      "187  17  18  18738     1.766343  0.077491  Not-Sig\n",
      "188  17  19  19779     1.676635  0.093770  Not-Sig\n",
      "189  18  19  19798    -0.102890  0.918061  Not-Sig\n"
     ]
    }
   ],
   "source": [
    "def reduce_significance(hash_table, values):  \n",
    "    ## Create a data framreturn the results of the \n",
    "    ## the reduce process. The results are grouped by the first \n",
    "    ## index i. \n",
    "    print('HASH TABLE')\n",
    "    ##print(hash_table)\n",
    "   \n",
    "    ncols = hash_table.shape[1]\n",
    "    nrows = hash_table.shape[0]\n",
    "    ##tt =ttest_ind(normal_vars[:,0], nr.normal(loc = 0.01, size=(1000,1)))\n",
    "    tt =ttest_ind(normal_vars[:,0], nr.normal(loc = 0.01, size=(1000,1)))\n",
    "    print( tt)\n",
    "    ##test_results = pd.DataFrame(columns=['i','j','t_statistic','p_value'])\n",
    "    test_results = pd.DataFrame(columns=['i','j'])\n",
    "    ##print('TEST_RESULTS: ' + test_results)\n",
    "    ## As a substitute for a shuffle we will use a simple search \n",
    "    ## through the data frame  \n",
    "    count2 = 0\n",
    "    ##print(  hash_table.loc[:,:'j']) \n",
    "       ## print[hindex]\n",
    "    ## Put your code below. \n",
    "    extract_i = []\n",
    "    extract_j = []\n",
    "    ##for hash in range(hash_table.shape[0]): \n",
    "    ##    if not np.isnan(hash_table.iloc[hash,0]):\n",
    "    ##        extract_i.append(hash_table['i'][hash])\n",
    "    ##        extract_j.append(hash_table['j'][hash])\n",
    "            ##print(hash1)\n",
    "    ##        count2 += 1\n",
    "    i_arr = []\n",
    "    j_arr = []\n",
    "    data = []\n",
    "    hash_arr  = []\n",
    "    tt_arr = []\n",
    "    pv_arr = []\n",
    "    sig_arr = []\n",
    "    sig_count = 0\n",
    "    ## ^^^\n",
    "    ##print(' NCOLUMNS =' + str(ncolumns))\n",
    "    ## \n",
    "    for i,j in combinations(range(ncolumns), 2):\n",
    "        i_arr.append(i)\n",
    "        j_arr.append(j)\n",
    "        \n",
    "        hash_arr = hash_function(i,j)\n",
    "        \n",
    "        data.append((i,j,hash_function(i,j)))\n",
    "        \n",
    "        tt = ttest_ind(values[:,i] , values[:,j]).statistic\n",
    "        ##print(tt)\n",
    "        tt_arr.append(tt)\n",
    "        pv = ttest_ind(values[:,i] , values[:,j]).pvalue\n",
    "        \n",
    "        if pv <= 0.05:\n",
    "            sig_arr.append('SIG')\n",
    "            sig_count += 1\n",
    "        else:\n",
    "            sig_arr.append('Not-Sig')\n",
    "        pv_arr.append(pv)\n",
    "        ##print('SIG-COUNT: ' + str(sig_count))\n",
    "        ##data.append((i,j))\n",
    "        ##tt = ttest_ind(values.iloc[][i] , values.iloc[][j]).statistic\n",
    "        \n",
    "\n",
    "    ##print('PRINT h')\n",
    "    print('THIS IS THE data')\n",
    "    print(data)\n",
    "    #test_results = pd.DataFrame(data)\n",
    "    \n",
    "    for hash in range(hash_table.shape[0]): \n",
    "        if not np.isnan(hash_table.iloc[hash,0]):\n",
    "            h = 0\n",
    "            ##tt = ttest_ind(hash_table.iloc[hash][1] , hash_table.iloc[hash][2])\n",
    "            ##tt_arr.append(tt)\n",
    "            ##pv = ttest_ind(hash_table.iloc[hash][1] , hash_table.iloc[hash][2]).pvalue\n",
    "            ##pv_arr.append(pv)\n",
    "            \n",
    "    ##        extract_i.append(hash_table['i'][hash])\n",
    "    ##        extract_j.append(hash_table['j'][hash])\n",
    "    \n",
    "   ## for h in range(len(hash_table.index)):\n",
    "   ##     tt = ttest_ind(h['i'][h] , ttest_ind(h['j'][h]))\n",
    "   ##     tt_ar.append(tt)\n",
    "   ##    pv = ttest_ind(h['i'][h] , ttest_ind(h['j'][h])).pvalue\n",
    "   ##     pv_arr.append(pv)\n",
    "        ##print(h)\n",
    "        \n",
    "    \n",
    "    print('len(hash_table): ' + str(len(hash_table)) )  \n",
    "    print('len(test_results): ' + str(len(test_results)) ) \n",
    "    print('len(tt_arr): '+ str(len(tt_arr)))\n",
    "    print('len(pv_arr): '+ str(len(pv_arr)))\n",
    "    print('len(sig_arr): '+ str(len(sig_arr)))\n",
    "    print('Sig-Count: ' + str(sig_count))\n",
    "    test_results = pd.DataFrame(data)\n",
    "    test_results.columns=['i','j','hash']\n",
    "    test_results.loc[:,'t_statistic'] = tt_arr\n",
    "    test_results.loc[:,'p_value'] = pv_arr\n",
    "    test_results.loc[:,'sig'] = sig_arr\n",
    "    print('PRINT TEST_RESULTS')\n",
    "    print(test_results)\n",
    "    #test_results[:,'i'] = pd.DataFrame(hash_table.loc[:,'i'])\n",
    "    #df_j = pd.DataFrame(hash_table.loc[:,'j'])\n",
    "    #print('df_i : ' + df_i)\n",
    "    #print(df_j)\n",
    "    #print('BEFORE .values')\n",
    "    #print(df_i.values)\n",
    "    #df_i_1D = []\n",
    "    #df_j_1D = []\n",
    "    #for item in df_i.values:\n",
    "    #    df_i_1D.append(df_i.values[0])\n",
    "    #for itemj in df_j.values:\n",
    "    #    df_j_1D.append(df_i.values[0])\n",
    "    ##print('DTYPE: ' + np.dtype(df_i)\n",
    "    #test_results.loc[:,'i'] = df_i_1D\n",
    "    #test_results.loc[:,'j'] = df_j_1D\n",
    "    ##print('TEST RESULTS II: ' + test_results)\n",
    "    ## Given the i,j pairs we need to compute the t-statistic and the p-value.   \n",
    "    ## This is the reduce step, since for each i,j pair there is only \n",
    "    ## a t-statistic and a p-value. \n",
    "    ## Put your code below. \n",
    "           \n",
    "    ## Sort and return the results\n",
    "    #test_results.sort_values('p_value', axis=0, ascending=True# , inplace=True)\n",
    "    return test_results.sort_values('p_value', axis=0, ascending=True).reset_index(drop=True)        \n",
    "        \n",
    "\n",
    "test_stats = reduce_significance(hash_table, normal_vars)       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 8. In the cell below, create a filter for pair test cases which are significant and save these cases in a data frame. \n",
    "> 9. Print the number (len) of significant results.\n",
    "> 10. Print the rows with the significant test results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HASH TABLE\n",
      "Ttest_indResult(statistic=array([-3.42583047]), pvalue=array([0.00062532]))\n",
      "len(hash_table): 190\n",
      "len(test_results): 190\n",
      "len(tt_arr): 190\n",
      "len(pv_arr): 190\n",
      "len(sig_arr): 190\n",
      "Sig-Count: 22\n",
      "PRINT TEST_RESULTS\n",
      "      0   1      2  t_statistic   p_value      sig\n",
      "0     0   1   1024    -3.227865  0.001267      SIG\n",
      "1     0   2   2048    -2.212194  0.027066      SIG\n",
      "2     0   3   3072    -2.321463  0.020362      SIG\n",
      "3     0   4   4096    -3.311203  0.000946      SIG\n",
      "4     0   5   5120    -2.872598  0.004114      SIG\n",
      "5     0   6   6144    -2.624430  0.008746      SIG\n",
      "6     0   7   7168    -2.681570  0.007388      SIG\n",
      "7     0   8   8192    -3.705426  0.000217      SIG\n",
      "8     0   9   9216    -1.680344  0.093047  Not-Sig\n",
      "9     0  10  10240    -2.462400  0.013885      SIG\n",
      "10    0  11  11264    -1.297003  0.194780  Not-Sig\n",
      "11    0  12  12288    -2.031263  0.042360      SIG\n",
      "12    0  13  13312    -2.903111  0.003735      SIG\n",
      "13    0  14  14336    -2.294894  0.021842      SIG\n",
      "14    0  15  15360    -2.291186  0.022056      SIG\n",
      "15    0  16  16384    -3.024120  0.002525      SIG\n",
      "16    0  17  17408    -3.992565  0.000068      SIG\n",
      "17    0  18  18432    -2.156647  0.031152      SIG\n",
      "18    0  19  19456    -2.279797  0.022725      SIG\n",
      "19    1   2   2050     1.008317  0.313424  Not-Sig\n",
      "20    1   3   3075     0.946950  0.343779  Not-Sig\n",
      "21    1   4   4100    -0.050450  0.959769  Not-Sig\n",
      "22    1   5   5125     0.351724  0.725082  Not-Sig\n",
      "23    1   6   6150     0.621446  0.534377  Not-Sig\n",
      "24    1   7   7175     0.577582  0.563611  Not-Sig\n",
      "25    1   8   8200    -0.465884  0.641349  Not-Sig\n",
      "26    1   9   9225     1.513680  0.130265  Not-Sig\n",
      "27    1  10  10250     0.752784  0.451668  Not-Sig\n",
      "28    1  11  11275     1.947443  0.051622  Not-Sig\n",
      "29    1  12  12300     1.176032  0.239722  Not-Sig\n",
      "30    1  13  13325     0.298015  0.765723  Not-Sig\n",
      "31    1  14  14350     0.936742  0.349005  Not-Sig\n",
      "32    1  15  15375     0.911799  0.361985  Not-Sig\n",
      "33    1  16  16400     0.220245  0.825703  Not-Sig\n",
      "34    1  17  17425    -0.681611  0.495564  Not-Sig\n",
      "35    1  18  18450     1.060706  0.288952  Not-Sig\n",
      "36    1  19  19475     0.966809  0.333757  Not-Sig\n",
      "37    2   3   3078    -0.075681  0.939681  Not-Sig\n",
      "38    2   4   4104    -1.068540  0.285406  Not-Sig\n",
      "39    2   5   5130    -0.656293  0.511711  Not-Sig\n",
      "40    2   6   6156    -0.393659  0.693875  Not-Sig\n",
      "41    2   7   7182    -0.441423  0.658954  Not-Sig\n",
      "42    2   8   8208    -1.476701  0.139913  Not-Sig\n",
      "43    2   9   9234     0.512248  0.608534  Not-Sig\n",
      "44    2  10  10260    -0.253359  0.800017  Not-Sig\n",
      "45    2  11  11286     0.929752  0.352612  Not-Sig\n",
      "46    2  12  12312     0.171335  0.863978  Not-Sig\n",
      "47    2  13  13338    -0.702898  0.482201  Not-Sig\n",
      "48    2  14  14364    -0.074774  0.940402  Not-Sig\n",
      "49    2  15  15390    -0.091011  0.927493  Not-Sig\n",
      "50    2  16  16416    -0.793516  0.427571  Not-Sig\n",
      "51    2  17  17442    -1.713348  0.086804  Not-Sig\n",
      "52    2  18  18468     0.053226  0.957557  Not-Sig\n",
      "53    2  19  19494    -0.049291  0.960692  Not-Sig\n",
      "54    3   4   4108    -1.007559  0.313788  Not-Sig\n",
      "55    3   5   5135    -0.589981  0.555270  Not-Sig\n",
      "56    3   6   6162    -0.323152  0.746614  Not-Sig\n",
      "57    3   7   7189    -0.371351  0.710415  Not-Sig\n",
      "58    3   8   8216    -1.421791  0.155243  Not-Sig\n",
      "59    3   9   9243     0.594678  0.552126  Not-Sig\n",
      "60    3  10  10270    -0.181457  0.856027  Not-Sig\n",
      "61    3  11  11297     1.019901  0.307899  Not-Sig\n",
      "62    3  12  12324     0.249178  0.803249  Not-Sig\n",
      "63    3  13  13351    -0.637654  0.523772  Not-Sig\n",
      "64    3  14  14378     0.000077  0.999939  Not-Sig\n",
      "65    3  15  15405    -0.017050  0.986399  Not-Sig\n",
      "66    3  16  16432    -0.728826  0.466193  Not-Sig\n",
      "67    3  17  17459    -1.661143  0.096842  Not-Sig\n",
      "68    3  18  18486     0.129604  0.896893  Not-Sig\n",
      "69    3  19  19513     0.026278  0.979038  Not-Sig\n",
      "70    4   5   5140     0.405542  0.685122  Not-Sig\n",
      "71    4   6   6168     0.678319  0.497648  Not-Sig\n",
      "72    4   7   7196     0.634264  0.525981  Not-Sig\n",
      "73    4   8   8224    -0.419777  0.674694  Not-Sig\n",
      "74    4   9   9252     1.578329  0.114648  Not-Sig\n",
      "75    4  10  10280     0.810369  0.417825  Not-Sig\n",
      "76    4  11  11308     2.017791  0.043747      SIG\n",
      "77    4  12  12336     1.237634  0.215997  Not-Sig\n",
      "78    4  13  13364     0.350916  0.725688  Not-Sig\n",
      "79    4  14  14392     0.996483  0.319136  Not-Sig\n",
      "80    4  15  15420     0.970702  0.331814  Not-Sig\n",
      "81    4  16  16448     0.273110  0.784797  Not-Sig\n",
      "82    4  17  17476    -0.636776  0.524344  Not-Sig\n",
      "83    4  18  18504     1.121382  0.262260  Not-Sig\n",
      "84    4  19  19532     1.027164  0.304468  Not-Sig\n",
      "85    5   6   6174     0.267185  0.789354  Not-Sig\n",
      "86    5   7   7203     0.221983  0.824350  Not-Sig\n",
      "87    5   8   8232    -0.818336  0.413263  Not-Sig\n",
      "88    5   9   9261     1.163893  0.244606  Not-Sig\n",
      "89    5  10  10290     0.401590  0.688029  Not-Sig\n",
      "90    5  11  11319     1.591878  0.111570  Not-Sig\n",
      "91    5  12  12348     0.825213  0.409349  Not-Sig\n",
      "92    5  13  13377    -0.051232  0.959146  Not-Sig\n",
      "93    5  14  14406     0.583630  0.559535  Not-Sig\n",
      "94    5  15  15435     0.561714  0.574374  Not-Sig\n",
      "95    5  16  16464    -0.133430  0.893867  Not-Sig\n",
      "96    5  17  17493    -1.041234  0.297893  Not-Sig\n",
      "97    5  18  18522     0.708956  0.478434  Not-Sig\n",
      "98    5  19  19551     0.612098  0.540542  Not-Sig\n",
      "99    6   7   7210    -0.046561  0.962868  Not-Sig\n",
      "100   6   8   8240    -1.091990  0.274969  Not-Sig\n",
      "101   6   9   9270     0.906621  0.364717  Not-Sig\n",
      "102   6  10  10300     0.137722  0.890474  Not-Sig\n",
      "103   6  11  11330     1.333657  0.182468  Not-Sig\n",
      "104   6  12  12360     0.564711  0.572334  Not-Sig\n",
      "105   6  13  13390    -0.316863  0.751380  Not-Sig\n",
      "106   6  14  14420     0.319647  0.749269  Not-Sig\n",
      "107   6  15  15450     0.299843  0.764328  Not-Sig\n",
      "108   6  16  16480    -0.403102  0.686917  Not-Sig\n",
      "109   6  17  17510    -1.322351  0.186203  Not-Sig\n",
      "110   6  18  18540     0.446913  0.654986  Not-Sig\n",
      "111   6  19  19570     0.347128  0.728532  Not-Sig\n",
      "112   7   8   8248    -1.049809  0.293933  Not-Sig\n",
      "113   7   9   9279     0.956016  0.339180  Not-Sig\n",
      "114   7  10  10310     0.184391  0.853725  Not-Sig\n",
      "115   7  11  11341     1.385648  0.166009  Not-Sig\n",
      "116   7  12  12372     0.612955  0.539976  Not-Sig\n",
      "117   7  13  13403    -0.272167  0.785522  Not-Sig\n",
      "118   7  14  14434     0.367282  0.713447  Not-Sig\n",
      "119   7  15  15465     0.346972  0.728649  Not-Sig\n",
      "120   7  16  16496    -0.358167  0.720256  Not-Sig\n",
      "121   7  17  17527    -1.280272  0.200598  Not-Sig\n",
      "122   7  18  18558     0.494842  0.620766  Not-Sig\n",
      "123   7  19  19589     0.395098  0.692813  Not-Sig\n",
      "124   8   9   9288     1.980137  0.047825      SIG\n",
      "125   8  10  10320     1.219527  0.222788  Not-Sig\n",
      "126   8  11  11352     2.422580  0.015499      SIG\n",
      "127   8  12  12384     1.643160  0.100507  Not-Sig\n",
      "128   8  13  13416     0.761235  0.446606  Not-Sig\n",
      "129   8  14  14448     1.406424  0.159754  Not-Sig\n",
      "130   8  15  15480     1.377399  0.168543  Not-Sig\n",
      "131   8  16  16512     0.689166  0.490799  Not-Sig\n",
      "132   8  17  17544    -0.206758  0.836220  Not-Sig\n",
      "133   8  18  18576     1.528834  0.126464  Not-Sig\n",
      "134   8  19  19608     1.438685  0.150397  Not-Sig\n",
      "135   9  10  10330    -0.762765  0.445694  Not-Sig\n",
      "136   9  11  11363     0.406133  0.684689  Not-Sig\n",
      "137   9  12  12396    -0.340246  0.733707  Not-Sig\n",
      "138   9  13  13429    -1.206633  0.227717  Not-Sig\n",
      "139   9  14  14462    -0.588132  0.556510  Not-Sig\n",
      "140   9  15  15495    -0.599836  0.548684  Not-Sig\n",
      "141   9  16  16528    -1.303046  0.192709  Not-Sig\n",
      "142   9  17  17561    -2.225359  0.026168      SIG\n",
      "143   9  18  18594    -0.458963  0.646311  Not-Sig\n",
      "144   9  19  19627    -0.565156  0.572031  Not-Sig\n",
      "145  10  11  11374     1.183391  0.236795  Not-Sig\n",
      "146  10  12  12408     0.423410  0.672042  Not-Sig\n",
      "147  10  13  13442    -0.449907  0.652826  Not-Sig\n",
      "148  10  14  14476     0.179555  0.857520  Not-Sig\n",
      "149  10  15  15510     0.161155  0.871987  Not-Sig\n",
      "150  10  16  16544    -0.537051  0.591292  Not-Sig\n",
      "151  10  17  17578    -1.450243  0.147148  Not-Sig\n",
      "152  10  18  18612     0.306258  0.759440  Not-Sig\n",
      "153  10  19  19646     0.206140  0.836703  Not-Sig\n",
      "154  11  12  12420    -0.753346  0.451331  Not-Sig\n",
      "155  11  13  13455    -1.632012  0.102835  Not-Sig\n",
      "156  11  14  14490    -1.008301  0.313432  Not-Sig\n",
      "157  11  15  15525    -1.016330  0.309595  Not-Sig\n",
      "158  11  16  16560    -1.735964  0.082724  Not-Sig\n",
      "159  11  17  17595    -2.682146  0.007375      SIG\n",
      "160  11  18  18630    -0.875224  0.381557  Not-Sig\n",
      "161  11  19  19665    -0.986980  0.323772  Not-Sig\n",
      "162  12  13  13468    -0.870473  0.384146  Not-Sig\n",
      "163  12  14  14504    -0.246382  0.805412  Not-Sig\n",
      "164  12  15  15540    -0.261079  0.794058  Not-Sig\n",
      "165  12  16  16576    -0.962894  0.335717  Not-Sig\n",
      "166  12  17  17612    -1.882288  0.059942  Not-Sig\n",
      "167  12  18  18648    -0.118159  0.905954  Not-Sig\n",
      "168  12  19  19684    -0.221774  0.824513  Not-Sig\n",
      "169  13  14  14518     0.630885  0.528188  Not-Sig\n",
      "170  13  15  15555     0.608759  0.542753  Not-Sig\n",
      "171  13  16  16592    -0.080965  0.935478  Not-Sig\n",
      "172  13  17  17629    -0.981247  0.326590  Not-Sig\n",
      "173  13  18  18666     0.755152  0.450247  Not-Sig\n",
      "174  13  19  19703     0.659339  0.509754  Not-Sig\n",
      "175  14  15  15570    -0.016940  0.986486  Not-Sig\n",
      "176  14  16  16608    -0.720882  0.471066  Not-Sig\n",
      "177  14  17  17646    -1.642459  0.100652  Not-Sig\n",
      "178  14  18  18684     0.128108  0.898077  Not-Sig\n",
      "179  14  19  19722     0.025909  0.979332  Not-Sig\n",
      "180  15  16  16624    -0.697624  0.485494  Not-Sig\n",
      "181  15  17  17663    -1.610502  0.107446  Not-Sig\n",
      "182  15  18  18702     0.143869  0.885619  Not-Sig\n",
      "183  15  19  19741     0.042698  0.965947  Not-Sig\n",
      "184  16  17  17680    -0.910770  0.362526  Not-Sig\n",
      "185  16  18  18720     0.846365  0.397451  Not-Sig\n",
      "186  16  19  19760     0.750158  0.453248  Not-Sig\n",
      "187  17  18  18738     1.766343  0.077491  Not-Sig\n",
      "188  17  19  19779     1.676635  0.093770  Not-Sig\n",
      "189  18  19  19798    -0.102890  0.918061  Not-Sig\n",
      "      0   1      2  t_statistic   p_value      sig\n",
      "0     0  17  17408    -3.992565  0.000068      SIG\n",
      "1     0   8   8192    -3.705426  0.000217      SIG\n",
      "2     0   4   4096    -3.311203  0.000946      SIG\n",
      "3     0   1   1024    -3.227865  0.001267      SIG\n",
      "4     0  16  16384    -3.024120  0.002525      SIG\n",
      "5     0  13  13312    -2.903111  0.003735      SIG\n",
      "6     0   5   5120    -2.872598  0.004114      SIG\n",
      "7    11  17  17595    -2.682146  0.007375      SIG\n",
      "8     0   7   7168    -2.681570  0.007388      SIG\n",
      "9     0   6   6144    -2.624430  0.008746      SIG\n",
      "10    0  10  10240    -2.462400  0.013885      SIG\n",
      "11    8  11  11352     2.422580  0.015499      SIG\n",
      "12    0   3   3072    -2.321463  0.020362      SIG\n",
      "13    0  14  14336    -2.294894  0.021842      SIG\n",
      "14    0  15  15360    -2.291186  0.022056      SIG\n",
      "15    0  19  19456    -2.279797  0.022725      SIG\n",
      "16    9  17  17561    -2.225359  0.026168      SIG\n",
      "17    0   2   2048    -2.212194  0.027066      SIG\n",
      "18    0  18  18432    -2.156647  0.031152      SIG\n",
      "19    0  12  12288    -2.031263  0.042360      SIG\n",
      "20    4  11  11308     2.017791  0.043747      SIG\n",
      "21    8   9   9288     1.980137  0.047825      SIG\n",
      "22    1  11  11275     1.947443  0.051622  Not-Sig\n",
      "23   12  17  17612    -1.882288  0.059942  Not-Sig\n",
      "24   17  18  18738     1.766343  0.077491  Not-Sig\n",
      "25   11  16  16560    -1.735964  0.082724  Not-Sig\n",
      "26    2  17  17442    -1.713348  0.086804  Not-Sig\n",
      "27    0   9   9216    -1.680344  0.093047  Not-Sig\n",
      "28   17  19  19779     1.676635  0.093770  Not-Sig\n",
      "29    3  17  17459    -1.661143  0.096842  Not-Sig\n",
      "30    8  12  12384     1.643160  0.100507  Not-Sig\n",
      "31   14  17  17646    -1.642459  0.100652  Not-Sig\n",
      "32   11  13  13455    -1.632012  0.102835  Not-Sig\n",
      "33   15  17  17663    -1.610502  0.107446  Not-Sig\n",
      "34    5  11  11319     1.591878  0.111570  Not-Sig\n",
      "35    4   9   9252     1.578329  0.114648  Not-Sig\n",
      "36    8  18  18576     1.528834  0.126464  Not-Sig\n",
      "37    1   9   9225     1.513680  0.130265  Not-Sig\n",
      "38    2   8   8208    -1.476701  0.139913  Not-Sig\n",
      "39   10  17  17578    -1.450243  0.147148  Not-Sig\n",
      "40    8  19  19608     1.438685  0.150397  Not-Sig\n",
      "41    3   8   8216    -1.421791  0.155243  Not-Sig\n",
      "42    8  14  14448     1.406424  0.159754  Not-Sig\n",
      "43    7  11  11341     1.385648  0.166009  Not-Sig\n",
      "44    8  15  15480     1.377399  0.168543  Not-Sig\n",
      "45    6  11  11330     1.333657  0.182468  Not-Sig\n",
      "46    6  17  17510    -1.322351  0.186203  Not-Sig\n",
      "47    9  16  16528    -1.303046  0.192709  Not-Sig\n",
      "48    0  11  11264    -1.297003  0.194780  Not-Sig\n",
      "49    7  17  17527    -1.280272  0.200598  Not-Sig\n",
      "50    4  12  12336     1.237634  0.215997  Not-Sig\n",
      "51    8  10  10320     1.219527  0.222788  Not-Sig\n",
      "52    9  13  13429    -1.206633  0.227717  Not-Sig\n",
      "53   10  11  11374     1.183391  0.236795  Not-Sig\n",
      "54    1  12  12300     1.176032  0.239722  Not-Sig\n",
      "55    5   9   9261     1.163893  0.244606  Not-Sig\n",
      "56    4  18  18504     1.121382  0.262260  Not-Sig\n",
      "57    6   8   8240    -1.091990  0.274969  Not-Sig\n",
      "58    2   4   4104    -1.068540  0.285406  Not-Sig\n",
      "59    1  18  18450     1.060706  0.288952  Not-Sig\n",
      "60    7   8   8248    -1.049809  0.293933  Not-Sig\n",
      "61    5  17  17493    -1.041234  0.297893  Not-Sig\n",
      "62    4  19  19532     1.027164  0.304468  Not-Sig\n",
      "63    3  11  11297     1.019901  0.307899  Not-Sig\n",
      "64   11  15  15525    -1.016330  0.309595  Not-Sig\n",
      "65    1   2   2050     1.008317  0.313424  Not-Sig\n",
      "66   11  14  14490    -1.008301  0.313432  Not-Sig\n",
      "67    3   4   4108    -1.007559  0.313788  Not-Sig\n",
      "68    4  14  14392     0.996483  0.319136  Not-Sig\n",
      "69   11  19  19665    -0.986980  0.323772  Not-Sig\n",
      "70   13  17  17629    -0.981247  0.326590  Not-Sig\n",
      "71    4  15  15420     0.970702  0.331814  Not-Sig\n",
      "72    1  19  19475     0.966809  0.333757  Not-Sig\n",
      "73   12  16  16576    -0.962894  0.335717  Not-Sig\n",
      "74    7   9   9279     0.956016  0.339180  Not-Sig\n",
      "75    1   3   3075     0.946950  0.343779  Not-Sig\n",
      "76    1  14  14350     0.936742  0.349005  Not-Sig\n",
      "77    2  11  11286     0.929752  0.352612  Not-Sig\n",
      "78    1  15  15375     0.911799  0.361985  Not-Sig\n",
      "79   16  17  17680    -0.910770  0.362526  Not-Sig\n",
      "80    6   9   9270     0.906621  0.364717  Not-Sig\n",
      "81   11  18  18630    -0.875224  0.381557  Not-Sig\n",
      "82   12  13  13468    -0.870473  0.384146  Not-Sig\n",
      "83   16  18  18720     0.846365  0.397451  Not-Sig\n",
      "84    5  12  12348     0.825213  0.409349  Not-Sig\n",
      "85    5   8   8232    -0.818336  0.413263  Not-Sig\n",
      "86    4  10  10280     0.810369  0.417825  Not-Sig\n",
      "87    2  16  16416    -0.793516  0.427571  Not-Sig\n",
      "88    9  10  10330    -0.762765  0.445694  Not-Sig\n",
      "89    8  13  13416     0.761235  0.446606  Not-Sig\n",
      "90   13  18  18666     0.755152  0.450247  Not-Sig\n",
      "91   11  12  12420    -0.753346  0.451331  Not-Sig\n",
      "92    1  10  10250     0.752784  0.451668  Not-Sig\n",
      "93   16  19  19760     0.750158  0.453248  Not-Sig\n",
      "94    3  16  16432    -0.728826  0.466193  Not-Sig\n",
      "95   14  16  16608    -0.720882  0.471066  Not-Sig\n",
      "96    5  18  18522     0.708956  0.478434  Not-Sig\n",
      "97    2  13  13338    -0.702898  0.482201  Not-Sig\n",
      "98   15  16  16624    -0.697624  0.485494  Not-Sig\n",
      "99    8  16  16512     0.689166  0.490799  Not-Sig\n",
      "100   1  17  17425    -0.681611  0.495564  Not-Sig\n",
      "101   4   6   6168     0.678319  0.497648  Not-Sig\n",
      "102  13  19  19703     0.659339  0.509754  Not-Sig\n",
      "103   2   5   5130    -0.656293  0.511711  Not-Sig\n",
      "104   3  13  13351    -0.637654  0.523772  Not-Sig\n",
      "105   4  17  17476    -0.636776  0.524344  Not-Sig\n",
      "106   4   7   7196     0.634264  0.525981  Not-Sig\n",
      "107  13  14  14518     0.630885  0.528188  Not-Sig\n",
      "108   1   6   6150     0.621446  0.534377  Not-Sig\n",
      "109   7  12  12372     0.612955  0.539976  Not-Sig\n",
      "110   5  19  19551     0.612098  0.540542  Not-Sig\n",
      "111  13  15  15555     0.608759  0.542753  Not-Sig\n",
      "112   9  15  15495    -0.599836  0.548684  Not-Sig\n",
      "113   3   9   9243     0.594678  0.552126  Not-Sig\n",
      "114   3   5   5135    -0.589981  0.555270  Not-Sig\n",
      "115   9  14  14462    -0.588132  0.556510  Not-Sig\n",
      "116   5  14  14406     0.583630  0.559535  Not-Sig\n",
      "117   1   7   7175     0.577582  0.563611  Not-Sig\n",
      "118   9  19  19627    -0.565156  0.572031  Not-Sig\n",
      "119   6  12  12360     0.564711  0.572334  Not-Sig\n",
      "120   5  15  15435     0.561714  0.574374  Not-Sig\n",
      "121  10  16  16544    -0.537051  0.591292  Not-Sig\n",
      "122   2   9   9234     0.512248  0.608534  Not-Sig\n",
      "123   7  18  18558     0.494842  0.620766  Not-Sig\n",
      "124   1   8   8200    -0.465884  0.641349  Not-Sig\n",
      "125   9  18  18594    -0.458963  0.646311  Not-Sig\n",
      "126  10  13  13442    -0.449907  0.652826  Not-Sig\n",
      "127   6  18  18540     0.446913  0.654986  Not-Sig\n",
      "128   2   7   7182    -0.441423  0.658954  Not-Sig\n",
      "129  10  12  12408     0.423410  0.672042  Not-Sig\n",
      "130   4   8   8224    -0.419777  0.674694  Not-Sig\n",
      "131   9  11  11363     0.406133  0.684689  Not-Sig\n",
      "132   4   5   5140     0.405542  0.685122  Not-Sig\n",
      "133   6  16  16480    -0.403102  0.686917  Not-Sig\n",
      "134   5  10  10290     0.401590  0.688029  Not-Sig\n",
      "135   7  19  19589     0.395098  0.692813  Not-Sig\n",
      "136   2   6   6156    -0.393659  0.693875  Not-Sig\n",
      "137   3   7   7189    -0.371351  0.710415  Not-Sig\n",
      "138   7  14  14434     0.367282  0.713447  Not-Sig\n",
      "139   7  16  16496    -0.358167  0.720256  Not-Sig\n",
      "140   1   5   5125     0.351724  0.725082  Not-Sig\n",
      "141   4  13  13364     0.350916  0.725688  Not-Sig\n",
      "142   6  19  19570     0.347128  0.728532  Not-Sig\n",
      "143   7  15  15465     0.346972  0.728649  Not-Sig\n",
      "144   9  12  12396    -0.340246  0.733707  Not-Sig\n",
      "145   3   6   6162    -0.323152  0.746614  Not-Sig\n",
      "146   6  14  14420     0.319647  0.749269  Not-Sig\n",
      "147   6  13  13390    -0.316863  0.751380  Not-Sig\n",
      "148  10  18  18612     0.306258  0.759440  Not-Sig\n",
      "149   6  15  15450     0.299843  0.764328  Not-Sig\n",
      "150   1  13  13325     0.298015  0.765723  Not-Sig\n",
      "151   4  16  16448     0.273110  0.784797  Not-Sig\n",
      "152   7  13  13403    -0.272167  0.785522  Not-Sig\n",
      "153   5   6   6174     0.267185  0.789354  Not-Sig\n",
      "154  12  15  15540    -0.261079  0.794058  Not-Sig\n",
      "155   2  10  10260    -0.253359  0.800017  Not-Sig\n",
      "156   3  12  12324     0.249178  0.803249  Not-Sig\n",
      "157  12  14  14504    -0.246382  0.805412  Not-Sig\n",
      "158   5   7   7203     0.221983  0.824350  Not-Sig\n",
      "159  12  19  19684    -0.221774  0.824513  Not-Sig\n",
      "160   1  16  16400     0.220245  0.825703  Not-Sig\n",
      "161   8  17  17544    -0.206758  0.836220  Not-Sig\n",
      "162  10  19  19646     0.206140  0.836703  Not-Sig\n",
      "163   7  10  10310     0.184391  0.853725  Not-Sig\n",
      "164   3  10  10270    -0.181457  0.856027  Not-Sig\n",
      "165  10  14  14476     0.179555  0.857520  Not-Sig\n",
      "166   2  12  12312     0.171335  0.863978  Not-Sig\n",
      "167  10  15  15510     0.161155  0.871987  Not-Sig\n",
      "168  15  18  18702     0.143869  0.885619  Not-Sig\n",
      "169   6  10  10300     0.137722  0.890474  Not-Sig\n",
      "170   5  16  16464    -0.133430  0.893867  Not-Sig\n",
      "171   3  18  18486     0.129604  0.896893  Not-Sig\n",
      "172  14  18  18684     0.128108  0.898077  Not-Sig\n",
      "173  12  18  18648    -0.118159  0.905954  Not-Sig\n",
      "174  18  19  19798    -0.102890  0.918061  Not-Sig\n",
      "175   2  15  15390    -0.091011  0.927493  Not-Sig\n",
      "176  13  16  16592    -0.080965  0.935478  Not-Sig\n",
      "177   2   3   3078    -0.075681  0.939681  Not-Sig\n",
      "178   2  14  14364    -0.074774  0.940402  Not-Sig\n",
      "179   2  18  18468     0.053226  0.957557  Not-Sig\n",
      "180   5  13  13377    -0.051232  0.959146  Not-Sig\n",
      "181   1   4   4100    -0.050450  0.959769  Not-Sig\n",
      "182   2  19  19494    -0.049291  0.960692  Not-Sig\n",
      "183   6   7   7210    -0.046561  0.962868  Not-Sig\n",
      "184  15  19  19741     0.042698  0.965947  Not-Sig\n",
      "185   3  19  19513     0.026278  0.979038  Not-Sig\n",
      "186  14  19  19722     0.025909  0.979332  Not-Sig\n",
      "187   3  15  15405    -0.017050  0.986399  Not-Sig\n",
      "188  14  15  15570    -0.016940  0.986486  Not-Sig\n",
      "189   3  14  14378     0.000077  0.999939  Not-Sig\n",
      "     0   1      2  t_statistic   p_value  sig\n",
      "0    0  17  17408    -3.992565  0.000068  SIG\n",
      "1    0   8   8192    -3.705426  0.000217  SIG\n",
      "2    0   4   4096    -3.311203  0.000946  SIG\n",
      "3    0   1   1024    -3.227865  0.001267  SIG\n",
      "4    0  16  16384    -3.024120  0.002525  SIG\n",
      "5    0  13  13312    -2.903111  0.003735  SIG\n",
      "6    0   5   5120    -2.872598  0.004114  SIG\n",
      "7   11  17  17595    -2.682146  0.007375  SIG\n",
      "8    0   7   7168    -2.681570  0.007388  SIG\n",
      "9    0   6   6144    -2.624430  0.008746  SIG\n",
      "10   0  10  10240    -2.462400  0.013885  SIG\n",
      "11   8  11  11352     2.422580  0.015499  SIG\n",
      "12   0   3   3072    -2.321463  0.020362  SIG\n",
      "13   0  14  14336    -2.294894  0.021842  SIG\n",
      "14   0  15  15360    -2.291186  0.022056  SIG\n",
      "15   0  19  19456    -2.279797  0.022725  SIG\n",
      "16   9  17  17561    -2.225359  0.026168  SIG\n",
      "17   0   2   2048    -2.212194  0.027066  SIG\n",
      "18   0  18  18432    -2.156647  0.031152  SIG\n",
      "19   0  12  12288    -2.031263  0.042360  SIG\n",
      "20   4  11  11308     2.017791  0.043747  SIG\n",
      "21   8   9   9288     1.980137  0.047825  SIG\n",
      "    0   1      2  t_statistic   p_value  sig\n",
      "0   0  17  17408    -3.992565  0.000068  SIG\n",
      "1   0   8   8192    -3.705426  0.000217  SIG\n",
      "2   0   4   4096    -3.311203  0.000946  SIG\n",
      "3   0   1   1024    -3.227865  0.001267  SIG\n",
      "4   0  16  16384    -3.024120  0.002525  SIG\n",
      "5   0  13  13312    -2.903111  0.003735  SIG\n",
      "6   0   5   5120    -2.872598  0.004114  SIG\n",
      "7  11  17  17595    -2.682146  0.007375  SIG\n",
      "8   0   7   7168    -2.681570  0.007388  SIG\n",
      "9   0   6   6144    -2.624430  0.008746  SIG\n",
      "9. The length of the Significant Results are: 22\n",
      "10. The Significant Test Results are as Follows:\n",
      "     0   1      2  t_statistic   p_value  sig\n",
      "0    0  17  17408    -3.992565  0.000068  SIG\n",
      "1    0   8   8192    -3.705426  0.000217  SIG\n",
      "2    0   4   4096    -3.311203  0.000946  SIG\n",
      "3    0   1   1024    -3.227865  0.001267  SIG\n",
      "4    0  16  16384    -3.024120  0.002525  SIG\n",
      "5    0  13  13312    -2.903111  0.003735  SIG\n",
      "6    0   5   5120    -2.872598  0.004114  SIG\n",
      "7   11  17  17595    -2.682146  0.007375  SIG\n",
      "8    0   7   7168    -2.681570  0.007388  SIG\n",
      "9    0   6   6144    -2.624430  0.008746  SIG\n",
      "10   0  10  10240    -2.462400  0.013885  SIG\n",
      "11   8  11  11352     2.422580  0.015499  SIG\n",
      "12   0   3   3072    -2.321463  0.020362  SIG\n",
      "13   0  14  14336    -2.294894  0.021842  SIG\n",
      "14   0  15  15360    -2.291186  0.022056  SIG\n",
      "15   0  19  19456    -2.279797  0.022725  SIG\n",
      "16   9  17  17561    -2.225359  0.026168  SIG\n",
      "17   0   2   2048    -2.212194  0.027066  SIG\n",
      "18   0  18  18432    -2.156647  0.031152  SIG\n",
      "19   0  12  12288    -2.031263  0.042360  SIG\n",
      "20   4  11  11308     2.017791  0.043747  SIG\n",
      "21   8   9   9288     1.980137  0.047825  SIG\n"
     ]
    }
   ],
   "source": [
    "significance_level = 0.05\n",
    "test_stats = reduce_significance(hash_table, normal_vars) \n",
    "## Put your code below. \n",
    "print(test_stats)\n",
    "sig_filter_arr = test_stats.loc[test_stats['sig']=='SIG']\n",
    "df_sig = pd.DataFrame(sig_filter_arr)\n",
    "print(df_sig)\n",
    "print(test_stats.head(10))\n",
    "\n",
    "print('9. The length of the Significant Results are: ' + str(test_stats.loc[test_stats['sig']=='SIG'].shape[0]))\n",
    "\n",
    "\n",
    "print('10. The Significant Test Results are as Follows:')\n",
    "print(test_stats.loc[test_stats['sig']=='SIG'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Notice the large number of apparently significant tests. Answer the following questions:  \n",
    "> 1. Is the number of false positive cases higher than expected?    \n",
    "> 2. Examine which of the iid Normal vectors contribute to the false positive results. Are there vectors which contribute multiple times?   \n",
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes. There seems to be an abundant number of possible False Positives. Column vector '0' seems to contribute to alot of these Significant cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonferroni correction  \n",
    "\n",
    "Several adjustments to the multiple comparisons problem have been proposed. In Dunn published a method know as the **Bonferroni correction** in 1961. The Bonferroni correction is a widely used method to reduce the false positive rate of hypothesis tests.  The adjustment is simple:\n",
    "$$\\alpha_b = \\frac{\\alpha}{m}\\\\\n",
    "with\\\\ \n",
    "m =\\ number\\ of\\ groups$$\n",
    "\n",
    "Can the Bonferroni correction help? Yes, by greatly increasing the confidence level required for a statistically significant result. The problem with the Bonferroni correction is the reduction in power as the  grows smaller. For big data problems with large numbers of groups, this issue can be especially serious. \n",
    "\n",
    "**Exercise 1-4:** You will now apply the Bonferroni correction to the iid Normal vectors. To do so, you will compute the Bonferroni threshold and the apply it to the p-values:   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bon_nu_alpha: 0.0025\n",
      "HASH TABLE\n",
      "Ttest_indResult(statistic=array([-3.02902532]), pvalue=array([0.00248487]))\n",
      "Sig-Count: 4\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-de8287f80a52>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[0mbon_nu_alpha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'bon_nu_alpha: '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbon_nu_alpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m \u001b[0mbon_test_stats\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreduce_bon_significance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhash_table\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormal_vars\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbon_nu_alpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;31m##print(bon_test_stats.loc[])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;31m##test_stats = reduce_significance(hash_table, normal_vars)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-31-de8287f80a52>\u001b[0m in \u001b[0;36mreduce_bon_significance\u001b[1;34m(hash_table, values, bon_alpha)\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[0mtest_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[0mtest_results\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'i'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'j'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'hash'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m     \u001b[0mtest_result\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'hash'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m     \u001b[0mtest_results\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m't_statistic'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtt_arr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[0mtest_results\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'p_value'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpv_arr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_result' is not defined"
     ]
    }
   ],
   "source": [
    "## Exercise 1-4        \n",
    "## m = number of groups\n",
    "## alpha = confidence = 0.05\n",
    "\n",
    "def reduce_bon_significance(hash_table, values,bon_alpha):  \n",
    "    ## Create a data framreturn the results of the \n",
    "    ## the reduce process. The results are grouped by the first \n",
    "    ## index i. \n",
    "    print('HASH TABLE')\n",
    "    ##print(hash_table)\n",
    "   \n",
    "    ncols = hash_table.shape[1]\n",
    "    nrows = hash_table.shape[0]\n",
    "\n",
    "    tt =ttest_ind(normal_vars[:,0], nr.normal(loc = 0.01, size=(1000,1)))\n",
    "    print( tt)\n",
    "\n",
    "    test_results = pd.DataFrame(columns=['i','j'])\n",
    "\n",
    "    count2 = 0\n",
    "\n",
    "    extract_i = []\n",
    "    extract_j = []\n",
    "    ##        count2 += 1\n",
    "    i_arr = []\n",
    "    j_arr = []\n",
    "    data = []\n",
    "    hash_arr  = []\n",
    "    tt_arr = []\n",
    "    pv_arr = []\n",
    "    sig_arr = []\n",
    "    sig_count = 0\n",
    "    ## ^^^\n",
    "    ##print(' NCOLUMNS =' + str(ncolumns))\n",
    "    ## \n",
    "    for i,j in combinations(range(ncolumns), 2):\n",
    "        i_arr.append(i)\n",
    "        j_arr.append(j)\n",
    "        \n",
    "        hash_arr = hash_function(i,j)\n",
    "        \n",
    "        data.append((i,j,hash_function(i,j)))\n",
    "        \n",
    "        tt = ttest_ind(values[:,i] , values[:,j]).statistic\n",
    "        ##print(tt)\n",
    "        tt_arr.append(tt)\n",
    "        pv = ttest_ind(values[:,i] , values[:,j]).pvalue\n",
    "        \n",
    "        if pv <= bon_alpha:\n",
    "            sig_arr.append('SIG')\n",
    "            sig_count += 1\n",
    "        else:\n",
    "            sig_arr.append('Not-Sig')\n",
    "        pv_arr.append(pv)\n",
    " \n",
    "    #test_results = pd.DataFrame(data)\n",
    "    for hash in range(hash_table.shape[0]): \n",
    "        if not np.isnan(hash_table.iloc[hash,0]):\n",
    "            h = 0\n",
    "     \n",
    "    \n",
    "    #print('len(hash_table): ' + str(len(hash_table)) )  \n",
    "    #print('len(test_results): ' + str(len(test_results)) ) \n",
    "    #print('len(tt_arr): '+ str(len(tt_arr)))\n",
    "    #print('len(pv_arr): '+ str(len(pv_arr)))\n",
    "    #print('len(sig_arr): '+ str(len(sig_arr)))\n",
    "    print('Sig-Count: ' + str(sig_count))\n",
    "    test_results = pd.DataFrame(data)\n",
    "    test_results.columns=['i','j','hash']\n",
    "    test_result.set_index('hash')\n",
    "    test_results.loc[:,'t_statistic'] = tt_arr\n",
    "    test_results.loc[:,'p_value'] = pv_arr\n",
    "    test_results.loc[:,'sig'] = sig_arr\n",
    "    print('PRINT TEST_RESULTS')\n",
    "    print(test_results)\n",
    "   #test_results.sort_values('p_value', axis=0, ascending=True# , inplace=True)\n",
    "    return test_results.sort_values('p_value', axis=0, ascending=True).reset_index(drop=True)        \n",
    "        \n",
    "alpha = .05\n",
    "m = 20\n",
    "bon_nu_alpha = alpha/m\n",
    "print('bon_nu_alpha: ' + str(bon_nu_alpha))\n",
    "bon_test_stats = reduce_bon_significance(hash_table, normal_vars,bon_nu_alpha) \n",
    "##print(bon_test_stats.loc[])\n",
    "##test_stats = reduce_significance(hash_table, normal_vars)\n",
    "bon_test_stats.index('hash')\n",
    "bon_test_stats.loc[bon_test_stats['sig'] == 'SIG']\n",
    "##print(test_stats.loc[test_stats['sig']=='SIG'])\n",
    "#print(df_bon_mt.loc[df_bon_mt['sig']=='SIG'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Even with the Bonferroni correction we have some false significance tests, if only just barely!    \n",
    "> **End of exercise.**\n",
    "\n",
    "But, can we detect small effect with Bonferroni correction, as this method significantly reduces power of tests? Execute the code in the cell below, which compares a standard Normal to a Normal with a small mean (effect size), to find out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr.seed(567)\n",
    "print(significance_bonferroni)\n",
    "ttest_ind(normal_vars[:,0], nr.normal(loc = 0.01, size=(1000,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the Bonferroni correction, this difference in means would not be found significant. This illustrates the downside of the correction, which may prevent detection of significant effects, while still finding false significance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## False Discovery Rate Control Methods \n",
    "\n",
    "We have seen the potential pitfalls of multiple hypothesis testing. Further, we have seen that a simple approach to **false discovery rate (FDR) control** is not effective. You will now apply more sophisticated FDR control methods to control the FDR. \n",
    "\n",
    "Inflammatory bowel disease is an auto immune disease that is characterized by chronic inflammation in the digestive tract. In 2020, there were around 2.5 million people with inflammatory bowel disease in the United States. It is estimated that the prevalence of IBD among U.S. population will rise to around 3.5 million by 2030.There are two forms of IBD: Ulcerative Colitis (UC) and Crohn’s disease (CD). \n",
    "\n",
    "The specific problem we will explore is to determine which genes lead to expression of a certain disease. In this example, there are gene expression data for 97 patients. Some of these patients have ulcerative colitis and others have Crohn's disease, which are believed to be genetically inherited.    \n",
    "\n",
    "One approach to this problem is to perform hypothesis tests on the expression of the genes between patients with the two conditions. Since there are over 10,000 genes there is considerable chance for false discovery. Therefore, careful application of FDR control is required.\n",
    "\n",
    "To continue with the example, execute the code in the cell below to load the data and print the dimensionality of the data frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_data = pd.read_csv('../data/ColonDiseaseGeneData-Cleaned.csv')\n",
    "print(gene_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are data from 97 patients for 10,497 genes. A large number of hypothesis tests are required!     \n",
    "\n",
    "Execute the code in the cell below to view the first 5 columns of the data frame, which includes the expression of the first 4 genes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " print(gene_data.iloc[:,:5])      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Holm's method\n",
    "\n",
    "You will apply two FDR control methods to these data.These methods attempt to conod trol the FDR while not being overly conservative like the Bonferronic correction. The first of these Holm's method.    \n",
    "\n",
    "The Holm's method operates on the ordered set of p-values, $D = \\{ p_{(1)}, p_{(2)}, p_{(3)}, \\ldots, p_{(n)} \\}$. The threshold for the $ith$ p-value, $p(i) is:  \n",
    "\n",
    "$$p(i) \\le Threshold(Holm's) = \\frac{\\alpha}{N - i + 1}$$\n",
    "\n",
    "For example: for the 10th ordered p-value with 1,000 total tests (genes) and significance level of 0.05, the cutoff is:   \n",
    "\n",
    "$$p(10) \\le \\frac{0.05}{1000 - 10 + 1} = 0.00005045$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map process  \n",
    "\n",
    "> **Exercise 01-4:** To start the processing of these data you will first create and execute code for a map process. The map process groups the data by the patient's disease into data frame, ulcerative, crohns. The keys for each of these key-value pairs are the gene identifier. Notice that one key is all that is needed in this case. Now do the following to create and execute a function, `map_gene`:   \n",
    "> 1. Create a logical mask and group the values by `Disease State` into two data frames.\n",
    "> 2. Return the transpose of the two data frames, removing the `Disease State` values. The result of this operation should be data frames with gene expressions in the columns and the gene identifier as the row index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_gene(gene_data):  \n",
    "    ## First, separate the columns by disease type  \n",
    "    ## Put your code below. \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "ulcerative, crohns = map_gene(gene_data)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 3. Execute the code in the cells below to display the heads of these data frames and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ulcerative.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crohns.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce process \n",
    "\n",
    "> **Exercise 01-5:** With the key-value pairs organized by disease state, it is time to create and execute code of a reduce process. The reduce process will compute the pairwise t-statistics and p-values for each gene and return the sorted results. Specifically, your `gene_test` with arguments of the two mapped data frames will do the following:   \n",
    "> 1. Create an empty data frame with columns gene, t_statistics, and p-value.\n",
    "> 2. A for loop iterates over the keys of either of the data frames.  \n",
    "> 3. Compute the t-statistic and p-value for the gene (key).\n",
    "> 4. Append the results to the data frame.   \n",
    "> 5. Sort the results data frame, inplace, into ascending order.\n",
    "> 6. Return the resulting data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def gene_test(ulcerative, crohns):  \n",
    "    test_results = pd.DataFrame(columns=['gene','t_statistic','p_value'])\n",
    "    ## Put your code below. \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return test_results.set_index('gene')\n",
    "    \n",
    "gene_statistics = gene_test(ulcerative, crohns)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Significance of results \n",
    "\n",
    "With the gene data reduced to the t-test statistics, you will now determine the significance of these tests. It is important to understand that scientists believe that expression of a disease, like Corhn's, is only in a small number of genes.  \n",
    "\n",
    "> **Exercise 01-6:** As a first step in understanding the gene expression significance complete and execute the code in the cell below to find the number of 'significant' genes using the simple single hypothesis test cutoff criteria.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "significance_level =0.05\n",
    "## Put your code below. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Does this large number of 'statistically significant' results appear credible, given that only a few genes are thought to have significant expression for this disease?    \n",
    "> **End of exercise.**\n",
    "\n",
    "> **Exercise 01-7:** We have already seen that the Bonferroni correction is a rather conservative approach to testing the significance of large numbers of hypotheses. You will now use the Bonferroni correction to test the significance of the gene expression, by completing the code in the cell below.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code below. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The foregoing result seems reasonable, but is it too conservative?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 01-08:** You will now apply the Holms method to determining significance of the gene expression test results. In the cell below complete the `holms_significance` function with arguments of the results data frame and the significance level. This function does the following:  \n",
    "> 1. Find the number of test results and compute the numerator used for the cutoff calculation. \n",
    "> 2. Compute the vector of thresholds using the Holms formula. Use the Python `range`function to get the values of the index i. But, keep in mind that range produces a zero-indexed iterator, and the algorithm needs a one-indexed list.  Use the [numpy.divide](https://numpy.org/doc/stable/reference/generated/numpy.divide.html) function to perform the vector divide. Save these threshold values in a data frame in a 'holms_threshold' column.   \n",
    "> 3. Using the threshold values compute a logical vector and save it in a column names 'significance' in the data frame.\n",
    "> 4. Return the data frame.\n",
    "> Finally, execute the function and save the results in a data frame. Then find the length of the subset where the 'significance' value is True. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def holms_significance(test_results, significance):\n",
    "    ## First compute the thresholds for each of the ordered tests\n",
    "    ## Put your code below. \n",
    "\n",
    "    \n",
    "    \n",
    "    ## Now we test the significance of the ordered p-values \n",
    "\n",
    "    return test_results\n",
    "\n",
    "holms_results = holms_significance(gene_statistics, significance_level)    \n",
    "len(holms_results.loc[holms_results.loc[:,'significant'],:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Despite the general properties that the Holm's method is considered less conservative than the Bonferroni correction the results agree in this case. Does this agreement give you some confidence in the result and why?  \n",
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can visualize the results of the Holm's method test. The plot has two key elements:  \n",
    "1. Plot the curve of the p-values vs. the order number, i. The line is color coded by significance or not.\n",
    "2. Plot the threshold line. This line is straight since the threshold is a linear function of i."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_significance(results, threshold):\n",
    "    results['number'] = range(len(results))\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    sns.lineplot(x='number',y=threshold, data=results, ax=ax, color='black', linewidth=0.5)\n",
    "    sns.scatterplot(x='number',y='p_value', hue='significant', data=results, s=3, ax=ax)\n",
    "    ax.set_title('Significance of gene expression')\n",
    "    ax.set_xlabel('Gene number')\n",
    "    ax.set_ylabel('p-value')\n",
    "    \n",
    "plot_significance(holms_results.iloc[:500,:], 'holms_threshold')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the following about this plot:  \n",
    "1. The p-value significance line crosses the threshold point at an apparent break point.   \n",
    "2. The significant p-values are all very small since there are so many tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benamini-Hochberg FDR Control \n",
    "\n",
    "The Benamini-Hochberg FDR control algorithm is another way to control false discoveries. Stat with an ordered set of $n$ p-values, $D = \\{ p_{(1)}, p_{(2)}, p_{(3)}, \\ldots, p_{(n)} \\}$ we define a false discovery rate, $q$:\n",
    "\n",
    "$$FDR(D) \\le q$$\n",
    "\n",
    "The cutoff threshold for the ith p-value is then:\n",
    "$$p_{(i)} \\le Threshold(D_q) = \\frac{q}{n} i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 01-9:** In this exercise you will apply the Benamini-Hochberg FDR control algorithm for testing the significance of the gene expressions. The `BH_significance` function is quite similar to the Holm's method function you have already created. Given the large number of genes you must use a low false discovery rate, $0.001$, or 1 out of 1,000. \n",
    "> Execute your function, saving the result. Then print the number of significant cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BH_significance(test_results, false_discovery_tollerance):\n",
    "    ## First compute the thresholds for each of the ordered tests\n",
    "    ## Put your code below. \n",
    "\n",
    "    \n",
    "    \n",
    "    ## Now we test the significance of the ordered p-values \n",
    "\n",
    "    return test_results\n",
    "\n",
    "BH_results = BH_significance(gene_statistics, 0.001)    \n",
    "len(BH_results.loc[BH_results.loc[:,'significant'],:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The result is similar to the first two FDR control methods. Given the false discovery parameter of 0.0001 do you think this is a reasonable result? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, execute the code in the cell below and examine the resulting plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_significance(BH_results.iloc[:500,:], 'bh_threshold')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 01-10**: Bonus question. Compare the plot above to the foregoing plot for Holm's method. Are the breaks in slope of the p-value curve at the crossing point with the threshold values reasonable in both cases? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Copyright 2021, Stephen F. Elston. All rights reserved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
