{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 01 - Pitfalls in Data Mining     \n",
    "## CSCI E-96\n",
    "\n",
    "The goal of data mining is to find important relationships in large complex datasets. These dataset typically contain a large number of variables. The **high-dimensional** nature of the data leads to some commonly encountered pitfalls which lead to incorrect inferences.   \n",
    "\n",
    "In this assignment you will gain a bit of experience with three important concepts in data mining:  \n",
    "\n",
    "1. **False Discovery Rate Control:** The goal of data mining is to find important relationships in large complex datasets. These dataset typically contain a large number of variables. The **high-dimensional** nature of the data leads to some commonly encountered pitfalls which lead to incorrect inferences. A related problem is cutting off a large-scale analysis when a desired relationship is 'found'. This practice of **p-value mining** often leads to unwarranted inferences. You will apply false discovery rate (FDR) control methods to address this problem.   \n",
    "2. **Key-Value Pairs:** Large scale data is typically managed using key-value (KV) pairs. The exercises in this assignment give you some experience working with KV pair data management.  \n",
    "3. **Map and Reduce Processes:** Much of large scale data mining requires use of a split-apply-combine approach. The data is split into manageable chunks, analytic transformations are applied, and the result combined or aggregated. A commonly used class of a split-apply-combine algorithm is MapReduce. \n",
    "\n",
    "In order to keep the scope of this assignment manageable, you will use limited versions of KV pair management and MapReduce. Specifically, you will use common Python tools to implement these concepts rather than dedicated large scale analytic platforms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Hypothesis Tesing\n",
    "\n",
    "Testing multiple hypothesis in high-dimensional data can be problematic. Exhaustively testing all pairwise relationships between variables in a data set is a commonly used, but generally misleading from of **multiple comparisons**. The chance of finding false significance, using such a **data dredging** approach, can be surprisingly high. \n",
    "\n",
    "In this exercise you will perform multiple comparisons on only 20 **identically distributed independent (iid)** variables. Ideally, such tests should not find significant relationships, but the actual result is quite different. \n",
    "\n",
    "To get started, execute the code in the cell below to load the required packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "from scipy.stats import ttest_ind, f_oneway\n",
    "from itertools import product, combinations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you will apply a t-test to all pairwise combinations of identical Normally distributed variables. In this case, we will create a data set with 20 iid Normal distributions of 1000 samples each. Execute the code in the cell below to find this data and display the mean and variance of each variable.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The means of the columns are\n",
      " [-1.16191649e-01  2.80829317e-02 -1.78516419e-02 -1.44691489e-02\n",
      "  3.03718152e-02  1.20007442e-02 -9.58845606e-05  1.98662580e-03\n",
      "  4.94154934e-02 -4.11640866e-02 -6.32977862e-03 -5.93868192e-02\n",
      " -2.56373595e-02  1.43568791e-02 -1.44725765e-02 -1.37023955e-02\n",
      "  1.80622439e-02  5.87029691e-02 -2.02650514e-02 -1.56346106e-02]\n",
      "\n",
      "The variances of the columns are\n",
      " [0.94834508 1.04744241 1.0258018  0.96977571 1.0089001  1.04113864\n",
      " 1.00657222 0.99192594 1.04713487 1.04329434 1.04023108 0.96791346\n",
      " 1.03706907 1.07179865 1.01431404 1.05060289 1.02054329 0.9686211\n",
      " 1.02810287 0.99521555]\n"
     ]
    }
   ],
   "source": [
    "ncolumns = 20\n",
    "nr.seed(234)\n",
    "normal_vars = nr.normal(size=(1000,ncolumns))\n",
    "print('The means of the columns are\\n', np.mean(normal_vars, axis = 0))\n",
    "print('\\nThe variances of the columns are\\n', np.var(normal_vars, axis = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that means and variances are close to 0.0 and 1.0 respectively. As expected, there is not much difference between these variables.\n",
    "\n",
    "How many of these t-tests will show **significance** at the 0.05 cut-off level? There are 380 pairwise combinations, so we expect to find a number of falsely significant test results at this level. To find out, complete and execute the code in the cell below to filter the test results and print those that show significance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a hash \n",
    "\n",
    "The goal of this exercise is to compute pairwise hypothesis tests of the differences in means for each of the iid Normal vectors. As an intermediate step you will create a Pandas data frame using a hash of the keys of the vectors. The data frame will contain the **key-value**, $(K,V)$, pairs. Each key must represent an index for the two vectors used to compute the test statistic. The keys will then be used to index the results of these hypothesis tests. \n",
    "\n",
    "The question is, how can we create a hash from the keys for the pair of vectors? In this case to we will use a simple, but far from optimal hash. For the two vector indicies $i, j$, for some key and modulo, $m$, we will compute the hash as:  \n",
    "\n",
    "$$h(i,j) = (i + key*j) mod m$$\n",
    "\n",
    "> **Computational Note:** The Pandas data frame is an efficient and reasonably scalable **hash table**. The hash function used depends on the type of the key; integer, string, etc. The resulting dictionary of key-value pairs, $(K,V)$, can therefore be access in far less than linear time, often about $O(log(N))$.  \n",
    "\n",
    "If you are not familiar with Python dictionaries you can find a short tutorial [here](https://www.tutorialspoint.com/python_data_structure/python_hash_table.htm), as well as many other places on the web."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 1-1:** Given that our space of vectors is actually quite small, just 20, we do not need a sophisticated and scalable hash function. This hashed key will then be used to store and retrieve the values using a Python dictionary, in about $O(log(N))$ time.     \n",
    "\n",
    "> In this exercise you will test a simple hash function and its inverse. Examine the code below and notice that the hash function encodes the two indexes into a single integer by simple additional and multiplication. The modulo operation limits the size of the hash table. However, to keep things simple you will not need to implement any hash collision resolution mechanism. As a result, the size of the table is set much larger than required.  \n",
    "\n",
    "> To test this hash, do the following:    \n",
    "> 1. Create a function called hash function to compute the hash. The arguments to the function are $i$ and $j$, the `hash\\_key` and the `modulo\\_multiplier`. The defaults of the arguments are $hash\\_key=1024$ and $modulo\\_multiplier=32$. The modulo number is $hash\\_key * modulo\\_multiplier$, e.g. $modulo = 32,768$. The multiplier is the ratio of expected values stored, $n$, to the number of unique hash keys, $m$, e.g. the ratio $m/n$.\n",
    "> 2. Using the Python [ittertools.combinations](https://docs.python.org/3/library/itertools.html#itertools.combinations) function create all unique pairwise combinations of indexes i and j. The arguments to this function are the indexes to the iid Normal vectors. The iterator is `range(ncolumns)` choose 2, since these comparisons are pairwise.    \n",
    "> 3. Within this loop call the hash with the values of $i$ and $j$ as arguments.   \n",
    "> 3. On a single line print the following; the values of i and j, the hash key value, but only if $i \\le 6$. The restriction is to keep the printed output shorter.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COUNT: 1:  hash[(0 , 1)] : 0\n",
      "COUNT: 2:  hash[(0 , 2)] : 0\n",
      "COUNT: 3:  hash[(0 , 3)] : 0\n",
      "COUNT: 4:  hash[(0 , 4)] : 0\n",
      "COUNT: 5:  hash[(0 , 5)] : 0\n",
      "COUNT: 6:  hash[(0 , 6)] : 0\n",
      "COUNT: 7:  hash[(0 , 7)] : 0\n",
      "COUNT: 8:  hash[(0 , 8)] : 0\n",
      "COUNT: 9:  hash[(0 , 9)] : 0\n",
      "COUNT: 10:  hash[(0 , 10)] : 0\n",
      "COUNT: 11:  hash[(0 , 11)] : 0\n",
      "COUNT: 12:  hash[(0 , 12)] : 0\n",
      "COUNT: 13:  hash[(0 , 13)] : 0\n",
      "COUNT: 14:  hash[(0 , 14)] : 0\n",
      "COUNT: 15:  hash[(0 , 15)] : 0\n",
      "COUNT: 16:  hash[(0 , 16)] : 0\n",
      "COUNT: 17:  hash[(0 , 17)] : 0\n",
      "COUNT: 18:  hash[(0 , 18)] : 0\n",
      "COUNT: 19:  hash[(0 , 19)] : 0\n",
      "COUNT: 20:  hash[(1 , 2)] : 2\n",
      "COUNT: 21:  hash[(1 , 3)] : 3\n",
      "COUNT: 22:  hash[(1 , 4)] : 4\n",
      "COUNT: 23:  hash[(1 , 5)] : 5\n",
      "COUNT: 24:  hash[(1 , 6)] : 6\n",
      "COUNT: 25:  hash[(1 , 7)] : 7\n",
      "COUNT: 26:  hash[(1 , 8)] : 8\n",
      "COUNT: 27:  hash[(1 , 9)] : 9\n",
      "COUNT: 28:  hash[(1 , 10)] : 10\n",
      "COUNT: 29:  hash[(1 , 11)] : 11\n",
      "COUNT: 30:  hash[(1 , 12)] : 12\n",
      "COUNT: 31:  hash[(1 , 13)] : 13\n",
      "COUNT: 32:  hash[(1 , 14)] : 14\n",
      "COUNT: 33:  hash[(1 , 15)] : 15\n",
      "COUNT: 34:  hash[(1 , 16)] : 16\n",
      "COUNT: 35:  hash[(1 , 17)] : 17\n",
      "COUNT: 36:  hash[(1 , 18)] : 18\n",
      "COUNT: 37:  hash[(1 , 19)] : 19\n",
      "COUNT: 38:  hash[(2 , 3)] : 6\n",
      "COUNT: 39:  hash[(2 , 4)] : 8\n",
      "COUNT: 40:  hash[(2 , 5)] : 10\n",
      "COUNT: 41:  hash[(2 , 6)] : 12\n",
      "COUNT: 42:  hash[(2 , 7)] : 14\n",
      "COUNT: 43:  hash[(2 , 8)] : 16\n",
      "COUNT: 44:  hash[(2 , 9)] : 18\n",
      "COUNT: 45:  hash[(2 , 10)] : 20\n",
      "COUNT: 46:  hash[(2 , 11)] : 22\n",
      "COUNT: 47:  hash[(2 , 12)] : 24\n",
      "COUNT: 48:  hash[(2 , 13)] : 26\n",
      "COUNT: 49:  hash[(2 , 14)] : 28\n",
      "COUNT: 50:  hash[(2 , 15)] : 30\n",
      "COUNT: 51:  hash[(2 , 16)] : 0\n",
      "COUNT: 52:  hash[(2 , 17)] : 2\n",
      "COUNT: 53:  hash[(2 , 18)] : 4\n",
      "COUNT: 54:  hash[(2 , 19)] : 6\n",
      "COUNT: 55:  hash[(3 , 4)] : 12\n",
      "COUNT: 56:  hash[(3 , 5)] : 15\n",
      "COUNT: 57:  hash[(3 , 6)] : 18\n",
      "COUNT: 58:  hash[(3 , 7)] : 21\n",
      "COUNT: 59:  hash[(3 , 8)] : 24\n",
      "COUNT: 60:  hash[(3 , 9)] : 27\n",
      "COUNT: 61:  hash[(3 , 10)] : 30\n",
      "COUNT: 62:  hash[(3 , 11)] : 1\n",
      "COUNT: 63:  hash[(3 , 12)] : 4\n",
      "COUNT: 64:  hash[(3 , 13)] : 7\n",
      "COUNT: 65:  hash[(3 , 14)] : 10\n",
      "COUNT: 66:  hash[(3 , 15)] : 13\n",
      "COUNT: 67:  hash[(3 , 16)] : 16\n",
      "COUNT: 68:  hash[(3 , 17)] : 19\n",
      "COUNT: 69:  hash[(3 , 18)] : 22\n",
      "COUNT: 70:  hash[(3 , 19)] : 25\n",
      "COUNT: 71:  hash[(4 , 5)] : 20\n",
      "COUNT: 72:  hash[(4 , 6)] : 24\n",
      "COUNT: 73:  hash[(4 , 7)] : 28\n",
      "COUNT: 74:  hash[(4 , 8)] : 0\n",
      "COUNT: 75:  hash[(4 , 9)] : 4\n",
      "COUNT: 76:  hash[(4 , 10)] : 8\n",
      "COUNT: 77:  hash[(4 , 11)] : 12\n",
      "COUNT: 78:  hash[(4 , 12)] : 16\n",
      "COUNT: 79:  hash[(4 , 13)] : 20\n",
      "COUNT: 80:  hash[(4 , 14)] : 24\n",
      "COUNT: 81:  hash[(4 , 15)] : 28\n",
      "COUNT: 82:  hash[(4 , 16)] : 0\n",
      "COUNT: 83:  hash[(4 , 17)] : 4\n",
      "COUNT: 84:  hash[(4 , 18)] : 8\n",
      "COUNT: 85:  hash[(4 , 19)] : 12\n",
      "COUNT: 86:  hash[(5 , 6)] : 30\n",
      "COUNT: 87:  hash[(5 , 7)] : 3\n",
      "COUNT: 88:  hash[(5 , 8)] : 8\n",
      "COUNT: 89:  hash[(5 , 9)] : 13\n",
      "COUNT: 90:  hash[(5 , 10)] : 18\n",
      "COUNT: 91:  hash[(5 , 11)] : 23\n",
      "COUNT: 92:  hash[(5 , 12)] : 28\n",
      "COUNT: 93:  hash[(5 , 13)] : 1\n",
      "COUNT: 94:  hash[(5 , 14)] : 6\n",
      "COUNT: 95:  hash[(5 , 15)] : 11\n",
      "COUNT: 96:  hash[(5 , 16)] : 16\n",
      "COUNT: 97:  hash[(5 , 17)] : 21\n",
      "COUNT: 98:  hash[(5 , 18)] : 26\n",
      "COUNT: 99:  hash[(5 , 19)] : 31\n",
      "COUNT: 100:  hash[(6 , 7)] : 10\n",
      "COUNT: 101:  hash[(6 , 8)] : 16\n",
      "COUNT: 102:  hash[(6 , 9)] : 22\n",
      "COUNT: 103:  hash[(6 , 10)] : 28\n",
      "COUNT: 104:  hash[(6 , 11)] : 2\n",
      "COUNT: 105:  hash[(6 , 12)] : 8\n",
      "COUNT: 106:  hash[(6 , 13)] : 14\n",
      "COUNT: 107:  hash[(6 , 14)] : 20\n",
      "COUNT: 108:  hash[(6 , 15)] : 26\n",
      "COUNT: 109:  hash[(6 , 16)] : 0\n",
      "COUNT: 110:  hash[(6 , 17)] : 6\n",
      "COUNT: 111:  hash[(6 , 18)] : 12\n",
      "COUNT: 112:  hash[(6 , 19)] : 18\n",
      "COUNT: 113:  hash[(7 , 8)] : 24\n",
      "COUNT: 114:  hash[(7 , 9)] : 31\n",
      "COUNT: 115:  hash[(7 , 10)] : 6\n",
      "COUNT: 116:  hash[(7 , 11)] : 13\n",
      "COUNT: 117:  hash[(7 , 12)] : 20\n",
      "COUNT: 118:  hash[(7 , 13)] : 27\n",
      "COUNT: 119:  hash[(7 , 14)] : 2\n",
      "COUNT: 120:  hash[(7 , 15)] : 9\n",
      "COUNT: 121:  hash[(7 , 16)] : 16\n",
      "COUNT: 122:  hash[(7 , 17)] : 23\n",
      "COUNT: 123:  hash[(7 , 18)] : 30\n",
      "COUNT: 124:  hash[(7 , 19)] : 5\n",
      "COUNT: 125:  hash[(8 , 9)] : 8\n",
      "COUNT: 126:  hash[(8 , 10)] : 16\n",
      "COUNT: 127:  hash[(8 , 11)] : 24\n",
      "COUNT: 128:  hash[(8 , 12)] : 0\n",
      "COUNT: 129:  hash[(8 , 13)] : 8\n",
      "COUNT: 130:  hash[(8 , 14)] : 16\n",
      "COUNT: 131:  hash[(8 , 15)] : 24\n",
      "COUNT: 132:  hash[(8 , 16)] : 0\n",
      "COUNT: 133:  hash[(8 , 17)] : 8\n",
      "COUNT: 134:  hash[(8 , 18)] : 16\n",
      "COUNT: 135:  hash[(8 , 19)] : 24\n",
      "COUNT: 136:  hash[(9 , 10)] : 26\n",
      "COUNT: 137:  hash[(9 , 11)] : 3\n",
      "COUNT: 138:  hash[(9 , 12)] : 12\n",
      "COUNT: 139:  hash[(9 , 13)] : 21\n",
      "COUNT: 140:  hash[(9 , 14)] : 30\n",
      "COUNT: 141:  hash[(9 , 15)] : 7\n",
      "COUNT: 142:  hash[(9 , 16)] : 16\n",
      "COUNT: 143:  hash[(9 , 17)] : 25\n",
      "COUNT: 144:  hash[(9 , 18)] : 2\n",
      "COUNT: 145:  hash[(9 , 19)] : 11\n",
      "COUNT: 146:  hash[(10 , 11)] : 14\n",
      "COUNT: 147:  hash[(10 , 12)] : 24\n",
      "COUNT: 148:  hash[(10 , 13)] : 2\n",
      "COUNT: 149:  hash[(10 , 14)] : 12\n",
      "COUNT: 150:  hash[(10 , 15)] : 22\n",
      "COUNT: 151:  hash[(10 , 16)] : 0\n",
      "COUNT: 152:  hash[(10 , 17)] : 10\n",
      "COUNT: 153:  hash[(10 , 18)] : 20\n",
      "COUNT: 154:  hash[(10 , 19)] : 30\n",
      "COUNT: 155:  hash[(11 , 12)] : 4\n",
      "COUNT: 156:  hash[(11 , 13)] : 15\n",
      "COUNT: 157:  hash[(11 , 14)] : 26\n",
      "COUNT: 158:  hash[(11 , 15)] : 5\n",
      "COUNT: 159:  hash[(11 , 16)] : 16\n",
      "COUNT: 160:  hash[(11 , 17)] : 27\n",
      "COUNT: 161:  hash[(11 , 18)] : 6\n",
      "COUNT: 162:  hash[(11 , 19)] : 17\n",
      "COUNT: 163:  hash[(12 , 13)] : 28\n",
      "COUNT: 164:  hash[(12 , 14)] : 8\n",
      "COUNT: 165:  hash[(12 , 15)] : 20\n",
      "COUNT: 166:  hash[(12 , 16)] : 0\n",
      "COUNT: 167:  hash[(12 , 17)] : 12\n",
      "COUNT: 168:  hash[(12 , 18)] : 24\n",
      "COUNT: 169:  hash[(12 , 19)] : 4\n",
      "COUNT: 170:  hash[(13 , 14)] : 22\n",
      "COUNT: 171:  hash[(13 , 15)] : 3\n",
      "COUNT: 172:  hash[(13 , 16)] : 16\n",
      "COUNT: 173:  hash[(13 , 17)] : 29\n",
      "COUNT: 174:  hash[(13 , 18)] : 10\n",
      "COUNT: 175:  hash[(13 , 19)] : 23\n",
      "COUNT: 176:  hash[(14 , 15)] : 18\n",
      "COUNT: 177:  hash[(14 , 16)] : 0\n",
      "COUNT: 178:  hash[(14 , 17)] : 14\n",
      "COUNT: 179:  hash[(14 , 18)] : 28\n",
      "COUNT: 180:  hash[(14 , 19)] : 10\n",
      "COUNT: 181:  hash[(15 , 16)] : 16\n",
      "COUNT: 182:  hash[(15 , 17)] : 31\n",
      "COUNT: 183:  hash[(15 , 18)] : 14\n",
      "COUNT: 184:  hash[(15 , 19)] : 29\n",
      "COUNT: 185:  hash[(16 , 17)] : 16\n",
      "COUNT: 186:  hash[(16 , 18)] : 0\n",
      "COUNT: 187:  hash[(16 , 19)] : 16\n",
      "COUNT: 188:  hash[(17 , 18)] : 18\n",
      "COUNT: 189:  hash[(17 , 19)] : 3\n",
      "COUNT: 190:  hash[(18 , 19)] : 22\n",
      " The Total Number of Combinations is: 190\n",
      " The Total Number of unique instances is: 32\n"
     ]
    }
   ],
   "source": [
    "def hash_function(i, j, hash_key=1024, modulo_multiplier=32):\n",
    "    ## Put your code below. \n",
    "    return ((i + hash_key) * j) % modulo_multiplier\n",
    "\n",
    "count =0\n",
    "hash = {}\n",
    "harr = []\n",
    "##h[(i,j)] = ((i + hash_key) * j ) % modulo_multiplier\n",
    "\n",
    "for i,j in combinations(range(ncolumns), 2):\n",
    "    ## if i <= 6: \n",
    "        \n",
    "        ##print( ' Count = ' + str(count))\n",
    "        hash[(i,j)] = hash_function(i,j)\n",
    "        harr.append( hash_function(i,j))\n",
    "\n",
    "        ##print('i = ' + str(i) + '  j = ' + str(j) + '   hash = ' + str(hash) + '     Count hash = ' + str(count+1))\n",
    "        count += 1\n",
    "        combCount = 1\n",
    "for i,j in combinations(range(ncolumns),2):        \n",
    "    print('COUNT: '+ str(combCount) + ':  hash[(' + str(i)+ ' , '+ str( j) + ')] : ' + str(hash[i,j]))\n",
    "    combCount += 1\n",
    "    \n",
    "print(' The Total Number of Combinations is: ' + str(count))\n",
    "Count1 = 0  \n",
    "harr_u = np.unique(harr)\n",
    "for u in harr_u:\n",
    "    \n",
    "    ##print('harr_unique(' + str(Count1) + '): ' + str(u) + ' COUNT: '+ str(Count1+1))\n",
    "    Count1 += 1\n",
    "## print (harr_u)\n",
    "## print('harr_unique(' + str(Count1-1) + '): ' + str(u) + ' COUNT: '+ str(Count1))\n",
    "print(' The Total Number of unique instances is: '+ str(Count1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Examine the key pairs and the hash values. The question is, are there any hash collisions? This can be done as follows:   \n",
    "> 5. Compute a list of the hash values for all combinations of $i$ and $j$.   \n",
    "> 6. Print the length of the list.  \n",
    "> 7. Print the length of the unique values of the hash. You can find the unique values in a list with the [numpy.unique](https://numpy.org/doc/stable/reference/generated/numpy.unique.html) function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "harr_unique(0): 0 Reoccurrances: 29\n",
      "harr_unique(1): 1 Reoccurrances: 2\n",
      "harr_unique(2): 2 Reoccurrances: 6\n",
      "harr_unique(3): 3 Reoccurrances: 5\n",
      "harr_unique(4): 4 Reoccurrances: 7\n",
      "harr_unique(5): 5 Reoccurrances: 3\n",
      "harr_unique(6): 6 Reoccurrances: 7\n",
      "harr_unique(7): 7 Reoccurrances: 3\n",
      "harr_unique(8): 8 Reoccurrances: 10\n",
      "harr_unique(9): 9 Reoccurrances: 2\n",
      "harr_unique(10): 10 Reoccurrances: 7\n",
      "harr_unique(11): 11 Reoccurrances: 3\n",
      "harr_unique(12): 12 Reoccurrances: 9\n",
      "harr_unique(13): 13 Reoccurrances: 4\n",
      "harr_unique(14): 14 Reoccurrances: 6\n",
      "harr_unique(15): 15 Reoccurrances: 3\n",
      "harr_unique(16): 16 Reoccurrances: 16\n",
      "harr_unique(17): 17 Reoccurrances: 2\n",
      "harr_unique(18): 18 Reoccurrances: 7\n",
      "harr_unique(19): 19 Reoccurrances: 2\n",
      "harr_unique(20): 20 Reoccurrances: 7\n",
      "harr_unique(21): 21 Reoccurrances: 3\n",
      "harr_unique(22): 22 Reoccurrances: 6\n",
      "harr_unique(23): 23 Reoccurrances: 3\n",
      "harr_unique(24): 24 Reoccurrances: 10\n",
      "harr_unique(25): 25 Reoccurrances: 2\n",
      "harr_unique(26): 26 Reoccurrances: 5\n",
      "harr_unique(27): 27 Reoccurrances: 3\n",
      "harr_unique(28): 28 Reoccurrances: 7\n",
      "harr_unique(29): 29 Reoccurrances: 2\n",
      "harr_unique(30): 30 Reoccurrances: 6\n",
      "harr_unique(31): 31 Reoccurrances: 3\n",
      " The Total Number of unique instances is: 32\n"
     ]
    }
   ],
   "source": [
    "## Put your code below. \n",
    "\n",
    "Count1 = 0\n",
    "ui, harr_u = np.unique(harr,return_counts = True)\n",
    "\n",
    "for uindex in range(len(harr_u)):\n",
    "    Count1 += 1\n",
    "    print('harr_unique(' + str(uindex) + '): ' + str(ui[uindex]) + ' Reoccurrances: '+ str(harr_u[uindex]))\n",
    "\n",
    "print(' The Total Number of unique instances is: '+ str(Count1))\n",
    "\n",
    "##print (ui)\n",
    "##print (harr_u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Examine the results you have printed. Is there any evidence of hash key collisions?     \n",
    "> The ratio of $m/n$ is deliberately kept high since the simple hash function has no collision resolution mechanism. Optionally, you can try reducing this ration (the multiplier) to 16 and 8, noting the increase in hash collisions.  \n",
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes: There is evidence of hash key collisions. There are 190 Combinations with only 32 unique values. Each reoccurance has been determined for the unique hash values indicating the number of collisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The map process\n",
    "\n",
    "We are constructing this example a map and a reduce process. The processes are intended to compute the hypothesis test for differences of means between all the pairs of vectors. The first step is the map process, which creates the keys, or values of $i$ and $j$ for these pairs.   \n",
    "\n",
    "> **Exercise 1-2:** You will now create the code for the map task which build a data frame with $i, j$ key pairs indexed by the hash. By the following steps you will create code that represents a map task.  \n",
    "> 1. Create a data frame with two columns $i$ and $j$ with rows $= hash_key * modulo_multiplier $ and set all values to $= numpy.nan$.\n",
    "> 2. Create a loop over all combinations of the pairs of i and j.   \n",
    "> 2. Compute the hash key value for the indexes, i and j.  \n",
    "> 4. Add the $i$ and $j$ values to the row indexed by the hash key.  \n",
    "> 5. Return the hash table. \n",
    "> 6. Execute the function to create the hash table.  \n",
    "> 7. Compute and print the length of the hash table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "def map_hypothesis(vars, hash_key=1024, modulo_multiplier=32):\n",
    "    ## Put your code below. \n",
    "    columns = ['i','j']\n",
    "    ## make the index or rows\n",
    "    index = hash_key * modulo_multiplier\n",
    "    df = pd.DataFrame(array, index, columns)\n",
    "    print(df)\n",
    "    ##for i,j in combinations(range(ncolumns), 2): \n",
    "        ## Put your code below. \n",
    "        ## Compute the hash key and added the values to the \n",
    "        ## row of the data frame. \n",
    "        \n",
    "    return print(df)\n",
    "    ##return hash_table\n",
    "\n",
    "hash_table = map_hypothesis(normal_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The shuffle and reduce task\n",
    "\n",
    "Now that you have the keys for the pairwise combinations of the vectors it is time to perform the reduce process. The reduce process computes the pair-wise t-statistics and p-values. These statistical values are indexed by the keys of the pair of vectors. This process reduces the full vectors of values down to just two numbers for each pair of vectors. \n",
    "\n",
    "> **Exercise 1-3:** You will now create and apply the following code for the reduce process:   \n",
    "> 1. Create an empty data frame with columns, `i`, `j`, `t_statistic`, and `p_value`.    \n",
    "> 2. Using a for loop iterate over all possible (hashed) keys of the data frame. An if statement is used to test if these are valid values of the key, i. Use the [numpy.isnan](https://numpy.org/doc/stable/reference/generated/numpy.isnan.html) function for this test.  \n",
    "> 3. Extract the values of i and j from the input data frame. \n",
    "> 4. Using keys, compute the t-statistic and p-value using [scipy.stats import ttest_ind](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind.html).\n",
    "> 5. Append a row to the output data frame.\n",
    "> 6. Return the data frame, sorted in ascending order, using the [Pandas.DataFrame.sort_values](https://turned.pydata.org/docs/reference/api/pandas.DataFrame.sort_values.html) method and re-indexed using the [Pandas.DataFrame.reset_index](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.reset_index.html) method.    \n",
    "> 7. Execute your function and save the returned data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def reduce_significance(hash_table, values):  \n",
    "    ## Create a data framreturn the results of the \n",
    "    ## the reduce process. The results are grouped by the first \n",
    "    ## index i. \n",
    "    test_results = pd.DataFrame(columns=['i','j','t_statistic','p_value'])\n",
    "\n",
    "    ## As a substitute for a shuffle we will use a simple search \n",
    "    ## through the data frame  \n",
    "    for hash in range(hash_table.shape[0]): \n",
    "    ## Put your code below. \n",
    "        if not np.isnan(hash_table.iloc[hash,0]):\n",
    "           \n",
    "        \n",
    "    ## Given the i,j pairs we need to compute the t-statistic and the p-value.   \n",
    "    ## This is the reduce step, since for each i,j pair there is only \n",
    "    ## a t-statistic and a p-value. \n",
    "    ## Put your code below. \n",
    "           \n",
    "    ## Sort and return the results\n",
    "    #test_results.sort_values('p_value', axis=0, ascending=True# , inplace=True)\n",
    "    return test_results.sort_values('p_value', axis=0, ascending=True).reset_index(drop=True)        \n",
    "        \n",
    "\n",
    "test_stats = reduce_significance(hash_table, normal_vars)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 8. In the cell below, create a filter for pair test cases which are significant and save these cases in a data frame. \n",
    "> 9. Print the number (len) of significant results.\n",
    "> 10. Print the rows with the significant test results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "significance_level = 0.05\n",
    "## Put your code below. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Notice the large number of apparently significant tests. Answer the following questions:  \n",
    "> 1. Is the number of false positive cases higher than expected?    \n",
    "> 2. Examine which of the iid Normal vectors contribute to the false positive results. Are there vectors which contribute multiple times?   \n",
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonferroni correction  \n",
    "\n",
    "Several adjustments to the multiple comparisons problem have been proposed. In Dunn published a method know as the **Bonferroni correction** in 1961. The Bonferroni correction is a widely used method to reduce the false positive rate of hypothesis tests.  The adjustment is simple:\n",
    "$$\\alpha_b = \\frac{\\alpha}{m}\\\\\n",
    "with\\\\ \n",
    "m =\\ number\\ of\\ groups$$\n",
    "\n",
    "Can the Bonferroni correction help? Yes, by greatly increasing the confidence level required for a statistically significant result. The problem with the Bonferroni correction is the reduction in power as the  grows smaller. For big data problems with large numbers of groups, this issue can be especially serious. \n",
    "\n",
    "**Exercise 1-4:** You will now apply the Bonferroni correction to the iid Normal vectors. To do so, you will compute the Bonferroni threshold and the apply it to the p-values:   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code below        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Even with the Bonferroni correction we have some false significance tests, if only just barely!    \n",
    "> **End of exercise.**\n",
    "\n",
    "But, can we detect small effect with Bonferroni correction, as this method significantly reduces power of tests? Execute the code in the cell below, which compares a standard Normal to a Normal with a small mean (effect size), to find out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr.seed(567)\n",
    "print(significance_bonferroni)\n",
    "ttest_ind(normal_vars[:,0], nr.normal(loc = 0.01, size=(1000,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the Bonferroni correction, this difference in means would not be found significant. This illustrates the downside of the correction, which may prevent detection of significant effects, while still finding false significance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## False Discovery Rate Control Methods \n",
    "\n",
    "We have seen the potential pitfalls of multiple hypothesis testing. Further, we have seen that a simple approach to **false discovery rate (FDR) control** is not effective. You will now apply more sophisticated FDR control methods to control the FDR. \n",
    "\n",
    "Inflammatory bowel disease is an auto immune disease that is characterized by chronic inflammation in the digestive tract. In 2020, there were around 2.5 million people with inflammatory bowel disease in the United States. It is estimated that the prevalence of IBD among U.S. population will rise to around 3.5 million by 2030.There are two forms of IBD: Ulcerative Colitis (UC) and Crohn’s disease (CD). \n",
    "\n",
    "The specific problem we will explore is to determine which genes lead to expression of a certain disease. In this example, there are gene expression data for 97 patients. Some of these patients have ulcerative colitis and others have Crohn's disease, which are believed to be genetically inherited.    \n",
    "\n",
    "One approach to this problem is to perform hypothesis tests on the expression of the genes between patients with the two conditions. Since there are over 10,000 genes there is considerable chance for false discovery. Therefore, careful application of FDR control is required.\n",
    "\n",
    "To continue with the example, execute the code in the cell below to load the data and print the dimensionality of the data frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_data = pd.read_csv('../data/ColonDiseaseGeneData-Cleaned.csv')\n",
    "print(gene_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are data from 97 patients for 10,497 genes. A large number of hypothesis tests are required!     \n",
    "\n",
    "Execute the code in the cell below to view the first 5 columns of the data frame, which includes the expression of the first 4 genes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " print(gene_data.iloc[:,:5])      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Holm's method\n",
    "\n",
    "You will apply two FDR control methods to these data.These methods attempt to conod trol the FDR while not being overly conservative like the Bonferronic correction. The first of these Holm's method.    \n",
    "\n",
    "The Holm's method operates on the ordered set of p-values, $D = \\{ p_{(1)}, p_{(2)}, p_{(3)}, \\ldots, p_{(n)} \\}$. The threshold for the $ith$ p-value, $p(i) is:  \n",
    "\n",
    "$$p(i) \\le Threshold(Holm's) = \\frac{\\alpha}{N - i + 1}$$\n",
    "\n",
    "For example: for the 10th ordered p-value with 1,000 total tests (genes) and significance level of 0.05, the cutoff is:   \n",
    "\n",
    "$$p(10) \\le \\frac{0.05}{1000 - 10 + 1} = 0.00005045$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map process  \n",
    "\n",
    "> **Exercise 01-4:** To start the processing of these data you will first create and execute code for a map process. The map process groups the data by the patient's disease into data frame, ulcerative, crohns. The keys for each of these key-value pairs are the gene identifier. Notice that one key is all that is needed in this case. Now do the following to create and execute a function, `map_gene`:   \n",
    "> 1. Create a logical mask and group the values by `Disease State` into two data frames.\n",
    "> 2. Return the transpose of the two data frames, removing the `Disease State` values. The result of this operation should be data frames with gene expressions in the columns and the gene identifier as the row index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_gene(gene_data):  \n",
    "    ## First, separate the columns by disease type  \n",
    "    ## Put your code below. \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "ulcerative, crohns = map_gene(gene_data)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 3. Execute the code in the cells below to display the heads of these data frames and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ulcerative.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crohns.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce process \n",
    "\n",
    "> **Exercise 01-5:** With the key-value pairs organized by disease state, it is time to create and execute code of a reduce process. The reduce process will compute the pairwise t-statistics and p-values for each gene and return the sorted results. Specifically, your `gene_test` with arguments of the two mapped data frames will do the following:   \n",
    "> 1. Create an empty data frame with columns gene, t_statistics, and p-value.\n",
    "> 2. A for loop iterates over the keys of either of the data frames.  \n",
    "> 3. Compute the t-statistic and p-value for the gene (key).\n",
    "> 4. Append the results to the data frame.   \n",
    "> 5. Sort the results data frame, inplace, into ascending order.\n",
    "> 6. Return the resulting data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def gene_test(ulcerative, crohns):  \n",
    "    test_results = pd.DataFrame(columns=['gene','t_statistic','p_value'])\n",
    "    ## Put your code below. \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return test_results.set_index('gene')\n",
    "    \n",
    "gene_statistics = gene_test(ulcerative, crohns)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Significance of results \n",
    "\n",
    "With the gene data reduced to the t-test statistics, you will now determine the significance of these tests. It is important to understand that scientists believe that expression of a disease, like Corhn's, is only in a small number of genes.  \n",
    "\n",
    "> **Exercise 01-6:** As a first step in understanding the gene expression significance complete and execute the code in the cell below to find the number of 'significant' genes using the simple single hypothesis test cutoff criteria.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "significance_level =0.05\n",
    "## Put your code below. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Does this large number of 'statistically significant' results appear credible, given that only a few genes are thought to have significant expression for this disease?    \n",
    "> **End of exercise.**\n",
    "\n",
    "> **Exercise 01-7:** We have already seen that the Bonferroni correction is a rather conservative approach to testing the significance of large numbers of hypotheses. You will now use the Bonferroni correction to test the significance of the gene expression, by completing the code in the cell below.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code below. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The foregoing result seems reasonable, but is it too conservative?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 01-08:** You will now apply the Holms method to determining significance of the gene expression test results. In the cell below complete the `holms_significance` function with arguments of the results data frame and the significance level. This function does the following:  \n",
    "> 1. Find the number of test results and compute the numerator used for the cutoff calculation. \n",
    "> 2. Compute the vector of thresholds using the Holms formula. Use the Python `range`function to get the values of the index i. But, keep in mind that range produces a zero-indexed iterator, and the algorithm needs a one-indexed list.  Use the [numpy.divide](https://numpy.org/doc/stable/reference/generated/numpy.divide.html) function to perform the vector divide. Save these threshold values in a data frame in a 'holms_threshold' column.   \n",
    "> 3. Using the threshold values compute a logical vector and save it in a column names 'significance' in the data frame.\n",
    "> 4. Return the data frame.\n",
    "> Finally, execute the function and save the results in a data frame. Then find the length of the subset where the 'significance' value is True. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def holms_significance(test_results, significance):\n",
    "    ## First compute the thresholds for each of the ordered tests\n",
    "    ## Put your code below. \n",
    "\n",
    "    \n",
    "    \n",
    "    ## Now we test the significance of the ordered p-values \n",
    "\n",
    "    return test_results\n",
    "\n",
    "holms_results = holms_significance(gene_statistics, significance_level)    \n",
    "len(holms_results.loc[holms_results.loc[:,'significant'],:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Despite the general properties that the Holm's method is considered less conservative than the Bonferroni correction the results agree in this case. Does this agreement give you some confidence in the result and why?  \n",
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can visualize the results of the Holm's method test. The plot has two key elements:  \n",
    "1. Plot the curve of the p-values vs. the order number, i. The line is color coded by significance or not.\n",
    "2. Plot the threshold line. This line is straight since the threshold is a linear function of i."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_significance(results, threshold):\n",
    "    results['number'] = range(len(results))\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    sns.lineplot(x='number',y=threshold, data=results, ax=ax, color='black', linewidth=0.5)\n",
    "    sns.scatterplot(x='number',y='p_value', hue='significant', data=results, s=3, ax=ax)\n",
    "    ax.set_title('Significance of gene expression')\n",
    "    ax.set_xlabel('Gene number')\n",
    "    ax.set_ylabel('p-value')\n",
    "    \n",
    "plot_significance(holms_results.iloc[:500,:], 'holms_threshold')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the following about this plot:  \n",
    "1. The p-value significance line crosses the threshold point at an apparent break point.   \n",
    "2. The significant p-values are all very small since there are so many tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benamini-Hochberg FDR Control \n",
    "\n",
    "The Benamini-Hochberg FDR control algorithm is another way to control false discoveries. Stat with an ordered set of $n$ p-values, $D = \\{ p_{(1)}, p_{(2)}, p_{(3)}, \\ldots, p_{(n)} \\}$ we define a false discovery rate, $q$:\n",
    "\n",
    "$$FDR(D) \\le q$$\n",
    "\n",
    "The cutoff threshold for the ith p-value is then:\n",
    "$$p_{(i)} \\le Threshold(D_q) = \\frac{q}{n} i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 01-9:** In this exercise you will apply the Benamini-Hochberg FDR control algorithm for testing the significance of the gene expressions. The `BH_significance` function is quite similar to the Holm's method function you have already created. Given the large number of genes you must use a low false discovery rate, $0.001$, or 1 out of 1,000. \n",
    "> Execute your function, saving the result. Then print the number of significant cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BH_significance(test_results, false_discovery_tollerance):\n",
    "    ## First compute the thresholds for each of the ordered tests\n",
    "    ## Put your code below. \n",
    "\n",
    "    \n",
    "    \n",
    "    ## Now we test the significance of the ordered p-values \n",
    "\n",
    "    return test_results\n",
    "\n",
    "BH_results = BH_significance(gene_statistics, 0.001)    \n",
    "len(BH_results.loc[BH_results.loc[:,'significant'],:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The result is similar to the first two FDR control methods. Given the false discovery parameter of 0.0001 do you think this is a reasonable result? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, execute the code in the cell below and examine the resulting plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_significance(BH_results.iloc[:500,:], 'bh_threshold')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 01-10**: Bonus question. Compare the plot above to the foregoing plot for Holm's method. Are the breaks in slope of the p-value curve at the crossing point with the threshold values reasonable in both cases? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Copyright 2021, Stephen F. Elston. All rights reserved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
