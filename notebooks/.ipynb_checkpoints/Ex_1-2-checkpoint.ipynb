{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 01 - Pitfalls in Data Mining     \n",
    "## CSCI E-96\n",
    "\n",
    "The goal of data mining is to find important relationships in large complex datasets. These dataset typically contain a large number of variables. The **high-dimensional** nature of the data leads to some commonly encountered pitfalls which lead to incorrect inferences.   \n",
    "\n",
    "In this assignment you will gain a bit of experience with three important concepts in data mining:  \n",
    "\n",
    "1. **False Discovery Rate Control:** The goal of data mining is to find important relationships in large complex datasets. These dataset typically contain a large number of variables. The **high-dimensional** nature of the data leads to some commonly encountered pitfalls which lead to incorrect inferences. A related problem is cutting off a large-scale analysis when a desired relationship is 'found'. This practice of **p-value mining** often leads to unwarranted inferences. You will apply false discovery rate (FDR) control methods to address this problem.   \n",
    "2. **Key-Value Pairs:** Large scale data is typically managed using key-value (KV) pairs. The exercises in this assignment give you some experience working with KV pair data management.  \n",
    "3. **Map and Reduce Processes:** Much of large scale data mining requires use of a split-apply-combine approach. The data is split into manageable chunks, analytic transformations are applied, and the result combined or aggregated. A commonly used class of a split-apply-combine algorithm is MapReduce. \n",
    "\n",
    "In order to keep the scope of this assignment manageable, you will use limited versions of KV pair management and MapReduce. Specifically, you will use common Python tools to implement these concepts rather than dedicated large scale analytic platforms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Hypothesis Tesing\n",
    "\n",
    "Testing multiple hypothesis in high-dimensional data can be problematic. Exhaustively testing all pairwise relationships between variables in a data set is a commonly used, but generally misleading from of **multiple comparisons**. The chance of finding false significance, using such a **data dredging** approach, can be surprisingly high. \n",
    "\n",
    "In this exercise you will perform multiple comparisons on only 20 **identically distributed independent (iid)** variables. Ideally, such tests should not find significant relationships, but the actual result is quite different. \n",
    "\n",
    "To get started, execute the code in the cell below to load the required packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "from scipy.stats import ttest_ind, f_oneway\n",
    "from itertools import product, combinations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you will apply a t-test to all pairwise combinations of identical Normally distributed variables. In this case, we will create a data set with 20 iid Normal distributions of 1000 samples each. Execute the code in the cell below to find this data and display the mean and variance of each variable.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NORMAL VARS\n",
      "[[ 0.81879162 -1.04355064  0.3509007  ... -0.17542066 -0.08270438\n",
      "  -0.88845473]\n",
      " [-0.30076649  0.90837517 -0.64559131 ... -0.46415036 -0.79785911\n",
      "   1.31076281]\n",
      " [ 1.17479389 -0.05046953  0.71895176 ...  0.71999964 -0.86680023\n",
      "  -0.34668516]\n",
      " ...\n",
      " [ 0.0943446  -0.88593117  2.21144516 ... -0.48951209 -0.14650596\n",
      "   1.468948  ]\n",
      " [-0.99396682 -0.60068401 -1.06598225 ...  0.80637671 -0.38532756\n",
      "  -0.06067443]\n",
      " [-0.7191216   0.0248463   0.2728283  ...  0.3302862   0.2384886\n",
      "   0.98507076]]\n",
      "Len of normal vars\n",
      "1000\n",
      "The means of the columns are\n",
      " [-1.16191649e-01  2.80829317e-02 -1.78516419e-02 -1.44691489e-02\n",
      "  3.03718152e-02  1.20007442e-02 -9.58845606e-05  1.98662580e-03\n",
      "  4.94154934e-02 -4.11640866e-02 -6.32977862e-03 -5.93868192e-02\n",
      " -2.56373595e-02  1.43568791e-02 -1.44725765e-02 -1.37023955e-02\n",
      "  1.80622439e-02  5.87029691e-02 -2.02650514e-02 -1.56346106e-02]\n",
      "\n",
      "The variances of the columns are\n",
      " [0.94834508 1.04744241 1.0258018  0.96977571 1.0089001  1.04113864\n",
      " 1.00657222 0.99192594 1.04713487 1.04329434 1.04023108 0.96791346\n",
      " 1.03706907 1.07179865 1.01431404 1.05060289 1.02054329 0.9686211\n",
      " 1.02810287 0.99521555]\n",
      "[-0.11619164926199306, 0.028082931743419342, -0.017851641926649935, -0.014469148926407141, 0.030371815211215986, 0.01200074415037437, -9.588456058957173e-05, 0.0019866258031239945, 0.04941549338764833, -0.04116408661462267, -0.0063297786154073365, -0.0593868191915101, -0.025637359468878882, 0.014356879111661789, -0.014472576450111173, -0.013702395461434471, 0.01806224389911345, 0.05870296913581026, -0.020265051397245912, -0.015634610559834186]\n",
      "[0.9483450841278454, 1.0474424115924386, 1.025801797556006, 0.9697757102682736, 1.0089000973212925, 1.0411386386684267, 1.0065722193307869, 0.9919259445834522, 1.0471348686585749, 1.0432943362335887, 1.040231080627933, 0.9679134633786484, 1.0370690685544615, 1.0717986540361213, 1.0143140442818062, 1.050602885420908, 1.0205432948150148, 0.9686211004006502, 1.0281028694152574, 0.9952155477459146]\n"
     ]
    }
   ],
   "source": [
    "ncolumns = 20\n",
    "nr.seed(234)\n",
    "mean = []\n",
    "var = []\n",
    "normal_vars = nr.normal(size=(1000,ncolumns))\n",
    "print('NORMAL VARS')\n",
    "print(normal_vars)\n",
    "print('Len of normal vars')\n",
    "print(len(normal_vars))\n",
    "print('The means of the columns are\\n', np.mean(normal_vars, axis = 0))\n",
    "print('\\nThe variances of the columns are\\n', np.var(normal_vars, axis = 0))\n",
    "##create arrays for mean and var\n",
    "for mn in np.mean(normal_vars, axis =0 ):\n",
    "    mean.append(mn)\n",
    "print(mean)\n",
    "    \n",
    "for vr in np.var(normal_vars, axis = 0):\n",
    "    var.append(vr)\n",
    "print(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that means and variances are close to 0.0 and 1.0 respectively. As expected, there is not much difference between these variables.\n",
    "\n",
    "How many of these t-tests will show **significance** at the 0.05 cut-off level? There are 380 pairwise combinations, so we expect to find a number of falsely significant test results at this level. To find out, complete and execute the code in the cell below to filter the test results and print those that show significance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a hash \n",
    "\n",
    "The goal of this exercise is to compute pairwise hypothesis tests of the differences in means for each of the iid Normal vectors. As an intermediate step you will create a Pandas data frame using a hash of the keys of the vectors. The data frame will contain the **key-value**, $(K,V)$, pairs. Each key must represent an index for the two vectors used to compute the test statistic. The keys will then be used to index the results of these hypothesis tests. \n",
    "\n",
    "The question is, how can we create a hash from the keys for the pair of vectors? In this case to we will use a simple, but far from optimal hash. For the two vector indicies $i, j$, for some key and modulo, $m$, we will compute the hash as:  \n",
    "\n",
    "$$h(i,j) = (i + key*j) mod m$$\n",
    "\n",
    "> **Computational Note:** The Pandas data frame is an efficient and reasonably scalable **hash table**. The hash function used depends on the type of the key; integer, string, etc. The resulting dictionary of key-value pairs, $(K,V)$, can therefore be access in far less than linear time, often about $O(log(N))$.  \n",
    "\n",
    "If you are not familiar with Python dictionaries you can find a short tutorial [here](https://www.tutorialspoint.com/python_data_structure/python_hash_table.htm), as well as many other places on the web."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 1-1:** Given that our space of vectors is actually quite small, just 20, we do not need a sophisticated and scalable hash function. This hashed key will then be used to store and retrieve the values using a Python dictionary, in about $O(log(N))$ time.     \n",
    "\n",
    "> In this exercise you will test a simple hash function and its inverse. Examine the code below and notice that the hash function encodes the two indexes into a single integer by simple additional and multiplication. The modulo operation limits the size of the hash table. However, to keep things simple you will not need to implement any hash collision resolution mechanism. As a result, the size of the table is set much larger than required.  \n",
    "\n",
    "> To test this hash, do the following:    \n",
    "> 1. Create a function called hash function to compute the hash. The arguments to the function are $i$ and $j$, the `hash\\_key` and the `modulo\\_multiplier`. The defaults of the arguments are $hash\\_key=1024$ and $modulo\\_multiplier=32$. The modulo number is $hash\\_key * modulo\\_multiplier$, e.g. $modulo = 32,768$. The multiplier is the ratio of expected values stored, $n$, to the number of unique hash keys, $m$, e.g. the ratio $m/n$.\n",
    "> 2. Using the Python [ittertools.combinations](https://docs.python.org/3/library/itertools.html#itertools.combinations) function create all unique pairwise combinations of indexes i and j. The arguments to this function are the indexes to the iid Normal vectors. The iterator is `range(ncolumns)` choose 2, since these comparisons are pairwise.    \n",
    "> 3. Within this loop call the hash with the values of $i$ and $j$ as arguments.   \n",
    "> 3. On a single line print the following; the values of i and j, the hash key value, but only if $i \\le 6$. The restriction is to keep the printed output shorter.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Count = 0\n",
      " Count = 1\n",
      " Count = 2\n",
      " Count = 3\n",
      " Count = 4\n",
      " Count = 5\n",
      " Count = 6\n",
      " Count = 7\n",
      " Count = 8\n",
      " Count = 9\n",
      " Count = 10\n",
      " Count = 11\n",
      " Count = 12\n",
      " Count = 13\n",
      " Count = 14\n",
      " Count = 15\n",
      " Count = 16\n",
      " Count = 17\n",
      " Count = 18\n",
      " Count = 19\n",
      " Count = 20\n",
      " Count = 21\n",
      " Count = 22\n",
      " Count = 23\n",
      " Count = 24\n",
      " Count = 25\n",
      " Count = 26\n",
      " Count = 27\n",
      " Count = 28\n",
      " Count = 29\n",
      " Count = 30\n",
      " Count = 31\n",
      " Count = 32\n",
      " Count = 33\n",
      " Count = 34\n",
      " Count = 35\n",
      " Count = 36\n",
      " Count = 37\n",
      " Count = 38\n",
      " Count = 39\n",
      " Count = 40\n",
      " Count = 41\n",
      " Count = 42\n",
      " Count = 43\n",
      " Count = 44\n",
      " Count = 45\n",
      " Count = 46\n",
      " Count = 47\n",
      " Count = 48\n",
      " Count = 49\n",
      " Count = 50\n",
      " Count = 51\n",
      " Count = 52\n",
      " Count = 53\n",
      " Count = 54\n",
      " Count = 55\n",
      " Count = 56\n",
      " Count = 57\n",
      " Count = 58\n",
      " Count = 59\n",
      " Count = 60\n",
      " Count = 61\n",
      " Count = 62\n",
      " Count = 63\n",
      " Count = 64\n",
      " Count = 65\n",
      " Count = 66\n",
      " Count = 67\n",
      " Count = 68\n",
      " Count = 69\n",
      " Count = 70\n",
      " Count = 71\n",
      " Count = 72\n",
      " Count = 73\n",
      " Count = 74\n",
      " Count = 75\n",
      " Count = 76\n",
      " Count = 77\n",
      " Count = 78\n",
      " Count = 79\n",
      " Count = 80\n",
      " Count = 81\n",
      " Count = 82\n",
      " Count = 83\n",
      " Count = 84\n",
      " Count = 85\n",
      " Count = 86\n",
      " Count = 87\n",
      " Count = 88\n",
      " Count = 89\n",
      " Count = 90\n",
      " Count = 91\n",
      " Count = 92\n",
      " Count = 93\n",
      " Count = 94\n",
      " Count = 95\n",
      " Count = 96\n",
      " Count = 97\n",
      " Count = 98\n",
      " Count = 99\n",
      " Count = 100\n",
      " Count = 101\n",
      " Count = 102\n",
      " Count = 103\n",
      " Count = 104\n",
      " Count = 105\n",
      " Count = 106\n",
      " Count = 107\n",
      " Count = 108\n",
      " Count = 109\n",
      " Count = 110\n",
      " Count = 111\n",
      "COUNT: 1:  hash[(0 , 1)] : 1024\n",
      "COUNT: 2:  hash[(0 , 2)] : 2048\n",
      "COUNT: 3:  hash[(0 , 3)] : 3072\n",
      "COUNT: 4:  hash[(0 , 4)] : 4096\n",
      "COUNT: 5:  hash[(0 , 5)] : 5120\n",
      "COUNT: 6:  hash[(0 , 6)] : 6144\n",
      "COUNT: 7:  hash[(0 , 7)] : 7168\n",
      "COUNT: 8:  hash[(0 , 8)] : 8192\n",
      "COUNT: 9:  hash[(0 , 9)] : 9216\n",
      "COUNT: 10:  hash[(0 , 10)] : 10240\n",
      "COUNT: 11:  hash[(0 , 11)] : 11264\n",
      "COUNT: 12:  hash[(0 , 12)] : 12288\n",
      "COUNT: 13:  hash[(0 , 13)] : 13312\n",
      "COUNT: 14:  hash[(0 , 14)] : 14336\n",
      "COUNT: 15:  hash[(0 , 15)] : 15360\n",
      "COUNT: 16:  hash[(0 , 16)] : 16384\n",
      "COUNT: 17:  hash[(0 , 17)] : 17408\n",
      "COUNT: 18:  hash[(0 , 18)] : 18432\n",
      "COUNT: 19:  hash[(0 , 19)] : 19456\n",
      "COUNT: 20:  hash[(1 , 2)] : 2050\n",
      "COUNT: 21:  hash[(1 , 3)] : 3075\n",
      "COUNT: 22:  hash[(1 , 4)] : 4100\n",
      "COUNT: 23:  hash[(1 , 5)] : 5125\n",
      "COUNT: 24:  hash[(1 , 6)] : 6150\n",
      "COUNT: 25:  hash[(1 , 7)] : 7175\n",
      "COUNT: 26:  hash[(1 , 8)] : 8200\n",
      "COUNT: 27:  hash[(1 , 9)] : 9225\n",
      "COUNT: 28:  hash[(1 , 10)] : 10250\n",
      "COUNT: 29:  hash[(1 , 11)] : 11275\n",
      "COUNT: 30:  hash[(1 , 12)] : 12300\n",
      "COUNT: 31:  hash[(1 , 13)] : 13325\n",
      "COUNT: 32:  hash[(1 , 14)] : 14350\n",
      "COUNT: 33:  hash[(1 , 15)] : 15375\n",
      "COUNT: 34:  hash[(1 , 16)] : 16400\n",
      "COUNT: 35:  hash[(1 , 17)] : 17425\n",
      "COUNT: 36:  hash[(1 , 18)] : 18450\n",
      "COUNT: 37:  hash[(1 , 19)] : 19475\n",
      "COUNT: 38:  hash[(2 , 3)] : 3078\n",
      "COUNT: 39:  hash[(2 , 4)] : 4104\n",
      "COUNT: 40:  hash[(2 , 5)] : 5130\n",
      "COUNT: 41:  hash[(2 , 6)] : 6156\n",
      "COUNT: 42:  hash[(2 , 7)] : 7182\n",
      "COUNT: 43:  hash[(2 , 8)] : 8208\n",
      "COUNT: 44:  hash[(2 , 9)] : 9234\n",
      "COUNT: 45:  hash[(2 , 10)] : 10260\n",
      "COUNT: 46:  hash[(2 , 11)] : 11286\n",
      "COUNT: 47:  hash[(2 , 12)] : 12312\n",
      "COUNT: 48:  hash[(2 , 13)] : 13338\n",
      "COUNT: 49:  hash[(2 , 14)] : 14364\n",
      "COUNT: 50:  hash[(2 , 15)] : 15390\n",
      "COUNT: 51:  hash[(2 , 16)] : 16416\n",
      "COUNT: 52:  hash[(2 , 17)] : 17442\n",
      "COUNT: 53:  hash[(2 , 18)] : 18468\n",
      "COUNT: 54:  hash[(2 , 19)] : 19494\n",
      "COUNT: 55:  hash[(3 , 4)] : 4108\n",
      "COUNT: 56:  hash[(3 , 5)] : 5135\n",
      "COUNT: 57:  hash[(3 , 6)] : 6162\n",
      "COUNT: 58:  hash[(3 , 7)] : 7189\n",
      "COUNT: 59:  hash[(3 , 8)] : 8216\n",
      "COUNT: 60:  hash[(3 , 9)] : 9243\n",
      "COUNT: 61:  hash[(3 , 10)] : 10270\n",
      "COUNT: 62:  hash[(3 , 11)] : 11297\n",
      "COUNT: 63:  hash[(3 , 12)] : 12324\n",
      "COUNT: 64:  hash[(3 , 13)] : 13351\n",
      "COUNT: 65:  hash[(3 , 14)] : 14378\n",
      "COUNT: 66:  hash[(3 , 15)] : 15405\n",
      "COUNT: 67:  hash[(3 , 16)] : 16432\n",
      "COUNT: 68:  hash[(3 , 17)] : 17459\n",
      "COUNT: 69:  hash[(3 , 18)] : 18486\n",
      "COUNT: 70:  hash[(3 , 19)] : 19513\n",
      "COUNT: 71:  hash[(4 , 5)] : 5140\n",
      "COUNT: 72:  hash[(4 , 6)] : 6168\n",
      "COUNT: 73:  hash[(4 , 7)] : 7196\n",
      "COUNT: 74:  hash[(4 , 8)] : 8224\n",
      "COUNT: 75:  hash[(4 , 9)] : 9252\n",
      "COUNT: 76:  hash[(4 , 10)] : 10280\n",
      "COUNT: 77:  hash[(4 , 11)] : 11308\n",
      "COUNT: 78:  hash[(4 , 12)] : 12336\n",
      "COUNT: 79:  hash[(4 , 13)] : 13364\n",
      "COUNT: 80:  hash[(4 , 14)] : 14392\n",
      "COUNT: 81:  hash[(4 , 15)] : 15420\n",
      "COUNT: 82:  hash[(4 , 16)] : 16448\n",
      "COUNT: 83:  hash[(4 , 17)] : 17476\n",
      "COUNT: 84:  hash[(4 , 18)] : 18504\n",
      "COUNT: 85:  hash[(4 , 19)] : 19532\n",
      "COUNT: 86:  hash[(5 , 6)] : 6174\n",
      "COUNT: 87:  hash[(5 , 7)] : 7203\n",
      "COUNT: 88:  hash[(5 , 8)] : 8232\n",
      "COUNT: 89:  hash[(5 , 9)] : 9261\n",
      "COUNT: 90:  hash[(5 , 10)] : 10290\n",
      "COUNT: 91:  hash[(5 , 11)] : 11319\n",
      "COUNT: 92:  hash[(5 , 12)] : 12348\n",
      "COUNT: 93:  hash[(5 , 13)] : 13377\n",
      "COUNT: 94:  hash[(5 , 14)] : 14406\n",
      "COUNT: 95:  hash[(5 , 15)] : 15435\n",
      "COUNT: 96:  hash[(5 , 16)] : 16464\n",
      "COUNT: 97:  hash[(5 , 17)] : 17493\n",
      "COUNT: 98:  hash[(5 , 18)] : 18522\n",
      "COUNT: 99:  hash[(5 , 19)] : 19551\n",
      "COUNT: 100:  hash[(6 , 7)] : 7210\n",
      "COUNT: 101:  hash[(6 , 8)] : 8240\n",
      "COUNT: 102:  hash[(6 , 9)] : 9270\n",
      "COUNT: 103:  hash[(6 , 10)] : 10300\n",
      "COUNT: 104:  hash[(6 , 11)] : 11330\n",
      "COUNT: 105:  hash[(6 , 12)] : 12360\n",
      "COUNT: 106:  hash[(6 , 13)] : 13390\n",
      "COUNT: 107:  hash[(6 , 14)] : 14420\n",
      "COUNT: 108:  hash[(6 , 15)] : 15450\n",
      "COUNT: 109:  hash[(6 , 16)] : 16480\n",
      "COUNT: 110:  hash[(6 , 17)] : 17510\n",
      "COUNT: 111:  hash[(6 , 18)] : 18540\n",
      "COUNT: 112:  hash[(6 , 19)] : 19570\n",
      " The Total Number of Combinations is: 190\n",
      "harr_unique(0): 1024 COUNT: 1\n",
      "harr_unique(1): 2048 COUNT: 2\n",
      "harr_unique(2): 2050 COUNT: 3\n",
      "harr_unique(3): 3072 COUNT: 4\n",
      "harr_unique(4): 3075 COUNT: 5\n",
      "harr_unique(5): 3078 COUNT: 6\n",
      "harr_unique(6): 4096 COUNT: 7\n",
      "harr_unique(7): 4100 COUNT: 8\n",
      "harr_unique(8): 4104 COUNT: 9\n",
      "harr_unique(9): 4108 COUNT: 10\n",
      "harr_unique(10): 5120 COUNT: 11\n",
      "harr_unique(11): 5125 COUNT: 12\n",
      "harr_unique(12): 5130 COUNT: 13\n",
      "harr_unique(13): 5135 COUNT: 14\n",
      "harr_unique(14): 5140 COUNT: 15\n",
      "harr_unique(15): 6144 COUNT: 16\n",
      "harr_unique(16): 6150 COUNT: 17\n",
      "harr_unique(17): 6156 COUNT: 18\n",
      "harr_unique(18): 6162 COUNT: 19\n",
      "harr_unique(19): 6168 COUNT: 20\n",
      "harr_unique(20): 6174 COUNT: 21\n",
      "harr_unique(21): 7168 COUNT: 22\n",
      "harr_unique(22): 7175 COUNT: 23\n",
      "harr_unique(23): 7182 COUNT: 24\n",
      "harr_unique(24): 7189 COUNT: 25\n",
      "harr_unique(25): 7196 COUNT: 26\n",
      "harr_unique(26): 7203 COUNT: 27\n",
      "harr_unique(27): 7210 COUNT: 28\n",
      "harr_unique(28): 8192 COUNT: 29\n",
      "harr_unique(29): 8200 COUNT: 30\n",
      "harr_unique(30): 8208 COUNT: 31\n",
      "harr_unique(31): 8216 COUNT: 32\n",
      "harr_unique(32): 8224 COUNT: 33\n",
      "harr_unique(33): 8232 COUNT: 34\n",
      "harr_unique(34): 8240 COUNT: 35\n",
      "harr_unique(35): 8248 COUNT: 36\n",
      "harr_unique(36): 9216 COUNT: 37\n",
      "harr_unique(37): 9225 COUNT: 38\n",
      "harr_unique(38): 9234 COUNT: 39\n",
      "harr_unique(39): 9243 COUNT: 40\n",
      "harr_unique(40): 9252 COUNT: 41\n",
      "harr_unique(41): 9261 COUNT: 42\n",
      "harr_unique(42): 9270 COUNT: 43\n",
      "harr_unique(43): 9279 COUNT: 44\n",
      "harr_unique(44): 9288 COUNT: 45\n",
      "harr_unique(45): 10240 COUNT: 46\n",
      "harr_unique(46): 10250 COUNT: 47\n",
      "harr_unique(47): 10260 COUNT: 48\n",
      "harr_unique(48): 10270 COUNT: 49\n",
      "harr_unique(49): 10280 COUNT: 50\n",
      "harr_unique(50): 10290 COUNT: 51\n",
      "harr_unique(51): 10300 COUNT: 52\n",
      "harr_unique(52): 10310 COUNT: 53\n",
      "harr_unique(53): 10320 COUNT: 54\n",
      "harr_unique(54): 10330 COUNT: 55\n",
      "harr_unique(55): 11264 COUNT: 56\n",
      "harr_unique(56): 11275 COUNT: 57\n",
      "harr_unique(57): 11286 COUNT: 58\n",
      "harr_unique(58): 11297 COUNT: 59\n",
      "harr_unique(59): 11308 COUNT: 60\n",
      "harr_unique(60): 11319 COUNT: 61\n",
      "harr_unique(61): 11330 COUNT: 62\n",
      "harr_unique(62): 11341 COUNT: 63\n",
      "harr_unique(63): 11352 COUNT: 64\n",
      "harr_unique(64): 11363 COUNT: 65\n",
      "harr_unique(65): 11374 COUNT: 66\n",
      "harr_unique(66): 12288 COUNT: 67\n",
      "harr_unique(67): 12300 COUNT: 68\n",
      "harr_unique(68): 12312 COUNT: 69\n",
      "harr_unique(69): 12324 COUNT: 70\n",
      "harr_unique(70): 12336 COUNT: 71\n",
      "harr_unique(71): 12348 COUNT: 72\n",
      "harr_unique(72): 12360 COUNT: 73\n",
      "harr_unique(73): 12372 COUNT: 74\n",
      "harr_unique(74): 12384 COUNT: 75\n",
      "harr_unique(75): 12396 COUNT: 76\n",
      "harr_unique(76): 12408 COUNT: 77\n",
      "harr_unique(77): 12420 COUNT: 78\n",
      "harr_unique(78): 13312 COUNT: 79\n",
      "harr_unique(79): 13325 COUNT: 80\n",
      "harr_unique(80): 13338 COUNT: 81\n",
      "harr_unique(81): 13351 COUNT: 82\n",
      "harr_unique(82): 13364 COUNT: 83\n",
      "harr_unique(83): 13377 COUNT: 84\n",
      "harr_unique(84): 13390 COUNT: 85\n",
      "harr_unique(85): 13403 COUNT: 86\n",
      "harr_unique(86): 13416 COUNT: 87\n",
      "harr_unique(87): 13429 COUNT: 88\n",
      "harr_unique(88): 13442 COUNT: 89\n",
      "harr_unique(89): 13455 COUNT: 90\n",
      "harr_unique(90): 13468 COUNT: 91\n",
      "harr_unique(91): 14336 COUNT: 92\n",
      "harr_unique(92): 14350 COUNT: 93\n",
      "harr_unique(93): 14364 COUNT: 94\n",
      "harr_unique(94): 14378 COUNT: 95\n",
      "harr_unique(95): 14392 COUNT: 96\n",
      "harr_unique(96): 14406 COUNT: 97\n",
      "harr_unique(97): 14420 COUNT: 98\n",
      "harr_unique(98): 14434 COUNT: 99\n",
      "harr_unique(99): 14448 COUNT: 100\n",
      "harr_unique(100): 14462 COUNT: 101\n",
      "harr_unique(101): 14476 COUNT: 102\n",
      "harr_unique(102): 14490 COUNT: 103\n",
      "harr_unique(103): 14504 COUNT: 104\n",
      "harr_unique(104): 14518 COUNT: 105\n",
      "harr_unique(105): 15360 COUNT: 106\n",
      "harr_unique(106): 15375 COUNT: 107\n",
      "harr_unique(107): 15390 COUNT: 108\n",
      "harr_unique(108): 15405 COUNT: 109\n",
      "harr_unique(109): 15420 COUNT: 110\n",
      "harr_unique(110): 15435 COUNT: 111\n",
      "harr_unique(111): 15450 COUNT: 112\n",
      "harr_unique(112): 15465 COUNT: 113\n",
      "harr_unique(113): 15480 COUNT: 114\n",
      "harr_unique(114): 15495 COUNT: 115\n",
      "harr_unique(115): 15510 COUNT: 116\n",
      "harr_unique(116): 15525 COUNT: 117\n",
      "harr_unique(117): 15540 COUNT: 118\n",
      "harr_unique(118): 15555 COUNT: 119\n",
      "harr_unique(119): 15570 COUNT: 120\n",
      "harr_unique(120): 16384 COUNT: 121\n",
      "harr_unique(121): 16400 COUNT: 122\n",
      "harr_unique(122): 16416 COUNT: 123\n",
      "harr_unique(123): 16432 COUNT: 124\n",
      "harr_unique(124): 16448 COUNT: 125\n",
      "harr_unique(125): 16464 COUNT: 126\n",
      "harr_unique(126): 16480 COUNT: 127\n",
      "harr_unique(127): 16496 COUNT: 128\n",
      "harr_unique(128): 16512 COUNT: 129\n",
      "harr_unique(129): 16528 COUNT: 130\n",
      "harr_unique(130): 16544 COUNT: 131\n",
      "harr_unique(131): 16560 COUNT: 132\n",
      "harr_unique(132): 16576 COUNT: 133\n",
      "harr_unique(133): 16592 COUNT: 134\n",
      "harr_unique(134): 16608 COUNT: 135\n",
      "harr_unique(135): 16624 COUNT: 136\n",
      "harr_unique(136): 17408 COUNT: 137\n",
      "harr_unique(137): 17425 COUNT: 138\n",
      "harr_unique(138): 17442 COUNT: 139\n",
      "harr_unique(139): 17459 COUNT: 140\n",
      "harr_unique(140): 17476 COUNT: 141\n",
      "harr_unique(141): 17493 COUNT: 142\n",
      "harr_unique(142): 17510 COUNT: 143\n",
      "harr_unique(143): 17527 COUNT: 144\n",
      "harr_unique(144): 17544 COUNT: 145\n",
      "harr_unique(145): 17561 COUNT: 146\n",
      "harr_unique(146): 17578 COUNT: 147\n",
      "harr_unique(147): 17595 COUNT: 148\n",
      "harr_unique(148): 17612 COUNT: 149\n",
      "harr_unique(149): 17629 COUNT: 150\n",
      "harr_unique(150): 17646 COUNT: 151\n",
      "harr_unique(151): 17663 COUNT: 152\n",
      "harr_unique(152): 17680 COUNT: 153\n",
      "harr_unique(153): 18432 COUNT: 154\n",
      "harr_unique(154): 18450 COUNT: 155\n",
      "harr_unique(155): 18468 COUNT: 156\n",
      "harr_unique(156): 18486 COUNT: 157\n",
      "harr_unique(157): 18504 COUNT: 158\n",
      "harr_unique(158): 18522 COUNT: 159\n",
      "harr_unique(159): 18540 COUNT: 160\n",
      "harr_unique(160): 18558 COUNT: 161\n",
      "harr_unique(161): 18576 COUNT: 162\n",
      "harr_unique(162): 18594 COUNT: 163\n",
      "harr_unique(163): 18612 COUNT: 164\n",
      "harr_unique(164): 18630 COUNT: 165\n",
      "harr_unique(165): 18648 COUNT: 166\n",
      "harr_unique(166): 18666 COUNT: 167\n",
      "harr_unique(167): 18684 COUNT: 168\n",
      "harr_unique(168): 18702 COUNT: 169\n",
      "harr_unique(169): 18720 COUNT: 170\n",
      "harr_unique(170): 18738 COUNT: 171\n",
      "harr_unique(171): 19456 COUNT: 172\n",
      "harr_unique(172): 19475 COUNT: 173\n",
      "harr_unique(173): 19494 COUNT: 174\n",
      "harr_unique(174): 19513 COUNT: 175\n",
      "harr_unique(175): 19532 COUNT: 176\n",
      "harr_unique(176): 19551 COUNT: 177\n",
      "harr_unique(177): 19570 COUNT: 178\n",
      "harr_unique(178): 19589 COUNT: 179\n",
      "harr_unique(179): 19608 COUNT: 180\n",
      "harr_unique(180): 19627 COUNT: 181\n",
      "harr_unique(181): 19646 COUNT: 182\n",
      "harr_unique(182): 19665 COUNT: 183\n",
      "harr_unique(183): 19684 COUNT: 184\n",
      "harr_unique(184): 19703 COUNT: 185\n",
      "harr_unique(185): 19722 COUNT: 186\n",
      "harr_unique(186): 19741 COUNT: 187\n",
      "harr_unique(187): 19760 COUNT: 188\n",
      "harr_unique(188): 19779 COUNT: 189\n",
      "harr_unique(189): 19798 COUNT: 190\n",
      " The Total Number of unique instances is: 190\n"
     ]
    }
   ],
   "source": [
    "def hash_function(i, j, hash_key=1024, modulo_multiplier=32):\n",
    "    ## Put your code below. \n",
    "    ##return ((i + hash_key) * j) % modulo_multiplier\n",
    "    return  ((i + hash_key) * j) % (hash_key*modulo_multiplier)\n",
    "\n",
    "count =0\n",
    "hash = {}\n",
    "harr =  []\n",
    "    ##h[(i,j)] = ((i + hash_key) * j ) % modulo_multiplier\n",
    "\n",
    "for i,j in combinations(range(ncolumns), 2):\n",
    "    if i <= 6: \n",
    "        \n",
    "        print( ' Count = ' + str(count))\n",
    "        ##hash[(i,j)] = hash_function(i,j)\n",
    "    hash[(i,j)] =     hash_function(i,j)\n",
    "    harr.append( hash_function(i,j))\n",
    "\n",
    "        ##print('i = ' + str(i) + '  j = ' + str(j) + '   hash = ' + str(hash) + '     Count hash = ' + str(count+1))\n",
    "    count += 1\n",
    "    combCount = 1\n",
    "        \n",
    "for i,j in combinations(range(ncolumns),2):     \n",
    "    if i<= 6:\n",
    "        print('COUNT: '+ str(combCount) + ':  hash[(' + str(i)+ ' , '+ str( j) + ')] : ' + str(hash[i,j]))\n",
    "    combCount += 1\n",
    "        \n",
    "print(' The Total Number of Combinations is: ' + str(count))\n",
    "    \n",
    "Count1 = 0  \n",
    "harr_u = np.unique(harr)\n",
    "for u in harr_u:\n",
    "    \n",
    "    print('harr_unique(' + str(Count1) + '): ' + str(u) + ' COUNT: '+ str(Count1+1))\n",
    "    Count1 += 1\n",
    "## print (harr_u)\n",
    "## print('harr_unique(' + str(Count1-1) + '): ' + str(u) + ' COUNT: '+ str(Count1))\n",
    "print(' The Total Number of unique instances is: '+ str(Count1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Examine the key pairs and the hash values. The question is, are there any hash collisions? This can be done as follows:   \n",
    "> 5. Compute a list of the hash values for all combinations of $i$ and $j$.   \n",
    "> 6. Print the length of the list.  \n",
    "> 7. Print the length of the unique values of the hash. You can find the unique values in a list with the [numpy.unique](https://numpy.org/doc/stable/reference/generated/numpy.unique.html) function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "harr_unique(0): 1024 Reoccurrances: 1\n",
      "harr_unique(1): 2048 Reoccurrances: 1\n",
      "harr_unique(2): 2050 Reoccurrances: 1\n",
      "harr_unique(3): 3072 Reoccurrances: 1\n",
      "harr_unique(4): 3075 Reoccurrances: 1\n",
      "harr_unique(5): 3078 Reoccurrances: 1\n",
      "harr_unique(6): 4096 Reoccurrances: 1\n",
      "harr_unique(7): 4100 Reoccurrances: 1\n",
      "harr_unique(8): 4104 Reoccurrances: 1\n",
      "harr_unique(9): 4108 Reoccurrances: 1\n",
      "harr_unique(10): 5120 Reoccurrances: 1\n",
      "harr_unique(11): 5125 Reoccurrances: 1\n",
      "harr_unique(12): 5130 Reoccurrances: 1\n",
      "harr_unique(13): 5135 Reoccurrances: 1\n",
      "harr_unique(14): 5140 Reoccurrances: 1\n",
      "harr_unique(15): 6144 Reoccurrances: 1\n",
      "harr_unique(16): 6150 Reoccurrances: 1\n",
      "harr_unique(17): 6156 Reoccurrances: 1\n",
      "harr_unique(18): 6162 Reoccurrances: 1\n",
      "harr_unique(19): 6168 Reoccurrances: 1\n",
      "harr_unique(20): 6174 Reoccurrances: 1\n",
      "harr_unique(21): 7168 Reoccurrances: 1\n",
      "harr_unique(22): 7175 Reoccurrances: 1\n",
      "harr_unique(23): 7182 Reoccurrances: 1\n",
      "harr_unique(24): 7189 Reoccurrances: 1\n",
      "harr_unique(25): 7196 Reoccurrances: 1\n",
      "harr_unique(26): 7203 Reoccurrances: 1\n",
      "harr_unique(27): 7210 Reoccurrances: 1\n",
      "harr_unique(28): 8192 Reoccurrances: 1\n",
      "harr_unique(29): 8200 Reoccurrances: 1\n",
      "harr_unique(30): 8208 Reoccurrances: 1\n",
      "harr_unique(31): 8216 Reoccurrances: 1\n",
      "harr_unique(32): 8224 Reoccurrances: 1\n",
      "harr_unique(33): 8232 Reoccurrances: 1\n",
      "harr_unique(34): 8240 Reoccurrances: 1\n",
      "harr_unique(35): 8248 Reoccurrances: 1\n",
      "harr_unique(36): 9216 Reoccurrances: 1\n",
      "harr_unique(37): 9225 Reoccurrances: 1\n",
      "harr_unique(38): 9234 Reoccurrances: 1\n",
      "harr_unique(39): 9243 Reoccurrances: 1\n",
      "harr_unique(40): 9252 Reoccurrances: 1\n",
      "harr_unique(41): 9261 Reoccurrances: 1\n",
      "harr_unique(42): 9270 Reoccurrances: 1\n",
      "harr_unique(43): 9279 Reoccurrances: 1\n",
      "harr_unique(44): 9288 Reoccurrances: 1\n",
      "harr_unique(45): 10240 Reoccurrances: 1\n",
      "harr_unique(46): 10250 Reoccurrances: 1\n",
      "harr_unique(47): 10260 Reoccurrances: 1\n",
      "harr_unique(48): 10270 Reoccurrances: 1\n",
      "harr_unique(49): 10280 Reoccurrances: 1\n",
      "harr_unique(50): 10290 Reoccurrances: 1\n",
      "harr_unique(51): 10300 Reoccurrances: 1\n",
      "harr_unique(52): 10310 Reoccurrances: 1\n",
      "harr_unique(53): 10320 Reoccurrances: 1\n",
      "harr_unique(54): 10330 Reoccurrances: 1\n",
      "harr_unique(55): 11264 Reoccurrances: 1\n",
      "harr_unique(56): 11275 Reoccurrances: 1\n",
      "harr_unique(57): 11286 Reoccurrances: 1\n",
      "harr_unique(58): 11297 Reoccurrances: 1\n",
      "harr_unique(59): 11308 Reoccurrances: 1\n",
      "harr_unique(60): 11319 Reoccurrances: 1\n",
      "harr_unique(61): 11330 Reoccurrances: 1\n",
      "harr_unique(62): 11341 Reoccurrances: 1\n",
      "harr_unique(63): 11352 Reoccurrances: 1\n",
      "harr_unique(64): 11363 Reoccurrances: 1\n",
      "harr_unique(65): 11374 Reoccurrances: 1\n",
      "harr_unique(66): 12288 Reoccurrances: 1\n",
      "harr_unique(67): 12300 Reoccurrances: 1\n",
      "harr_unique(68): 12312 Reoccurrances: 1\n",
      "harr_unique(69): 12324 Reoccurrances: 1\n",
      "harr_unique(70): 12336 Reoccurrances: 1\n",
      "harr_unique(71): 12348 Reoccurrances: 1\n",
      "harr_unique(72): 12360 Reoccurrances: 1\n",
      "harr_unique(73): 12372 Reoccurrances: 1\n",
      "harr_unique(74): 12384 Reoccurrances: 1\n",
      "harr_unique(75): 12396 Reoccurrances: 1\n",
      "harr_unique(76): 12408 Reoccurrances: 1\n",
      "harr_unique(77): 12420 Reoccurrances: 1\n",
      "harr_unique(78): 13312 Reoccurrances: 1\n",
      "harr_unique(79): 13325 Reoccurrances: 1\n",
      "harr_unique(80): 13338 Reoccurrances: 1\n",
      "harr_unique(81): 13351 Reoccurrances: 1\n",
      "harr_unique(82): 13364 Reoccurrances: 1\n",
      "harr_unique(83): 13377 Reoccurrances: 1\n",
      "harr_unique(84): 13390 Reoccurrances: 1\n",
      "harr_unique(85): 13403 Reoccurrances: 1\n",
      "harr_unique(86): 13416 Reoccurrances: 1\n",
      "harr_unique(87): 13429 Reoccurrances: 1\n",
      "harr_unique(88): 13442 Reoccurrances: 1\n",
      "harr_unique(89): 13455 Reoccurrances: 1\n",
      "harr_unique(90): 13468 Reoccurrances: 1\n",
      "harr_unique(91): 14336 Reoccurrances: 1\n",
      "harr_unique(92): 14350 Reoccurrances: 1\n",
      "harr_unique(93): 14364 Reoccurrances: 1\n",
      "harr_unique(94): 14378 Reoccurrances: 1\n",
      "harr_unique(95): 14392 Reoccurrances: 1\n",
      "harr_unique(96): 14406 Reoccurrances: 1\n",
      "harr_unique(97): 14420 Reoccurrances: 1\n",
      "harr_unique(98): 14434 Reoccurrances: 1\n",
      "harr_unique(99): 14448 Reoccurrances: 1\n",
      "harr_unique(100): 14462 Reoccurrances: 1\n",
      "harr_unique(101): 14476 Reoccurrances: 1\n",
      "harr_unique(102): 14490 Reoccurrances: 1\n",
      "harr_unique(103): 14504 Reoccurrances: 1\n",
      "harr_unique(104): 14518 Reoccurrances: 1\n",
      "harr_unique(105): 15360 Reoccurrances: 1\n",
      "harr_unique(106): 15375 Reoccurrances: 1\n",
      "harr_unique(107): 15390 Reoccurrances: 1\n",
      "harr_unique(108): 15405 Reoccurrances: 1\n",
      "harr_unique(109): 15420 Reoccurrances: 1\n",
      "harr_unique(110): 15435 Reoccurrances: 1\n",
      "harr_unique(111): 15450 Reoccurrances: 1\n",
      "harr_unique(112): 15465 Reoccurrances: 1\n",
      "harr_unique(113): 15480 Reoccurrances: 1\n",
      "harr_unique(114): 15495 Reoccurrances: 1\n",
      "harr_unique(115): 15510 Reoccurrances: 1\n",
      "harr_unique(116): 15525 Reoccurrances: 1\n",
      "harr_unique(117): 15540 Reoccurrances: 1\n",
      "harr_unique(118): 15555 Reoccurrances: 1\n",
      "harr_unique(119): 15570 Reoccurrances: 1\n",
      "harr_unique(120): 16384 Reoccurrances: 1\n",
      "harr_unique(121): 16400 Reoccurrances: 1\n",
      "harr_unique(122): 16416 Reoccurrances: 1\n",
      "harr_unique(123): 16432 Reoccurrances: 1\n",
      "harr_unique(124): 16448 Reoccurrances: 1\n",
      "harr_unique(125): 16464 Reoccurrances: 1\n",
      "harr_unique(126): 16480 Reoccurrances: 1\n",
      "harr_unique(127): 16496 Reoccurrances: 1\n",
      "harr_unique(128): 16512 Reoccurrances: 1\n",
      "harr_unique(129): 16528 Reoccurrances: 1\n",
      "harr_unique(130): 16544 Reoccurrances: 1\n",
      "harr_unique(131): 16560 Reoccurrances: 1\n",
      "harr_unique(132): 16576 Reoccurrances: 1\n",
      "harr_unique(133): 16592 Reoccurrances: 1\n",
      "harr_unique(134): 16608 Reoccurrances: 1\n",
      "harr_unique(135): 16624 Reoccurrances: 1\n",
      "harr_unique(136): 17408 Reoccurrances: 1\n",
      "harr_unique(137): 17425 Reoccurrances: 1\n",
      "harr_unique(138): 17442 Reoccurrances: 1\n",
      "harr_unique(139): 17459 Reoccurrances: 1\n",
      "harr_unique(140): 17476 Reoccurrances: 1\n",
      "harr_unique(141): 17493 Reoccurrances: 1\n",
      "harr_unique(142): 17510 Reoccurrances: 1\n",
      "harr_unique(143): 17527 Reoccurrances: 1\n",
      "harr_unique(144): 17544 Reoccurrances: 1\n",
      "harr_unique(145): 17561 Reoccurrances: 1\n",
      "harr_unique(146): 17578 Reoccurrances: 1\n",
      "harr_unique(147): 17595 Reoccurrances: 1\n",
      "harr_unique(148): 17612 Reoccurrances: 1\n",
      "harr_unique(149): 17629 Reoccurrances: 1\n",
      "harr_unique(150): 17646 Reoccurrances: 1\n",
      "harr_unique(151): 17663 Reoccurrances: 1\n",
      "harr_unique(152): 17680 Reoccurrances: 1\n",
      "harr_unique(153): 18432 Reoccurrances: 1\n",
      "harr_unique(154): 18450 Reoccurrances: 1\n",
      "harr_unique(155): 18468 Reoccurrances: 1\n",
      "harr_unique(156): 18486 Reoccurrances: 1\n",
      "harr_unique(157): 18504 Reoccurrances: 1\n",
      "harr_unique(158): 18522 Reoccurrances: 1\n",
      "harr_unique(159): 18540 Reoccurrances: 1\n",
      "harr_unique(160): 18558 Reoccurrances: 1\n",
      "harr_unique(161): 18576 Reoccurrances: 1\n",
      "harr_unique(162): 18594 Reoccurrances: 1\n",
      "harr_unique(163): 18612 Reoccurrances: 1\n",
      "harr_unique(164): 18630 Reoccurrances: 1\n",
      "harr_unique(165): 18648 Reoccurrances: 1\n",
      "harr_unique(166): 18666 Reoccurrances: 1\n",
      "harr_unique(167): 18684 Reoccurrances: 1\n",
      "harr_unique(168): 18702 Reoccurrances: 1\n",
      "harr_unique(169): 18720 Reoccurrances: 1\n",
      "harr_unique(170): 18738 Reoccurrances: 1\n",
      "harr_unique(171): 19456 Reoccurrances: 1\n",
      "harr_unique(172): 19475 Reoccurrances: 1\n",
      "harr_unique(173): 19494 Reoccurrances: 1\n",
      "harr_unique(174): 19513 Reoccurrances: 1\n",
      "harr_unique(175): 19532 Reoccurrances: 1\n",
      "harr_unique(176): 19551 Reoccurrances: 1\n",
      "harr_unique(177): 19570 Reoccurrances: 1\n",
      "harr_unique(178): 19589 Reoccurrances: 1\n",
      "harr_unique(179): 19608 Reoccurrances: 1\n",
      "harr_unique(180): 19627 Reoccurrances: 1\n",
      "harr_unique(181): 19646 Reoccurrances: 1\n",
      "harr_unique(182): 19665 Reoccurrances: 1\n",
      "harr_unique(183): 19684 Reoccurrances: 1\n",
      "harr_unique(184): 19703 Reoccurrances: 1\n",
      "harr_unique(185): 19722 Reoccurrances: 1\n",
      "harr_unique(186): 19741 Reoccurrances: 1\n",
      "harr_unique(187): 19760 Reoccurrances: 1\n",
      "harr_unique(188): 19779 Reoccurrances: 1\n",
      "harr_unique(189): 19798 Reoccurrances: 1\n",
      " The Total Number of unique instances is: 190\n"
     ]
    }
   ],
   "source": [
    "## Put your code below. \n",
    "\n",
    "Count1 = 0\n",
    "ui, harr_u = np.unique(harr,return_counts = True)\n",
    "\n",
    "for uindex in range(len(harr_u)):\n",
    "    Count1 += 1\n",
    "    print('harr_unique(' + str(uindex) + '): ' + str(ui[uindex]) + ' Reoccurrances: '+ str(harr_u[uindex]))\n",
    "\n",
    "print(' The Total Number of unique instances is: '+ str(Count1))\n",
    "\n",
    "##print (ui)\n",
    "##print (harr_u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Examine the results you have printed. Is there any evidence of hash key collisions?     \n",
    "> The ratio of $m/n$ is deliberately kept high since the simple hash function has no collision resolution mechanism. Optionally, you can try reducing this ration (the multiplier) to 16 and 8, noting the increase in hash collisions.  \n",
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No: There is no evidence of hash key collisions. There are 190 Combinations with 190 unique values. Each reoccurance has been determined for the unique hash values indicating the number of collisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The map process\n",
    "\n",
    "We are constructing this example a map and a reduce process. The processes are intended to compute the hypothesis test for differences of means between all the pairs of vectors. The first step is the map process, which creates the keys, or values of $i$ and $j$ for these pairs.   \n",
    "\n",
    "> **Exercise 1-2:** You will now create the code for the map task which build a data frame with $i, j$ key pairs indexed by the hash. By the following steps you will create code that represents a map task.  \n",
    "> 1. Create a data frame with two columns $i$ and $j$ with rows $= hash_key * modulo_multiplier $ and set all values to $= numpy.nan$.\n",
    "> 2. Create a loop over all combinations of the pairs of i and j.   \n",
    "> 2. Compute the hash key value for the indexes, i and j.  \n",
    "> 4. Add the $i$ and $j$ values to the row indexed by the hash key.  \n",
    "> 5. Return the hash table. \n",
    "> 6. Execute the function to create the hash table.  \n",
    "> 7. Compute and print the length of the hash table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-97-9127a3c5f2e7>, line 43)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-97-9127a3c5f2e7>\"\u001b[1;36m, line \u001b[1;32m43\u001b[0m\n\u001b[1;33m    for i,j in combinations(range(ncols), 2):\u001b[0m\n\u001b[1;37m                                            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def map_hypothesis(vars, hash_key=1024, modulo_multiplier=32):\n",
    "    ## Put your code below. \n",
    "    print('VARS ARE HERE: 1000 rows with 20 columns')\n",
    "    print(vars)\n",
    "    print('VARS Rows')\n",
    "    print(len(vars))\n",
    "    print('SHAPE Rows')\n",
    "    print(vars.shape)\n",
    "    arr = [[], []]\n",
    "    arr_vals = []\n",
    "    hash_arr = []\n",
    "    arr = [[0,0],\n",
    "           [0,1],\n",
    "           [1,0],\n",
    "           [1,1],\n",
    "           [1,2]]\n",
    "    lenrow = len(arr)\n",
    "    lencol = len(arr[0])\n",
    "    ind = 0\n",
    "    ##for x in range(lenrow):\n",
    "    ##    for y in range(lencol):\n",
    "    ##        arr_vals[ind]= [(i),(j),np.NAN]\n",
    "    ##        ind += 1\n",
    "    \n",
    "    ##ncolumns = 20\n",
    "    ##nr.seed(234)\n",
    "    ##normal_vars = nr.normal(size=(1000,ncolumns))\n",
    "    \n",
    "    \n",
    "    \n",
    "    ## make the index or rows\n",
    "    df = pd.DataFrame(arr)\n",
    "  \n",
    "    ##df.index = hash_key * modulo_multiplier\n",
    "    df.columns = ['i','j']\n",
    "    print(df)\n",
    "    ##df.loc = pd.columns\n",
    "    ##print(df)\n",
    "    ncols = vars.shape[1]\n",
    "    ncols1 = len(vars[0])\n",
    "    print('ncols-shape :' + str(ncols) + ' ncols1-len(vars[0]) :' + str(ncols1)\n",
    "    ncols = 20\n",
    "    for i,j in combinations(range(ncols), 2):\n",
    "        newRow = [i,j,np.NAN]\n",
    "        arr_vals.append(newRow)\n",
    "        ind += 1\n",
    "        \n",
    "        ##hash_function(i, j, hash_key=1024, modulo_multiplier=32):\n",
    "        hash[(i,j)] = hash_function(i,j)\n",
    "        \n",
    "        ##arr[i,j] = np.nan\n",
    "        ##df.append(new_row, ignore_index = True)\n",
    "        ##arr[int(str(i)),int(str(j))] = np.nan\n",
    "        \n",
    "        ## Put your code below. \n",
    "        ## Compute the hash key and added the values to the \n",
    "        ## row of the data frame. \n",
    "    ##print(df.loc[:,'i','j'])\n",
    "    df_arr = pd.DataFrame(arr_vals)\n",
    "    df_arr.columns = ['i','j', 'hash']\n",
    "    combCount = 1\n",
    "    for x,y in combinations(range(ncolumns), 2):\n",
    "        hash_arr.append(hash_function(x,y))\n",
    "        print('COUNT: '+ str(combCount) + ':  hash[(' + str(x)+ ' , '+ str( y) + ')] : ' + str(hash_function(x,y)))\n",
    "    df_arr.loc[:,'hash'] = hash_arr\n",
    "    print(df_arr)\n",
    "    return df_arr \n",
    "    ##return hash_table\n",
    "    ##return print(normal_vars)\n",
    "\n",
    "##normal_vars = nr.normal(size=(1000,ncolumns))\n",
    "hash_table = map_hypothesis(normal_vars)\n",
    "print('Length of Hash Table is: '+ str(len(hash_table)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The shuffle and reduce task\n",
    "\n",
    "Now that you have the keys for the pairwise combinations of the vectors it is time to perform the reduce process. The reduce process computes the pair-wise t-statistics and p-values. These statistical values are indexed by the keys of the pair of vectors. This process reduces the full vectors of values down to just two numbers for each pair of vectors. \n",
    "\n",
    "> **Exercise 1-3:** You will now create and apply the following code for the reduce process:   \n",
    "> 1. Create an empty data frame with columns, `i`, `j`, `t_statistic`, and `p_value`.    \n",
    "> 2. Using a for loop iterate over all possible (hashed) keys of the data frame. An if statement is used to test if these are valid values of the key, i. Use the [numpy.isnan](https://numpy.org/doc/stable/reference/generated/numpy.isnan.html) function for this test.  \n",
    "> 3. Extract the values of i and j from the input data frame. \n",
    "> 4. Using keys, compute the t-statistic and p-value using [scipy.stats import ttest_ind](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind.html).\n",
    "> 5. Append a row to the output data frame.\n",
    "> 6. Return the data frame, sorted in ascending order, using the [Pandas.DataFrame.sort_values](https://turned.pydata.org/docs/reference/api/pandas.DataFrame.sort_values.html) method and re-indexed using the [Pandas.DataFrame.reset_index](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.reset_index.html) method.    \n",
    "> 7. Execute your function and save the returned data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ttest_indResult(statistic=array([-2.4115317]), pvalue=array([0.01597535]))\n",
      "Empty DataFrame\n",
      "Columns: [i, j, t_statistic, p_value]\n",
      "Index: []\n",
      "      0   1   2\n",
      "0     0   1   0\n",
      "1     0   2   0\n",
      "2     0   3   0\n",
      "3     0   4   0\n",
      "4     0   5   0\n",
      "5     0   6   0\n",
      "6     0   7   0\n",
      "7     0   8   0\n",
      "8     0   9   0\n",
      "9     0  10   0\n",
      "10    0  11   0\n",
      "11    0  12   0\n",
      "12    0  13   0\n",
      "13    0  14   0\n",
      "14    0  15   0\n",
      "15    0  16   0\n",
      "16    0  17   0\n",
      "17    0  18   0\n",
      "18    0  19   0\n",
      "19    1   2   2\n",
      "20    1   3   3\n",
      "21    1   4   4\n",
      "22    1   5   5\n",
      "23    1   6   6\n",
      "24    1   7   7\n",
      "25    1   8   8\n",
      "26    1   9   9\n",
      "27    1  10  10\n",
      "28    1  11  11\n",
      "29    1  12  12\n",
      "30    1  13  13\n",
      "31    1  14  14\n",
      "32    1  15  15\n",
      "33    1  16  16\n",
      "34    1  17  17\n",
      "35    1  18  18\n",
      "36    1  19  19\n",
      "37    2   3   6\n",
      "38    2   4   8\n",
      "39    2   5  10\n",
      "40    2   6  12\n",
      "41    2   7  14\n",
      "42    2   8  16\n",
      "43    2   9  18\n",
      "44    2  10  20\n",
      "45    2  11  22\n",
      "46    2  12  24\n",
      "47    2  13  26\n",
      "48    2  14  28\n",
      "49    2  15  30\n",
      "50    2  16   0\n",
      "51    2  17   2\n",
      "52    2  18   4\n",
      "53    2  19   6\n",
      "54    3   4  12\n",
      "55    3   5  15\n",
      "56    3   6  18\n",
      "57    3   7  21\n",
      "58    3   8  24\n",
      "59    3   9  27\n",
      "60    3  10  30\n",
      "61    3  11   1\n",
      "62    3  12   4\n",
      "63    3  13   7\n",
      "64    3  14  10\n",
      "65    3  15  13\n",
      "66    3  16  16\n",
      "67    3  17  19\n",
      "68    3  18  22\n",
      "69    3  19  25\n",
      "70    4   5  20\n",
      "71    4   6  24\n",
      "72    4   7  28\n",
      "73    4   8   0\n",
      "74    4   9   4\n",
      "75    4  10   8\n",
      "76    4  11  12\n",
      "77    4  12  16\n",
      "78    4  13  20\n",
      "79    4  14  24\n",
      "80    4  15  28\n",
      "81    4  16   0\n",
      "82    4  17   4\n",
      "83    4  18   8\n",
      "84    4  19  12\n",
      "85    5   6  30\n",
      "86    5   7   3\n",
      "87    5   8   8\n",
      "88    5   9  13\n",
      "89    5  10  18\n",
      "90    5  11  23\n",
      "91    5  12  28\n",
      "92    5  13   1\n",
      "93    5  14   6\n",
      "94    5  15  11\n",
      "95    5  16  16\n",
      "96    5  17  21\n",
      "97    5  18  26\n",
      "98    5  19  31\n",
      "99    6   7  10\n",
      "100   6   8  16\n",
      "101   6   9  22\n",
      "102   6  10  28\n",
      "103   6  11   2\n",
      "104   6  12   8\n",
      "105   6  13  14\n",
      "106   6  14  20\n",
      "107   6  15  26\n",
      "108   6  16   0\n",
      "109   6  17   6\n",
      "110   6  18  12\n",
      "111   6  19  18\n",
      "112   7   8  24\n",
      "113   7   9  31\n",
      "114   7  10   6\n",
      "115   7  11  13\n",
      "116   7  12  20\n",
      "117   7  13  27\n",
      "118   7  14   2\n",
      "119   7  15   9\n",
      "120   7  16  16\n",
      "121   7  17  23\n",
      "122   7  18  30\n",
      "123   7  19   5\n",
      "124   8   9   8\n",
      "125   8  10  16\n",
      "126   8  11  24\n",
      "127   8  12   0\n",
      "128   8  13   8\n",
      "129   8  14  16\n",
      "130   8  15  24\n",
      "131   8  16   0\n",
      "132   8  17   8\n",
      "133   8  18  16\n",
      "134   8  19  24\n",
      "135   9  10  26\n",
      "136   9  11   3\n",
      "137   9  12  12\n",
      "138   9  13  21\n",
      "139   9  14  30\n",
      "140   9  15   7\n",
      "141   9  16  16\n",
      "142   9  17  25\n",
      "143   9  18   2\n",
      "144   9  19  11\n",
      "145  10  11  14\n",
      "146  10  12  24\n",
      "147  10  13   2\n",
      "148  10  14  12\n",
      "149  10  15  22\n",
      "150  10  16   0\n",
      "151  10  17  10\n",
      "152  10  18  20\n",
      "153  10  19  30\n",
      "154  11  12   4\n",
      "155  11  13  15\n",
      "156  11  14  26\n",
      "157  11  15   5\n",
      "158  11  16  16\n",
      "159  11  17  27\n",
      "160  11  18   6\n",
      "161  11  19  17\n",
      "162  12  13  28\n",
      "163  12  14   8\n",
      "164  12  15  20\n",
      "165  12  16   0\n",
      "166  12  17  12\n",
      "167  12  18  24\n",
      "168  12  19   4\n",
      "169  13  14  22\n",
      "170  13  15   3\n",
      "171  13  16  16\n",
      "172  13  17  29\n",
      "173  13  18  10\n",
      "174  13  19  23\n",
      "175  14  15  18\n",
      "176  14  16   0\n",
      "177  14  17  14\n",
      "178  14  18  28\n",
      "179  14  19  10\n",
      "180  15  16  16\n",
      "181  15  17  31\n",
      "182  15  18  14\n",
      "183  15  19  29\n",
      "184  16  17  16\n",
      "185  16  18   0\n",
      "186  16  19  16\n",
      "187  17  18  18\n",
      "188  17  19   3\n",
      "189  18  19  22\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'p_value'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-81-771472ae5686>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m \u001b[0mtest_stats\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreduce_significance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhash_table\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormal_vars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-81-771472ae5686>\u001b[0m in \u001b[0;36mreduce_significance\u001b[1;34m(hash_table, values)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[1;31m## Sort and return the results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[1;31m#test_results.sort_values('p_value', axis=0, ascending=True# , inplace=True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtest_results\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'p_value'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36msort_values\u001b[1;34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[0m\n\u001b[0;32m   5453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5454\u001b[0m             \u001b[0mby\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mby\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5455\u001b[1;33m             \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5456\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5457\u001b[0m             \u001b[1;31m# need to rewrap column in Series to apply key function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_get_label_or_level_values\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1682\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1683\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1684\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1686\u001b[0m         \u001b[1;31m# Check for duplicates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'p_value'"
     ]
    }
   ],
   "source": [
    "def reduce_significance(hash_table, values):  \n",
    "    ## Create a data framreturn the results of the \n",
    "    ## the reduce process. The results are grouped by the first \n",
    "    ## index i. \n",
    "    ncolumns = 20\n",
    "    nr.seed(234)\n",
    "    normal_vars = nr.normal(size=(1000,ncolumns))\n",
    "    ##print('The means of the columns are\\n', np.mean(normal_vars, axis = 0))\n",
    "    ##print('\\nThe variances of the columns are\\n', np.var(normal_vars, axis = 0))\n",
    "    tt =ttest_ind(normal_vars[:,0], nr.normal(loc = 0.01, size=(1000,1)))\n",
    "    print( tt)\n",
    "    test_results = pd.DataFrame(columns=['i','j','t_statistic','p_value'])\n",
    "    print('TEST_RESULTS: ' + test_results)\n",
    "    ## As a substitute for a shuffle we will use a simple search \n",
    "    ## through the data frame  \n",
    "    count2 = 0\n",
    "    for hash in range(hash_table.shape[0]): \n",
    "    ## Put your code below. \n",
    "        if not np.isnan(hash_table.iloc[hash,0]):\n",
    "            ##print(hash)\n",
    "            count2 += 1\n",
    "    i_arr = []\n",
    "    j_arr = []\n",
    "    data = []\n",
    "    hash_arr  = []\n",
    "    tt_arr = []\n",
    "    pv_arr = []\n",
    "    for i,j in combinations(range(ncolumns), 2):\n",
    "        i_arr.append(i)\n",
    "        j_arr.append(j)\n",
    "        \n",
    "        hash_arr = hash_function(i,j)\n",
    "        data.append((i,j,hash_function(i,j)))\n",
    "        \n",
    "        \n",
    "    test_results = pd.DataFrame(data)\n",
    "    print(test_results)\n",
    "    #test_results[:,'i'] = pd.DataFrame(hash_table.loc[:,'i'])\n",
    "    #df_j = pd.DataFrame(hash_table.loc[:,'j'])\n",
    "    #print('df_i : ' + df_i)\n",
    "    #print(df_j)\n",
    "    #print('BEFORE .values')\n",
    "    #print(df_i.values)\n",
    "    #df_i_1D = []\n",
    "    #df_j_1D = []\n",
    "    #for item in df_i.values:\n",
    "    #    df_i_1D.append(df_i.values[0])\n",
    "    #for itemj in df_j.values:\n",
    "    #    df_j_1D.append(df_i.values[0])\n",
    "    ##print('DTYPE: ' + np.dtype(df_i)\n",
    "    #test_results.loc[:,'i'] = df_i_1D\n",
    "    #test_results.loc[:,'j'] = df_j_1D\n",
    "    ##print('TEST RESULTS II: ' + test_results)\n",
    "    ## Given the i,j pairs we need to compute the t-statistic and the p-value.   \n",
    "    ## This is the reduce step, since for each i,j pair there is only \n",
    "    ## a t-statistic and a p-value. \n",
    "    ## Put your code below. \n",
    "           \n",
    "    ## Sort and return the results\n",
    "    #test_results.sort_values('p_value', axis=0, ascending=True# , inplace=True)\n",
    "    return test_results.sort_values('p_value', axis=0, ascending=True).reset_index(drop=True)        \n",
    "        \n",
    "\n",
    "test_stats = reduce_significance(hash_table, normal_vars)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 8. In the cell below, create a filter for pair test cases which are significant and save these cases in a data frame. \n",
    "> 9. Print the number (len) of significant results.\n",
    "> 10. Print the rows with the significant test results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "significance_level = 0.05\n",
    "## Put your code below. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Notice the large number of apparently significant tests. Answer the following questions:  \n",
    "> 1. Is the number of false positive cases higher than expected?    \n",
    "> 2. Examine which of the iid Normal vectors contribute to the false positive results. Are there vectors which contribute multiple times?   \n",
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonferroni correction  \n",
    "\n",
    "Several adjustments to the multiple comparisons problem have been proposed. In Dunn published a method know as the **Bonferroni correction** in 1961. The Bonferroni correction is a widely used method to reduce the false positive rate of hypothesis tests.  The adjustment is simple:\n",
    "$$\\alpha_b = \\frac{\\alpha}{m}\\\\\n",
    "with\\\\ \n",
    "m =\\ number\\ of\\ groups$$\n",
    "\n",
    "Can the Bonferroni correction help? Yes, by greatly increasing the confidence level required for a statistically significant result. The problem with the Bonferroni correction is the reduction in power as the  grows smaller. For big data problems with large numbers of groups, this issue can be especially serious. \n",
    "\n",
    "**Exercise 1-4:** You will now apply the Bonferroni correction to the iid Normal vectors. To do so, you will compute the Bonferroni threshold and the apply it to the p-values:   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code below        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Even with the Bonferroni correction we have some false significance tests, if only just barely!    \n",
    "> **End of exercise.**\n",
    "\n",
    "But, can we detect small effect with Bonferroni correction, as this method significantly reduces power of tests? Execute the code in the cell below, which compares a standard Normal to a Normal with a small mean (effect size), to find out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr.seed(567)\n",
    "print(significance_bonferroni)\n",
    "ttest_ind(normal_vars[:,0], nr.normal(loc = 0.01, size=(1000,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the Bonferroni correction, this difference in means would not be found significant. This illustrates the downside of the correction, which may prevent detection of significant effects, while still finding false significance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## False Discovery Rate Control Methods \n",
    "\n",
    "We have seen the potential pitfalls of multiple hypothesis testing. Further, we have seen that a simple approach to **false discovery rate (FDR) control** is not effective. You will now apply more sophisticated FDR control methods to control the FDR. \n",
    "\n",
    "Inflammatory bowel disease is an auto immune disease that is characterized by chronic inflammation in the digestive tract. In 2020, there were around 2.5 million people with inflammatory bowel disease in the United States. It is estimated that the prevalence of IBD among U.S. population will rise to around 3.5 million by 2030.There are two forms of IBD: Ulcerative Colitis (UC) and Crohn’s disease (CD). \n",
    "\n",
    "The specific problem we will explore is to determine which genes lead to expression of a certain disease. In this example, there are gene expression data for 97 patients. Some of these patients have ulcerative colitis and others have Crohn's disease, which are believed to be genetically inherited.    \n",
    "\n",
    "One approach to this problem is to perform hypothesis tests on the expression of the genes between patients with the two conditions. Since there are over 10,000 genes there is considerable chance for false discovery. Therefore, careful application of FDR control is required.\n",
    "\n",
    "To continue with the example, execute the code in the cell below to load the data and print the dimensionality of the data frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_data = pd.read_csv('../data/ColonDiseaseGeneData-Cleaned.csv')\n",
    "print(gene_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are data from 97 patients for 10,497 genes. A large number of hypothesis tests are required!     \n",
    "\n",
    "Execute the code in the cell below to view the first 5 columns of the data frame, which includes the expression of the first 4 genes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " print(gene_data.iloc[:,:5])      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Holm's method\n",
    "\n",
    "You will apply two FDR control methods to these data.These methods attempt to conod trol the FDR while not being overly conservative like the Bonferronic correction. The first of these Holm's method.    \n",
    "\n",
    "The Holm's method operates on the ordered set of p-values, $D = \\{ p_{(1)}, p_{(2)}, p_{(3)}, \\ldots, p_{(n)} \\}$. The threshold for the $ith$ p-value, $p(i) is:  \n",
    "\n",
    "$$p(i) \\le Threshold(Holm's) = \\frac{\\alpha}{N - i + 1}$$\n",
    "\n",
    "For example: for the 10th ordered p-value with 1,000 total tests (genes) and significance level of 0.05, the cutoff is:   \n",
    "\n",
    "$$p(10) \\le \\frac{0.05}{1000 - 10 + 1} = 0.00005045$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map process  \n",
    "\n",
    "> **Exercise 01-4:** To start the processing of these data you will first create and execute code for a map process. The map process groups the data by the patient's disease into data frame, ulcerative, crohns. The keys for each of these key-value pairs are the gene identifier. Notice that one key is all that is needed in this case. Now do the following to create and execute a function, `map_gene`:   \n",
    "> 1. Create a logical mask and group the values by `Disease State` into two data frames.\n",
    "> 2. Return the transpose of the two data frames, removing the `Disease State` values. The result of this operation should be data frames with gene expressions in the columns and the gene identifier as the row index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_gene(gene_data):  \n",
    "    ## First, separate the columns by disease type  \n",
    "    ## Put your code below. \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "ulcerative, crohns = map_gene(gene_data)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 3. Execute the code in the cells below to display the heads of these data frames and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ulcerative.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crohns.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce process \n",
    "\n",
    "> **Exercise 01-5:** With the key-value pairs organized by disease state, it is time to create and execute code of a reduce process. The reduce process will compute the pairwise t-statistics and p-values for each gene and return the sorted results. Specifically, your `gene_test` with arguments of the two mapped data frames will do the following:   \n",
    "> 1. Create an empty data frame with columns gene, t_statistics, and p-value.\n",
    "> 2. A for loop iterates over the keys of either of the data frames.  \n",
    "> 3. Compute the t-statistic and p-value for the gene (key).\n",
    "> 4. Append the results to the data frame.   \n",
    "> 5. Sort the results data frame, inplace, into ascending order.\n",
    "> 6. Return the resulting data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def gene_test(ulcerative, crohns):  \n",
    "    test_results = pd.DataFrame(columns=['gene','t_statistic','p_value'])\n",
    "    ## Put your code below. \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return test_results.set_index('gene')\n",
    "    \n",
    "gene_statistics = gene_test(ulcerative, crohns)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Significance of results \n",
    "\n",
    "With the gene data reduced to the t-test statistics, you will now determine the significance of these tests. It is important to understand that scientists believe that expression of a disease, like Corhn's, is only in a small number of genes.  \n",
    "\n",
    "> **Exercise 01-6:** As a first step in understanding the gene expression significance complete and execute the code in the cell below to find the number of 'significant' genes using the simple single hypothesis test cutoff criteria.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "significance_level =0.05\n",
    "## Put your code below. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Does this large number of 'statistically significant' results appear credible, given that only a few genes are thought to have significant expression for this disease?    \n",
    "> **End of exercise.**\n",
    "\n",
    "> **Exercise 01-7:** We have already seen that the Bonferroni correction is a rather conservative approach to testing the significance of large numbers of hypotheses. You will now use the Bonferroni correction to test the significance of the gene expression, by completing the code in the cell below.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code below. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The foregoing result seems reasonable, but is it too conservative?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 01-08:** You will now apply the Holms method to determining significance of the gene expression test results. In the cell below complete the `holms_significance` function with arguments of the results data frame and the significance level. This function does the following:  \n",
    "> 1. Find the number of test results and compute the numerator used for the cutoff calculation. \n",
    "> 2. Compute the vector of thresholds using the Holms formula. Use the Python `range`function to get the values of the index i. But, keep in mind that range produces a zero-indexed iterator, and the algorithm needs a one-indexed list.  Use the [numpy.divide](https://numpy.org/doc/stable/reference/generated/numpy.divide.html) function to perform the vector divide. Save these threshold values in a data frame in a 'holms_threshold' column.   \n",
    "> 3. Using the threshold values compute a logical vector and save it in a column names 'significance' in the data frame.\n",
    "> 4. Return the data frame.\n",
    "> Finally, execute the function and save the results in a data frame. Then find the length of the subset where the 'significance' value is True. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def holms_significance(test_results, significance):\n",
    "    ## First compute the thresholds for each of the ordered tests\n",
    "    ## Put your code below. \n",
    "\n",
    "    \n",
    "    \n",
    "    ## Now we test the significance of the ordered p-values \n",
    "\n",
    "    return test_results\n",
    "\n",
    "holms_results = holms_significance(gene_statistics, significance_level)    \n",
    "len(holms_results.loc[holms_results.loc[:,'significant'],:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Despite the general properties that the Holm's method is considered less conservative than the Bonferroni correction the results agree in this case. Does this agreement give you some confidence in the result and why?  \n",
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can visualize the results of the Holm's method test. The plot has two key elements:  \n",
    "1. Plot the curve of the p-values vs. the order number, i. The line is color coded by significance or not.\n",
    "2. Plot the threshold line. This line is straight since the threshold is a linear function of i."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_significance(results, threshold):\n",
    "    results['number'] = range(len(results))\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    sns.lineplot(x='number',y=threshold, data=results, ax=ax, color='black', linewidth=0.5)\n",
    "    sns.scatterplot(x='number',y='p_value', hue='significant', data=results, s=3, ax=ax)\n",
    "    ax.set_title('Significance of gene expression')\n",
    "    ax.set_xlabel('Gene number')\n",
    "    ax.set_ylabel('p-value')\n",
    "    \n",
    "plot_significance(holms_results.iloc[:500,:], 'holms_threshold')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the following about this plot:  \n",
    "1. The p-value significance line crosses the threshold point at an apparent break point.   \n",
    "2. The significant p-values are all very small since there are so many tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benamini-Hochberg FDR Control \n",
    "\n",
    "The Benamini-Hochberg FDR control algorithm is another way to control false discoveries. Stat with an ordered set of $n$ p-values, $D = \\{ p_{(1)}, p_{(2)}, p_{(3)}, \\ldots, p_{(n)} \\}$ we define a false discovery rate, $q$:\n",
    "\n",
    "$$FDR(D) \\le q$$\n",
    "\n",
    "The cutoff threshold for the ith p-value is then:\n",
    "$$p_{(i)} \\le Threshold(D_q) = \\frac{q}{n} i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 01-9:** In this exercise you will apply the Benamini-Hochberg FDR control algorithm for testing the significance of the gene expressions. The `BH_significance` function is quite similar to the Holm's method function you have already created. Given the large number of genes you must use a low false discovery rate, $0.001$, or 1 out of 1,000. \n",
    "> Execute your function, saving the result. Then print the number of significant cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BH_significance(test_results, false_discovery_tollerance):\n",
    "    ## First compute the thresholds for each of the ordered tests\n",
    "    ## Put your code below. \n",
    "\n",
    "    \n",
    "    \n",
    "    ## Now we test the significance of the ordered p-values \n",
    "\n",
    "    return test_results\n",
    "\n",
    "BH_results = BH_significance(gene_statistics, 0.001)    \n",
    "len(BH_results.loc[BH_results.loc[:,'significant'],:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The result is similar to the first two FDR control methods. Given the false discovery parameter of 0.0001 do you think this is a reasonable result? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, execute the code in the cell below and examine the resulting plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_significance(BH_results.iloc[:500,:], 'bh_threshold')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 01-10**: Bonus question. Compare the plot above to the foregoing plot for Holm's method. Are the breaks in slope of the p-value curve at the crossing point with the threshold values reasonable in both cases? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Copyright 2021, Stephen F. Elston. All rights reserved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
