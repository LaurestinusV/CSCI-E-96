{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49cb284d",
   "metadata": {},
   "source": [
    "# Assignment 02     \n",
    "## CSCI S-96    \n",
    "\n",
    "> **Instructions:** For this assignment you will complete the exercises shown. All exercises involve creating and executing some Python code. Additionally, most exercises have questions for you to answer. You can answer questions by creating a Markdown cell and writing your answer. If you are not familiar with Markdown, you can find a brief tutorial [here](https://www.markdownguide.org/cheat-sheet/).   \n",
    "\n",
    "In this assignment you will work with some basic streaming analytic algorithms. To avoid the complexities of installing and setting up a real streaming analytics platform,you will work with stream flow data loaded from local files. Specifically in this assignment you will:    \n",
    "1. Create and apply code to perform basic stream queries.    \n",
    "2. Using stream queries and plots, explore the stream data.    \n",
    "3. Use moving windows to compute moving averages and sub-sample a stream.    \n",
    "4. Use exponential decay filters to compute moving averages and sub-sample a stream.    \n",
    "\n",
    "## Overview \n",
    "\n",
    "The United States Geological Survey (USGS) maintains over 13,500 stream flow gages in the United States. Measurements from most of these gages are recoded every 15 min and uploaded every 4 hours are [available for download](https://waterdata.usgs.gov/nwis/rt). Stream flow data are used as inputs for complex water management tasks for purposes such as agriculture, residential use and conservation. For this assignment you will work with the time series of measurements for two stream flow gages on tributaries of the the Columbia River in the US State of Washington.     \n",
    "\n",
    "To get started, execute the code in the cell below to import the packages you will need. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da1ad9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ff02ac",
   "metadata": {},
   "source": [
    "## Loading the Dataset  \n",
    "\n",
    "The next step is to load the stream gage data. The code in the cell below loads the time series data for the first gage. This gage is sited on the Okanogan river.  \n",
    "\n",
    "The code in the cell below does the following:  \n",
    "1. Loads the data from a .csv file. \n",
    "2. Converts the time stamp column to an index of the Pandas data frame. \n",
    "3. Assigns human-understandable names to the columns.  \n",
    "4. Returns just the first 4 columns of the data frame. \n",
    "\n",
    "Execute this code and examine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04afbed2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Agency</th>\n",
       "      <th>Site_number</th>\n",
       "      <th>Time_zone</th>\n",
       "      <th>Stream_flow</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-03-01 00:00</th>\n",
       "      <td>USGS</td>\n",
       "      <td>12447200</td>\n",
       "      <td>PST</td>\n",
       "      <td>2050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-01 00:15</th>\n",
       "      <td>USGS</td>\n",
       "      <td>12447200</td>\n",
       "      <td>PST</td>\n",
       "      <td>2050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-01 00:30</th>\n",
       "      <td>USGS</td>\n",
       "      <td>12447200</td>\n",
       "      <td>PST</td>\n",
       "      <td>2050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-01 00:45</th>\n",
       "      <td>USGS</td>\n",
       "      <td>12447200</td>\n",
       "      <td>PST</td>\n",
       "      <td>2050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-01 01:00</th>\n",
       "      <td>USGS</td>\n",
       "      <td>12447200</td>\n",
       "      <td>PST</td>\n",
       "      <td>2050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-31 22:45</th>\n",
       "      <td>USGS</td>\n",
       "      <td>12447200</td>\n",
       "      <td>PDT</td>\n",
       "      <td>1350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-31 23:00</th>\n",
       "      <td>USGS</td>\n",
       "      <td>12447200</td>\n",
       "      <td>PDT</td>\n",
       "      <td>1350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-31 23:15</th>\n",
       "      <td>USGS</td>\n",
       "      <td>12447200</td>\n",
       "      <td>PDT</td>\n",
       "      <td>1350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-31 23:30</th>\n",
       "      <td>USGS</td>\n",
       "      <td>12447200</td>\n",
       "      <td>PDT</td>\n",
       "      <td>1350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-31 23:45</th>\n",
       "      <td>USGS</td>\n",
       "      <td>12447200</td>\n",
       "      <td>PDT</td>\n",
       "      <td>1350</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17660 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Agency  Site_number Time_zone  Stream_flow\n",
       "datetime                                                   \n",
       "2020-03-01 00:00   USGS     12447200       PST         2050\n",
       "2020-03-01 00:15   USGS     12447200       PST         2050\n",
       "2020-03-01 00:30   USGS     12447200       PST         2050\n",
       "2020-03-01 00:45   USGS     12447200       PST         2050\n",
       "2020-03-01 01:00   USGS     12447200       PST         2050\n",
       "...                 ...          ...       ...          ...\n",
       "2020-08-31 22:45   USGS     12447200       PDT         1350\n",
       "2020-08-31 23:00   USGS     12447200       PDT         1350\n",
       "2020-08-31 23:15   USGS     12447200       PDT         1350\n",
       "2020-08-31 23:30   USGS     12447200       PDT         1350\n",
       "2020-08-31 23:45   USGS     12447200       PDT         1350\n",
       "\n",
       "[17660 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_index_series(file_name):  \n",
    "    df = pd.read_csv(file_name, sep='\\t')\n",
    "    df.index = df.datetime\n",
    "    df.drop('datetime', axis=1, inplace=True)\n",
    "    df = df.iloc[:,:4]\n",
    "    df.columns = ['Agency', 'Site_number', 'Time_zone', 'Stream_flow']\n",
    "    return df.iloc[:,:4]\n",
    "\n",
    "Malott = read_index_series('../data/12447200_Okanogan_at_Malott.txt')\n",
    "Malott"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12aa937",
   "metadata": {},
   "source": [
    "The other time series is for a gage on the Yakima River. Execute the code in the cell below and examine the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04ed6190",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Agency</th>\n",
       "      <th>Site_number</th>\n",
       "      <th>Time_zone</th>\n",
       "      <th>Stream_flow</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-03-01 00:00</th>\n",
       "      <td>USGS</td>\n",
       "      <td>12484500</td>\n",
       "      <td>PST</td>\n",
       "      <td>1620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-01 00:15</th>\n",
       "      <td>USGS</td>\n",
       "      <td>12484500</td>\n",
       "      <td>PST</td>\n",
       "      <td>1620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-01 00:30</th>\n",
       "      <td>USGS</td>\n",
       "      <td>12484500</td>\n",
       "      <td>PST</td>\n",
       "      <td>1620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-01 00:45</th>\n",
       "      <td>USGS</td>\n",
       "      <td>12484500</td>\n",
       "      <td>PST</td>\n",
       "      <td>1620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-01 01:00</th>\n",
       "      <td>USGS</td>\n",
       "      <td>12484500</td>\n",
       "      <td>PST</td>\n",
       "      <td>1620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-31 22:45</th>\n",
       "      <td>USGS</td>\n",
       "      <td>12484500</td>\n",
       "      <td>PDT</td>\n",
       "      <td>3140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-31 23:00</th>\n",
       "      <td>USGS</td>\n",
       "      <td>12484500</td>\n",
       "      <td>PDT</td>\n",
       "      <td>3140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-31 23:15</th>\n",
       "      <td>USGS</td>\n",
       "      <td>12484500</td>\n",
       "      <td>PDT</td>\n",
       "      <td>3140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-31 23:30</th>\n",
       "      <td>USGS</td>\n",
       "      <td>12484500</td>\n",
       "      <td>PDT</td>\n",
       "      <td>3140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-31 23:45</th>\n",
       "      <td>USGS</td>\n",
       "      <td>12484500</td>\n",
       "      <td>PDT</td>\n",
       "      <td>3140</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17660 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Agency  Site_number Time_zone  Stream_flow\n",
       "datetime                                                   \n",
       "2020-03-01 00:00   USGS     12484500       PST         1620\n",
       "2020-03-01 00:15   USGS     12484500       PST         1620\n",
       "2020-03-01 00:30   USGS     12484500       PST         1620\n",
       "2020-03-01 00:45   USGS     12484500       PST         1620\n",
       "2020-03-01 01:00   USGS     12484500       PST         1620\n",
       "...                 ...          ...       ...          ...\n",
       "2020-08-31 22:45   USGS     12484500       PDT         3140\n",
       "2020-08-31 23:00   USGS     12484500       PDT         3140\n",
       "2020-08-31 23:15   USGS     12484500       PDT         3140\n",
       "2020-08-31 23:30   USGS     12484500       PDT         3140\n",
       "2020-08-31 23:45   USGS     12484500       PDT         3140\n",
       "\n",
       "[17660 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CleElm = read_index_series('../data/12479500_Yakima_At_CleElm.txt')\n",
    "CleElm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b7b2f4",
   "metadata": {},
   "source": [
    "Since we really only want to work with one data frame. The code in the cell below merges the two time series and sorts them into time index order. Execute this code and examine the result, paying attention to the site number and the datetime index.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93ccdeee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Agency</th>\n",
       "      <th>Site_number</th>\n",
       "      <th>Time_zone</th>\n",
       "      <th>Stream_flow</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-03-01 00:00</th>\n",
       "      <td>USGS</td>\n",
       "      <td>12447200</td>\n",
       "      <td>PST</td>\n",
       "      <td>2050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-01 00:00</th>\n",
       "      <td>USGS</td>\n",
       "      <td>12484500</td>\n",
       "      <td>PST</td>\n",
       "      <td>1620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-01 00:15</th>\n",
       "      <td>USGS</td>\n",
       "      <td>12484500</td>\n",
       "      <td>PST</td>\n",
       "      <td>1620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-01 00:15</th>\n",
       "      <td>USGS</td>\n",
       "      <td>12447200</td>\n",
       "      <td>PST</td>\n",
       "      <td>2050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-01 00:30</th>\n",
       "      <td>USGS</td>\n",
       "      <td>12447200</td>\n",
       "      <td>PST</td>\n",
       "      <td>2050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-31 23:15</th>\n",
       "      <td>USGS</td>\n",
       "      <td>12447200</td>\n",
       "      <td>PDT</td>\n",
       "      <td>1350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-31 23:30</th>\n",
       "      <td>USGS</td>\n",
       "      <td>12484500</td>\n",
       "      <td>PDT</td>\n",
       "      <td>3140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-31 23:30</th>\n",
       "      <td>USGS</td>\n",
       "      <td>12447200</td>\n",
       "      <td>PDT</td>\n",
       "      <td>1350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-31 23:45</th>\n",
       "      <td>USGS</td>\n",
       "      <td>12447200</td>\n",
       "      <td>PDT</td>\n",
       "      <td>1350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-31 23:45</th>\n",
       "      <td>USGS</td>\n",
       "      <td>12484500</td>\n",
       "      <td>PDT</td>\n",
       "      <td>3140</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35320 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Agency  Site_number Time_zone  Stream_flow\n",
       "datetime                                                   \n",
       "2020-03-01 00:00   USGS     12447200       PST         2050\n",
       "2020-03-01 00:00   USGS     12484500       PST         1620\n",
       "2020-03-01 00:15   USGS     12484500       PST         1620\n",
       "2020-03-01 00:15   USGS     12447200       PST         2050\n",
       "2020-03-01 00:30   USGS     12447200       PST         2050\n",
       "...                 ...          ...       ...          ...\n",
       "2020-08-31 23:15   USGS     12447200       PDT         1350\n",
       "2020-08-31 23:30   USGS     12484500       PDT         3140\n",
       "2020-08-31 23:30   USGS     12447200       PDT         1350\n",
       "2020-08-31 23:45   USGS     12447200       PDT         1350\n",
       "2020-08-31 23:45   USGS     12484500       PDT         3140\n",
       "\n",
       "[35320 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stream_flow = Malott.append(CleElm).sort_index()\n",
    "stream_flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fb8b33",
   "metadata": {},
   "source": [
    "## Querying Stream Data\n",
    "\n",
    "Common stream data operations are often formulated as queries on the stream data. Many streaming data platforms use extensions of SQL for these queries.   \n",
    "\n",
    "To keep things simple in this assignment we will just use a simple query function. The function shown in the cell below supports simple queries on the    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f66133e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def query_stream(df, Columns=None, site_numbers=None, start_time=None, end_time=None):    \n",
    "    '''\n",
    "    Function to query the stream gage time series data. The arguments are:    \n",
    "    df = the data frame containing the data.  \n",
    "    Columns = a list of columns to return.   \n",
    "    site_numbers = a list of gage site numbers to return. \n",
    "    start_time = the start time of the returned data.   \n",
    "    end_time = the end time of the returned data.\n",
    "    '''\n",
    "    ## First set values for arguments set to Null  \n",
    "    if Columns==None: Columns = df.columns\n",
    "    if start_time==None: start_time = df.index[0]\n",
    "    if end_time==None: end_time = df.index[df.shape[0]-1]\n",
    "    if site_numbers==None: site_numbers = df.Site_number.unique()\n",
    "    ## Test if index is a string datetime or an integer\n",
    "    ## use iloc method if an integer.\n",
    "    ## A slice over the time range is created based on the index type. \n",
    "    if isinstance(start_time, str):\n",
    "        df = df.loc[start_time:end_time,:]\n",
    "    else:     \n",
    "        df = df.iloc[start_time:end_time,:]\n",
    "    df = df.loc[df.Site_number.isin(site_numbers), Columns]\n",
    "    ## Return the results of the query\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc31bf56",
   "metadata": {},
   "source": [
    "You can see the options to run `query_stream` function by executing the code in the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "111d0fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Function to query the stream gage time series data. The arguments are:    \n",
      "    df = the data frame containing the data.  \n",
      "    Columns = a list of columns to return.   \n",
      "    site_numbers = a list of gage site numbers to return. \n",
      "    start_time = the start time of the returned data.   \n",
      "    end_time = the end time of the returned data.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(query_stream.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79daad7f",
   "metadata": {},
   "source": [
    "An example of using the query function is shown in the cell below. Execute this code and examine the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff424adb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17660"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(query_stream(stream_flow, Columns='Stream_flow', site_numbers=[12484500]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c0c9f9",
   "metadata": {},
   "source": [
    "> **Exercise 02-01:** Using the `query_stream` function, write and execute the code in the cell below to compute and display the mean `Stream_flow`for the month of April of 2020 of site 1248500. Use the [Pandas.DataFrame.mean](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.mean.html) method to compute the mean. Notice that using this approach we can compute most any statistic of interest on the query result.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22e3173b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2238.241525423729"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_stream(stream_flow, Columns='Stream_flow', site_numbers=[12484500], start_time='2020-04-01 00:00', end_time='2020-04-30 11:59').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc98d30",
   "metadata": {},
   "source": [
    "## Plotting Streaming Data\n",
    "\n",
    "Visualization is important tool in data exploration and discovery. Numerical stream data is ideal for visual exploration if it can be subsampled to manageable size.  \n",
    "\n",
    "The function in the cell below creates a time series plot. The time index of the Pandas data frame is used to generate the x-axis values. Execute the code in this cell to load this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768467d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_time_series(df, ax=None): \n",
    "    if ax==None: fig, ax = plt.subplots(figsize=(20, 6))\n",
    "    df.plot(ax=ax);\n",
    "    ax.set_xlabel('Date');\n",
    "    plt.show()\n",
    "    return ax    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ea571c",
   "metadata": {},
   "source": [
    "The code in the cell below creates time series plots of the stream flow data. The flow time series for two stream gages queried as arguments to the plot function. Execute this code and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e82d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "_=plot_time_series(query_stream(stream_flow, Columns='Stream_flow', site_numbers=[12447200])) \n",
    "_=plot_time_series(query_stream(stream_flow, Columns='Stream_flow', site_numbers=[12484500]))    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542c53ca",
   "metadata": {},
   "source": [
    "The time series of stream flow at both of these gages is rather complex. Both rivers have several dams used to control the flow. The flow is optimized to conserve fisheries and to supply agriculture in the Columbia River Basin. Water in reservoirs accumulates in the spring as mountain snow melts. The water is then is released throughout the spring and summer. \n",
    "\n",
    "But, what can we make of the noticeable spikes in flow, particularly for gage $12484500$ on the Yakima River. Even with the control provided by dams and reservoirs spring and early summer storm events can cause temporary increases in water flow. These storms bring heavy, and often warm, rain. Flow in the rivers increases not only because of the rainfall, but also since warm rain accelerates snow melt in the higher elevations.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e06c6f",
   "metadata": {},
   "source": [
    "> **Exercise 02-02:** The transitory flow events on the Yakima River warrant some further investigation. You now have the tools to query and plot the stream flow time series. Your goal is to determine if there are common properties (e.g. duration or amplitude) of these events. Plot the results of a query for stream flows on gage $12484500$, Yakima River, from the 6th day of April to the 20th day of June, 2020. Discuss any common pattern in terms of approximately common amplitudes or durations of these events you can see.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482f5afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "_=plot_time_series(query_stream(stream_flow, Columns='Stream_flow', site_numbers=[12484500], start_time='2020-04-06 00:00', end_time='2020-06-20 11:59'))    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e8589b",
   "metadata": {},
   "source": [
    "## Applying Moving Window Filters\n",
    "\n",
    "Moving window filters are a commonly used method to compute statistical samples from streaming data. \n",
    "\n",
    "Apply a moving window filter. \n",
    "\n",
    "> **Exercise 02-03:** You will complete and test the function in the cell below. The function queries a time series to create overlapping windows of a specified length and stride. For each window the mean of the stream flow is computed. The function returns a [Pandas Series](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html) object. The time index of the Series object is the mid-point index of the window used to compute the statistic. \n",
    "> 1. Use a `for` loop to create the overlapping moving window samples of the input. The window will use the `length` and `stride` arguments to the function. At each iteration, the window will advance by `stride` time steps. See the [tutorial on the Python range function](https://www.w3schools.com/python/ref_func_range.asp) for help.    \n",
    ">   - Query the input stream data for the stream flow values in the window. The `query_stream` function will accept integer indices for the `start_time` and `end_time` arguments. Make sure the these indicies are within the range of the original time series.  \n",
    ">   - Append the mean of the stream flow values in the window to the `out` list.\n",
    ">   - Append the time index of the midpoint of the window to the `idx` list.  \n",
    "> 2. Once the loop has terminated used the [Pandas.Series](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html) constructor to instantiate the return series. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0373cc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_average(ts, length=16, stride=8, Columns='Stream_flow', site_numbers=[12484500]):\n",
    "    half_length = int(length/2)-1\n",
    "    idx = []\n",
    "    out = []\n",
    "## Put your code below\n",
    "   \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    out = pd.Series(out, index=idx)    \n",
    "    return out     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be829a8",
   "metadata": {},
   "source": [
    "> 3. Next you will test your function by completing and executing the code in the cell below. Use your `window_average` function to create a Pandas Series with 4-hour stream flow averages (length of 16 time steps), taken every 2 hours (stride of 8 time steps). Name your Series filtered_12 and compute and print the length. The code provided queries the data so that you are working with only values from the Yakima River gage. Using flow rate values from only one gage simplifies the bookkeeping for window sampling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501d37a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Query to create a series with only the Yakima River stream gage data. \n",
    "## Put your code below\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ca04cf",
   "metadata": {},
   "source": [
    "> 4. Notice how the length of the time series has been significantly reduced. Is the reduction the length of the time series consistent with a stride of 8 time steps?   \n",
    "> **End of Exercise.**\n",
    "\n",
    "To examine the 4-hour moving average time series you have computed, execute the code in the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1759af3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd0b31d",
   "metadata": {},
   "source": [
    "> **Exercise 02-4:** You will compute and display a time series using a longer, 1-day (96 time steps) moving window with a stride of 1/2 of a day (48 time steps). For this exercise, do the following:   \n",
    "> 1. Query the Yakima River stream gage data with the longer time window and stride. \n",
    "> 2. Print the length of the resulting Pandas Series.  \n",
    "> 3. Plot the moving average series.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881ebcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code below\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38d9f87",
   "metadata": {},
   "source": [
    "> Answer the following questions:   \n",
    "> A. What is the data compression ratio of the longer time window? Is this consistent with the stride of the window?   \n",
    "> B. Compare the plots of the results of the two moving window summaries. What are the obvious differences? If the goal is to measure total volume of water passing the gage on a daily and weekly basis, does the series from the longer filter contain sufficient detail? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438bb96b",
   "metadata": {},
   "source": [
    "## Exponential Decay Filters\n",
    "\n",
    "The idea of using exponential smooth for time series analysis is an old one, dating at least to use by Weiner in the 1920s. The related idea of moving average filters was developed by Kolmogorov and Zurbenko in the 1940s. Exponential smoothers were used extensively in signal process in the 1940s. The general idea was expanded by Robert Goodell Brown (1956) and C.E. Holt (1957) and his student P.R. Winters (1960). The higher-order Holt-Winters model accounts for trend and seasonality of time series."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c336514",
   "metadata": {},
   "source": [
    "### Basic exponential Smoothing\n",
    "\n",
    "Exponential smoothing uses a weighted sum of the current observation and the past smoothed value to compute a new smoothed value. This basic exponential smoothing relationship is shown below.  \n",
    "\n",
    "$$\n",
    "s_0 = x_0 \\\\\n",
    "s_t = \\alpha x_t + (1-\\alpha) s_{t-1} = s_{t-1} \\alpha(x_t - s_{t-1}),\\ t \\gt 0\n",
    "$$\n",
    "\n",
    "The smoothing hyperparameter, $\\alpha$, controls the trade-off between the last observation and the previous smoothed values. The possible values are in the range, $0 \\le \\alpha \\le 1$. A large value of $\\alpha$ puts more weight on the current observation. Whereas, a small value of $\\alpha$ puts more weight on the smoothed history.      \n",
    "\n",
    "How can we understand the exponential decay of the smoothed history of a time series. The smoothing hyperparameter, $\\alpha$, an be expressed in terms of the decay constant, $\\tau$ and time interval $\\delta T$ as shown below.  \n",
    "\n",
    "$$\n",
    "\\alpha = 1 - e^{\\big( \\frac{- \\Delta T}{\\tau} \\big)}\n",
    "$$\n",
    "\n",
    "From this relationship you can see that the influence of the smoothed history decays exponentially as $\\delta T$ increases. The decay time increases as $\\tau$ decreases.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e56565",
   "metadata": {},
   "source": [
    "### Smoothing with higher-order terms   \n",
    "\n",
    "The basic exponential smoothing algorithm is effective in many cases. However, the simple first order exponential smoothing method cannot accommodate time series with trend or seasonality. Higher order smoothing models are required.   \n",
    "\n",
    "The **double exponential smoothing** or **Holt-Winters double exponential smoothing** algorithm is a second order smoothing method. Using two coupled difference equations a trend and non-seasonal component of the time series can be modeled. The model updates a smoothed measure of the non-seasonal component and the trend.   \n",
    "\n",
    "The model is initialized with the values:   \n",
    "$$\n",
    "s_1 = x_1 \\\\\n",
    "b_1 = x_2 - x_1\n",
    "$$\n",
    "\n",
    "At each time step the a pair of time difference equations are updated. The following relationships update the smoothed non-seasonal component, $s_t$, and the slope, $b_t$:   \n",
    "\n",
    "$$\n",
    "s_t = \\alpha x_t + (1-\\alpha) (s_{t-1} + b_{t-1}) \\\\\n",
    "b_t = \\beta(s_t - s_{t-1}) + (1 - \\beta)b_{t-1}\n",
    "$$\n",
    "\n",
    "The smoothed non-seasonal component and smoothed slope can be used to compute a forecast. The relationship below computes the forecast $m$ time steps ahead.      \n",
    "\n",
    "$$ F_{t+m} = s_t + m b_t $$   \n",
    "\n",
    "What about seasonal components? A third-order difference relationship can updated a smoothed seasonal component, along with the smoothed non-seasonal and slope components. The details of this process are not discussed further here. The details are available elsewhere, including the [exponential smoothing Wikipedia page](https://en.wikipedia.org/wiki/Exponential_smoothing#:~:text=Exponential%20smoothing%20is%20a%20rule,exponentially%20decreasing%20weights%20over%20time.).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4642b7",
   "metadata": {},
   "source": [
    "### Example of Exponential Decay Filtering     \n",
    "\n",
    "> **Exercise 02-5:** You will now create and test an exponentially weighted decay filter. This function will have a stride argument just as the window filter function. Your function, `exponential_smooth`, will have arguments of the time series, the exponential smoothing parameter and a stride. Your function will do the following:    \n",
    "> 1. Save the index of the incoming time series to a variable.   \n",
    "> 2. Initialize the an empty index list for the output series.  \n",
    "3. Initialize the value list for the samples. The samples list will contain the exponentially weighted smoothed samples. Make sure you save the first value in the list.   \n",
    "> 4. Initialize an empty output value list.   \n",
    "> 5. A for loop iterates over all the values of the time series starting with the second one. In this case a query is not used since for a live stream the exponential decay filter is updated each time a value arrives.    \n",
    ">   - Append the computed exponentially weighted values smoothed values to the samples list. \n",
    ">   - If the loop index modulo the stride is 0 then, append the sample value to the output list and the index of that sample to the index list.  \n",
    "> 8. Create an output Pandas Series from the output list and the output index list. \n",
    "> 9. Return the Pandas Series.  \n",
    "> 10. Execute your function with for site number $12484500$ and default arguments of `alpha=0.01` and `stride=8`, 2 hours. \n",
    "> 11. Print the length the resulting series. \n",
    "> 12. Plot the series with the `plot_time_series` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68414cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_smooth(ts, alpha=0.01, stride=8):\n",
    "    in_index = ts.index\n",
    "    idx = []\n",
    "    samples=[ts[0]]\n",
    "    out = []\n",
    "    ## Put your code below\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    out = pd.Series(out, index=idx)    \n",
    "    return out       \n",
    "\n",
    "\n",
    "smoothed_01 = exponential_smooth(query_stream(stream_flow, Columns='Stream_flow', site_numbers=[12484500]))\n",
    "\n",
    "print(len(smoothed_01))\n",
    "_=plot_time_series(smoothed_01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10f002f",
   "metadata": {},
   "source": [
    "> Answer the following questions. Are the number of smoothed samples correct for the stride of the exponential decay filter selected? Are the details of the filtered time series substantially the same as the original series?  \n",
    "> **End of exercise.** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac1a1de",
   "metadata": {},
   "source": [
    "> **Exercise 02-6:** A question we should ask is what happens if you increase the smoothing constant of the exponential decay filter? In other words, what is the effect of giving greater weight to past values? To find out do the following:  \n",
    "> 1. Execute the `exponential_smooth` function with arguments `alpha=0.99` and `stride=8`.\n",
    "> 2. Print the length of the resulting Pandas Series. \n",
    "> 3. Plot the smoothed time series using the `plot_time_series` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfc23f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code below\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15224fcb",
   "metadata": {},
   "source": [
    "> Answer the following questions. Compare the plot of this time series to the previous series less smoothing. What is the main difference you can see?     \n",
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf8d3e3",
   "metadata": {},
   "source": [
    "> **Exercise 02-7:** The next question to ask is what is the effect of changing the stride? A longer stride reduces the number of smoothed samples used for further processing. To find out do the following:  \n",
    "> 1. Execute the `exponential_smooth` function with arguments `alpha=0.99` and `stride=96`, one day or 24 hours.\n",
    "> 2. Print the length of the resulting Pandas Series. \n",
    "> 3. Plot the smoothed time series using the `plot_time_series` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5b74bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code below\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21d1f30",
   "metadata": {},
   "source": [
    "> Answer the following questions. Compare the plot of this time series to the previous series with a shorter stride. What is the main difference you can see? Do you think any difference is significant in terms of managing water flow on a daily or weekly basis? Finally, is the number of samples consistent with the stride defined?       \n",
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9103ccd9",
   "metadata": {},
   "source": [
    "#### Copyright 2021 Stephen F Elston. All rights reserved. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
